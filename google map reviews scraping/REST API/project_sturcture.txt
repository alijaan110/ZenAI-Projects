maps-scraper-api/
├── app/
│   ├── __init__.py
│   ├── main.py                 # FastAPI application with all endpoints
│   ├── scraper.py              # Refactored scraper class
│   └── celery_worker.py        # Optional: Celery task queue
│
├── output/                     # Generated review JSON files
│   └── .gitkeep
│
├── tests/                      # Unit and integration tests
│   ├── __init__.py
│   ├── test_api.py
│   └── test_scraper.py
│
├── .env.example               # Environment variables template
├── .gitignore                 # Git ignore file
├── docker-compose.yml         # Docker Compose configuration
├── Dockerfile                 # Docker container definition
├── maps-scraper.service       # Systemd service file
├── nginx.conf                 # Nginx reverse proxy config
├── README.md                  # Complete documentation
├── requirements.txt           # Python dependencies
└── setup.sh                   # Ubuntu server setup script


DEPLOYMENT OPTIONS:
==================

1. Docker (Recommended)
   - docker-compose up -d
   - Automatic setup of Chrome, ChromeDriver, and dependencies

2. Manual Installation
   - Run setup.sh on Ubuntu server
   - Install dependencies and configure systemd service

3. Systemd Service
   - For production deployment without Docker
   - Automatic restart on failure

4. Nginx Reverse Proxy
   - SSL/TLS support
   - Load balancing
   - Static file serving


QUICK START COMMANDS:
====================

# Docker deployment
docker-compose up -d
docker-compose logs -f api

# Manual deployment
sudo bash setup.sh
cd /opt/maps-scraper-api
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 8000

# Systemd service
sudo systemctl enable maps-scraper
sudo systemctl start maps-scraper
sudo systemctl status maps-scraper

# View API docs
http://localhost:8000/docs