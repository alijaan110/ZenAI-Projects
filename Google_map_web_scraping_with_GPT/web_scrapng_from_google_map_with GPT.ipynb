{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "\"\"\"\n",
    "Install all required dependencies for web scraping and LLM integration\n",
    "\"\"\"\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q requests beautifulsoup4 openai tqdm fake-useragent selenium webdriver-manager lxml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Map Web Scraping with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "================================================================================\n",
      "üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "üîç Extracting website from Google Maps URL...\n",
      "‚ö†Ô∏è Could not extract website URL\n",
      "‚ùå Failed to extract website URL from Google Maps\n",
      "‚ùå No data extracted. Please check the Google Maps URL and try again.\n",
      "\n",
      "‚úÖ Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# Enhanced Google Colab Notebook with Smart Page Selection\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: SETUP AND INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OpenAI for LLM integration\n",
    "import openai\n",
    "\n",
    "# Selenium imports (for dynamic content if needed)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# print(\"‚úÖ All packages installed successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION AND API SETUP\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Configure API keys and scraping parameters\n",
    "\"\"\"\n",
    "\n",
    "# Set your OpenAI API Key\n",
    "# For Google Colab, use userdata secrets or set directly\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Fallback for local environment\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Scraping configuration\n",
    "MAX_CRAWL_DEPTH = 2  # How deep to crawl internal links\n",
    "MAX_PAGES = 10  # Maximum pages to discover during crawl\n",
    "TOP_PAGES_TO_ANALYZE = 6  # Number of most relevant pages to analyze\n",
    "REQUEST_TIMEOUT = 10  # Seconds\n",
    "RATE_LIMIT_DELAY = 1  # Seconds between requests\n",
    "\n",
    "# Initialize user agent rotator\n",
    "ua = UserAgent()\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Extract business website URL from Google Maps link\n",
    "\"\"\"\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    \"\"\"Initialize Selenium WebDriver for dynamic content\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    \"\"\"\n",
    "    Extract the main business website URL from a Google Maps link\n",
    "    \n",
    "    Args:\n",
    "        maps_url (str): Google Maps URL\n",
    "        use_selenium (bool): Whether to use Selenium for dynamic content\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted website URL or None\n",
    "    \"\"\"\n",
    "    print(f\"üîç Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Look for website link with multiple selectors\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"‚úÖ Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback: search in page source\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Find all links\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"‚úÖ Found website via fallback: {href}\")\n",
    "                        return href\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    # Fallback to requests\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Search for website links in HTML\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"‚úÖ Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with requests: {e}\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Crawl all internal pages of the website\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    \"\"\"Check if URL is valid and belongs to the same domain\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        \n",
    "        # Check if same domain\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        \n",
    "        # Skip common file extensions and external resources\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        \n",
    "        # Skip common non-content paths\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    \"\"\"\n",
    "    Crawl website and collect all internal page URLs\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): Starting URL\n",
    "        max_depth (int): Maximum crawl depth\n",
    "        max_pages (int): Maximum number of pages to crawl\n",
    "    \n",
    "    Returns:\n",
    "        list: List of unique page URLs\n",
    "    \"\"\"\n",
    "    print(f\"üï∑Ô∏è Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url, 0)])  # (url, depth)\n",
    "    pages = []\n",
    "    \n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        \n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        \n",
    "        visited.add(current_url)\n",
    "        \n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            \n",
    "            pages.append(current_url)\n",
    "            print(f\"  ‚úì Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            \n",
    "            # Parse page for more links\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                # Remove fragments and query parameters for deduplication\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0]\n",
    "                \n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Use LLM to intelligently select the most relevant pages for data extraction\n",
    "\"\"\"\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    \"\"\"\n",
    "    Use LLM to select the most relevant pages for business information extraction\n",
    "    \n",
    "    Args:\n",
    "        page_urls (list): List of all discovered page URLs\n",
    "        top_n (int): Number of top pages to select\n",
    "    \n",
    "    Returns:\n",
    "        list: List of selected page URLs\n",
    "    \"\"\"\n",
    "    print(f\"\\nü§ñ Using LLM to select top {top_n} most relevant pages...\")\n",
    "    \n",
    "    # Create a numbered list of URLs for the LLM\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your task is to identify the most relevant pages \n",
    "from a website that would contain business information such as company details, services, contact information, \n",
    "and business description.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"I have crawled a business website and found {len(page_urls)} pages. \n",
    "Please analyze the URLs and select the TOP {top_n} most relevant pages that would likely contain:\n",
    "- Company name and overview\n",
    "- About/Company information\n",
    "- Services or products offered\n",
    "- Contact information (email, phone, address)\n",
    "- Social media links\n",
    "- Business description\n",
    "\n",
    "Here are all the discovered page URLs:\n",
    "{url_list}\n",
    "\n",
    "Prioritize pages like:\n",
    "- Home/Index pages\n",
    "- About pages\n",
    "- Services/Products pages\n",
    "- Contact pages\n",
    "- Company/Team pages\n",
    "- Portfolio/Work pages\n",
    "\n",
    "Return ONLY a valid JSON array containing the numbers of the selected pages (1-indexed).\n",
    "Example format: [1, 3, 5, 7, 9, 12, 15]\n",
    "\n",
    "Your response must be ONLY the JSON array, nothing else.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        # Parse JSON array\n",
    "        selected_indices = json.loads(content)\n",
    "        \n",
    "        # Convert 1-indexed to 0-indexed and get URLs\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"‚úÖ LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        \n",
    "        return selected_urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in LLM page selection: {e}\")\n",
    "        print(\"‚ö†Ô∏è Falling back to heuristic selection...\")\n",
    "        \n",
    "        # Fallback: Use keyword-based heuristic\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company']\n",
    "        scored_pages = []\n",
    "        \n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(1 for keyword in priority_keywords if keyword in url_lower)\n",
    "            # Prioritize shorter URLs (often more important)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        \n",
    "        # Sort by score and take top N\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        \n",
    "        print(f\"‚úÖ Heuristically selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: CONTENT EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Extract clean text content from web pages\n",
    "\"\"\"\n",
    "\n",
    "def extract_page_content(url):\n",
    "    \"\"\"\n",
    "    Extract visible text content from a webpage\n",
    "    \n",
    "    Args:\n",
    "        url (str): Page URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: LLM-POWERED CONSOLIDATED DATA EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Use LLM to extract all business data from combined page content\n",
    "\"\"\"\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url):\n",
    "    \"\"\"\n",
    "    Use LLM to extract consolidated business information from multiple pages\n",
    "    \n",
    "    Args:\n",
    "        page_contents (dict): Dictionary mapping URLs to their text content\n",
    "        main_website_url (str): Main website URL\n",
    "    \n",
    "    Returns:\n",
    "        dict: Consolidated business information\n",
    "    \"\"\"\n",
    "    print(\"\\nü§ñ Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    # Combine all page contents with clear separation\n",
    "    combined_content = \"\"\n",
    "    for url, content in page_contents.items():\n",
    "        # Truncate individual pages if too long\n",
    "        truncated_content = content[:8000] if len(content) > 8000 else content\n",
    "        combined_content += f\"\\n\\n--- PAGE: {url} ---\\n{truncated_content}\\n\"\n",
    "    \n",
    "    # Limit total content size\n",
    "    max_total_chars = 40000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated due to length...]\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Analyze the provided webpage content \n",
    "from multiple pages of a business website and extract comprehensive, accurate business information. \n",
    "Consolidate information from all pages to provide the most complete picture.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze the content from {len(page_contents)} pages of this business website: {main_website_url}\n",
    "\n",
    "Extract and return a SINGLE consolidated JSON object with these exact fields:\n",
    "\n",
    "1. company_name: The official business/company name\n",
    "2. company_main_url: The main website URL ({main_website_url})\n",
    "3. emails: Array of ALL unique email addresses found across all pages\n",
    "4. contact_numbers: Array of ALL unique phone numbers found (include country code if present)\n",
    "5. social_media_links: Array of ALL social media profile URLs (Facebook, Instagram, LinkedIn, Twitter/X, YouTube, TikTok, etc.)\n",
    "6. summary: A comprehensive 3-5 sentence summary describing:\n",
    "   - What the business does\n",
    "   - Main services/products offered\n",
    "   - Key specialties or unique offerings\n",
    "   - Target market/audience if mentioned\n",
    "\n",
    "IMPORTANT:\n",
    "- Use null for fields where no information is found\n",
    "- For arrays, return empty array [] if no items found\n",
    "- Deduplicate all arrays (no repeated emails, phones, or social links)\n",
    "- The summary should be detailed and informative, covering all services/activities mentioned\n",
    "\n",
    "Combined website content:\n",
    "{combined_content}\n",
    "\n",
    "Return ONLY a valid JSON object with these exact field names. No additional text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"  ‚è≥ Sending request to LLM (this may take 10-20 seconds)...\")\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        # Ensure all required fields exist\n",
    "        required_fields = [\"company_name\", \"company_main_url\", \"emails\", \"contact_numbers\", \n",
    "                          \"social_media_links\", \"summary\"]\n",
    "        for field in required_fields:\n",
    "            if field not in extracted_data:\n",
    "                extracted_data[field] = None if field not in [\"emails\", \"contact_numbers\", \"social_media_links\"] else []\n",
    "        \n",
    "        print(\"‚úÖ Data extraction successful!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM extraction error: {e}\")\n",
    "        return {\n",
    "            \"company_name\": None,\n",
    "            \"company_main_url\": main_website_url,\n",
    "            \"emails\": [],\n",
    "            \"contact_numbers\": [],\n",
    "            \"social_media_links\": [],\n",
    "            \"summary\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: MAIN ORCHESTRATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Main function to orchestrate the entire scraping pipeline\n",
    "\"\"\"\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    \"\"\"\n",
    "    Complete pipeline to scrape business data from Google Maps URL\n",
    "    \n",
    "    Args:\n",
    "        google_maps_url (str): Google Maps URL of the business\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (extracted_data dict, business_name str)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Extract website from Google Maps\n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    \n",
    "    if not website_url:\n",
    "        print(\"‚ùå Failed to extract website URL from Google Maps\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nüìç Main Website: {website_url}\\n\")\n",
    "    \n",
    "    # Step 2: Crawl website to discover all pages\n",
    "    all_pages = crawl_website(website_url)\n",
    "    \n",
    "    if not all_pages:\n",
    "        print(\"‚ùå No pages found to scrape\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nüìÑ Discovered {len(all_pages)} total pages\")\n",
    "    \n",
    "    # Step 3: Use LLM to select most relevant pages\n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    \n",
    "    if not selected_pages:\n",
    "        print(\"‚ùå No pages selected for analysis\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 4: Extract content from selected pages\n",
    "    print(f\"\\nüì• Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    \n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        content = extract_page_content(page_url)\n",
    "        if content:\n",
    "            page_contents[page_url] = content\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted content from {len(page_contents)} pages\")\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"‚ùå No content extracted from any page\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 5: Use LLM to extract consolidated business data\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url)\n",
    "    \n",
    "    # Extract business name for filename\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        # Clean business name for filename\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', business_name)\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return extracted_data, business_name\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DATA SAVING AND DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Save and display extraction results\n",
    "\"\"\"\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    \"\"\"\n",
    "    Save extraction results to JSON file with business name\n",
    "    \n",
    "    Args:\n",
    "        extracted_data (dict): Extracted business data\n",
    "        business_name (str): Business name for filename\n",
    "        all_pages_count (int): Total pages discovered\n",
    "        selected_pages_count (int): Pages analyzed\n",
    "    \n",
    "    Returns:\n",
    "        str: Output filename\n",
    "    \"\"\"\n",
    "    # Create filename based on business name\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    \n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    \"\"\"Display a formatted summary of extracted data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüè¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"üåê Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nüìß Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   ‚Ä¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nüì± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   ‚Ä¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nüîó Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            # Extract platform name\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link:\n",
    "                platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link:\n",
    "                platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link:\n",
    "                platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link:\n",
    "                platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link:\n",
    "                platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link:\n",
    "                platform = \"TikTok\"\n",
    "            print(f\"   ‚Ä¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nüìù Business Summary:\")\n",
    "    print(f\"   {summary}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Run the complete scraping pipeline\n",
    "\"\"\"\n",
    "\n",
    "# INPUT: Paste your Google Maps URL here\n",
    "GOOGLE_MAPS_URL = \"https://www.google.com/maps/place/Xiao+Chi+Jie/@47.6135353,-122.2003497,1017m/data=!3m2!1e3!5s0x54906c87f6b05be7:0x7257abf958f252ea!4m6!3m5!1s0x54906d9668cef7b3:0xb7b3f7bd67692ab2!8m2!3d47.6126588!4d-122.1986349!16s%2Fg%2F11ghnpwhdz?entry=ttu&g_ep=EgoyMDI1MTExMS4wIKXMDSoASAFQAw%3D%3D\"\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate API key\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"‚ùå ERROR: Please set your OPENAI_API_KEY!\")\n",
    "        print(\"In Google Colab: Use Secrets (key icon) to add OPENAI_API_KEY\")\n",
    "        print(\"Locally: Set environment variable or update the code\")\n",
    "    else:\n",
    "        # Store initial counts\n",
    "        all_pages_discovered = 0\n",
    "        pages_analyzed = 0\n",
    "        \n",
    "        # Run the scraper\n",
    "        extracted_data, business_name = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            # Save results with business name in filename\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                TOP_PAGES_TO_ANALYZE\n",
    "            )\n",
    "            \n",
    "            # Display summary\n",
    "            display_summary(extracted_data)\n",
    "            \n",
    "            # Download file in Colab\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(f\"\\n‚¨áÔ∏è Downloading: {filename}\")\n",
    "            except:\n",
    "                print(f\"üìÅ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted. Please check the Google Maps URL and try again.\")\n",
    "\n",
    "print(\"\\n‚úÖ Script execution complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "================================================================================\n",
      "üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "üîç Extracting website from Google Maps URL...\n",
      "‚úÖ Found website: http://www.tasteoftexas.com/\n",
      "\n",
      "üìç Main Website: http://www.tasteoftexas.com/\n",
      "\n",
      "üï∑Ô∏è Starting website crawl from: http://www.tasteoftexas.com/\n",
      "  ‚úì Discovered [1/30]: http://www.tasteoftexas.com/\n",
      "  ‚úì Discovered [2/30]: http://www.tasteoftexas.com/visit-us/\n",
      "  ‚úì Discovered [3/30]: http://www.tasteoftexas.com/compare\n",
      "  ‚úì Discovered [4/30]: http://www.tasteoftexas.com/cart.php\n",
      "  ‚úì Discovered [5/30]: https://www.tasteoftexas.com/\n",
      "  ‚úì Discovered [6/30]: https://www.tasteoftexas.com/menu/\n",
      "  ‚úì Discovered [7/30]: https://www.tasteoftexas.com/wine-comparison/\n",
      "  ‚úì Discovered [8/30]: https://www.tasteoftexas.com/visit-us/\n",
      "  ‚úì Discovered [9/30]: https://www.tasteoftexas.com/private-events/\n",
      "  ‚úì Discovered [10/30]: https://www.tasteoftexas.com/faqs/\n",
      "  ‚úì Discovered [11/30]: https://www.tasteoftexas.com/to-go/\n",
      "  ‚úì Discovered [12/30]: https://www.tasteoftexas.com/steak-gift-boxes/\n",
      "  ‚úì Discovered [13/30]: https://www.tasteoftexas.com/about-us/\n",
      "  ‚úì Discovered [14/30]: https://www.tasteoftexas.com/our-beef/\n",
      "  ‚úì Discovered [15/30]: https://www.tasteoftexas.com/texas-history-museum/\n",
      "  ‚úì Discovered [16/30]: https://www.tasteoftexas.com/grilling-101/\n",
      "  ‚úì Discovered [17/30]: https://www.tasteoftexas.com/signature-dishes/\n",
      "  ‚úì Discovered [18/30]: https://www.tasteoftexas.com/newsletters/\n",
      "  ‚úì Discovered [19/30]: https://www.tasteoftexas.com/press-accolades/\n",
      "  ‚úì Discovered [20/30]: https://www.tasteoftexas.com/school-tours/\n",
      "  ‚úì Discovered [21/30]: https://www.tasteoftexas.com/contact-us/\n",
      "  ‚úì Discovered [22/30]: http://www.tasteoftexas.com/holiday/\n",
      "  ‚úì Discovered [23/30]: http://www.tasteoftexas.com/restaurant/\n",
      "  ‚úì Discovered [24/30]: http://www.tasteoftexas.com/shop/\n",
      "  ‚úì Discovered [25/30]: http://www.tasteoftexas.com/refunds-and-returns/\n",
      "  ‚úì Discovered [26/30]: http://www.tasteoftexas.com/website-terms-of-use/\n",
      "  ‚úì Discovered [27/30]: http://www.tasteoftexas.com/privacy-policy/\n",
      "  ‚úì Discovered [28/30]: http://www.tasteoftexas.com/accessibility-statement/\n",
      "  ‚úì Discovered [29/30]: http://www.tasteoftexas.com/menu/\n",
      "  ‚úì Discovered [30/30]: http://www.tasteoftexas.com/signature-dishes/\n",
      "‚úÖ Crawl complete! Discovered 30 pages\n",
      "\n",
      "üìÑ Discovered 30 total pages\n",
      "\n",
      "ü§ñ Using LLM to select top 30 most relevant pages...\n",
      "‚ùå Error in LLM page selection: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "‚ö†Ô∏è Falling back to heuristic selection...\n",
      "‚úÖ Heuristically selected 30 pages\n",
      "\n",
      "üì• Extracting content from 30 selected pages...\n",
      "  [1/30] Extracting: https://www.tasteoftexas.com/about-us/\n",
      "  [2/30] Extracting: https://www.tasteoftexas.com/contact-us/\n",
      "  [3/30] Extracting: http://www.tasteoftexas.com/\n",
      "  [4/30] Extracting: https://www.tasteoftexas.com/\n",
      "  [5/30] Extracting: http://www.tasteoftexas.com/shop/\n",
      "  [6/30] Extracting: http://www.tasteoftexas.com/menu/\n",
      "  [7/30] Extracting: https://www.tasteoftexas.com/menu/\n",
      "  [8/30] Extracting: https://www.tasteoftexas.com/faqs/\n",
      "  [9/30] Extracting: http://www.tasteoftexas.com/compare\n",
      "  [10/30] Extracting: https://www.tasteoftexas.com/to-go/\n",
      "  [11/30] Extracting: http://www.tasteoftexas.com/cart.php\n",
      "  [12/30] Extracting: http://www.tasteoftexas.com/holiday/\n",
      "  [13/30] Extracting: http://www.tasteoftexas.com/visit-us/\n",
      "  [14/30] Extracting: https://www.tasteoftexas.com/visit-us/\n",
      "  [15/30] Extracting: https://www.tasteoftexas.com/our-beef/\n",
      "  [16/30] Extracting: http://www.tasteoftexas.com/restaurant/\n",
      "  [17/30] Extracting: https://www.tasteoftexas.com/newsletters/\n",
      "  [18/30] Extracting: https://www.tasteoftexas.com/grilling-101/\n",
      "  [19/30] Extracting: https://www.tasteoftexas.com/school-tours/\n",
      "  [20/30] Extracting: http://www.tasteoftexas.com/privacy-policy/\n",
      "  [21/30] Extracting: https://www.tasteoftexas.com/private-events/\n",
      "  [22/30] Extracting: https://www.tasteoftexas.com/wine-comparison/\n",
      "  [23/30] Extracting: https://www.tasteoftexas.com/press-accolades/\n",
      "  [24/30] Extracting: http://www.tasteoftexas.com/signature-dishes/\n",
      "  [25/30] Extracting: https://www.tasteoftexas.com/steak-gift-boxes/\n",
      "  [26/30] Extracting: https://www.tasteoftexas.com/signature-dishes/\n",
      "  [27/30] Extracting: http://www.tasteoftexas.com/refunds-and-returns/\n",
      "  [28/30] Extracting: http://www.tasteoftexas.com/website-terms-of-use/\n",
      "  [29/30] Extracting: https://www.tasteoftexas.com/texas-history-museum/\n",
      "  [30/30] Extracting: http://www.tasteoftexas.com/accessibility-statement/\n",
      "‚úÖ Extracted content from 30 pages\n",
      "\n",
      "ü§ñ Using LLM to extract consolidated business data...\n",
      "  ‚è≥ Sending request to LLM (this may take 10-20 seconds)...\n",
      "‚ùå LLM extraction error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "‚ùå No data extracted. Please check the Google Maps URL and try again.\n",
      "\n",
      "‚úÖ Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# Enhanced Google Colab Notebook with Smart Page Selection\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 1: SETUP AND INSTALLATIONS\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Import all necessary libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# OpenAI for LLM integration\n",
    "import openai\n",
    "\n",
    "# Selenium imports (for dynamic content if needed)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# print(\"‚úÖ All packages installed successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 2: CONFIGURATION AND API SETUP\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Configure API keys and scraping parameters\n",
    "\"\"\"\n",
    "\n",
    "# Set your OpenAI API Key\n",
    "# For Google Colab, use userdata secrets or set directly\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    # Fallback for local environment\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# Scraping configuration\n",
    "MAX_CRAWL_DEPTH = 2  # How deep to crawl internal links\n",
    "MAX_PAGES = 30  # Maximum pages to discover during crawl\n",
    "TOP_PAGES_TO_ANALYZE = 30  # Number of most relevant pages to analyze\n",
    "REQUEST_TIMEOUT = 10  # Seconds\n",
    "RATE_LIMIT_DELAY = 1  # Seconds between requests\n",
    "\n",
    "# Initialize user agent rotator\n",
    "ua = UserAgent()\n",
    "\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 3: GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Extract business website URL from Google Maps link\n",
    "\"\"\"\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    \"\"\"Initialize Selenium WebDriver for dynamic content\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    \"\"\"\n",
    "    Extract the main business website URL from a Google Maps link\n",
    "    \n",
    "    Args:\n",
    "        maps_url (str): Google Maps URL\n",
    "        use_selenium (bool): Whether to use Selenium for dynamic content\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted website URL or None\n",
    "    \"\"\"\n",
    "    print(f\"üîç Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Look for website link with multiple selectors\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"‚úÖ Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Fallback: search in page source\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "            \n",
    "            # Find all links\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"‚úÖ Found website via fallback: {href}\")\n",
    "                        return href\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    # Fallback to requests\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Search for website links in HTML\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"‚úÖ Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with requests: {e}\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 4: WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Crawl all internal pages of the website\n",
    "\"\"\"\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    \"\"\"Check if URL is valid and belongs to the same domain\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        \n",
    "        # Check if same domain\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        \n",
    "        # Skip common file extensions and external resources\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        \n",
    "        # Skip common non-content paths\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    \"\"\"\n",
    "    Crawl website and collect all internal page URLs\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): Starting URL\n",
    "        max_depth (int): Maximum crawl depth\n",
    "        max_pages (int): Maximum number of pages to crawl\n",
    "    \n",
    "    Returns:\n",
    "        list: List of unique page URLs\n",
    "    \"\"\"\n",
    "    print(f\"üï∑Ô∏è Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url, 0)])  # (url, depth)\n",
    "    pages = []\n",
    "    \n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        \n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        \n",
    "        visited.add(current_url)\n",
    "        \n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            \n",
    "            pages.append(current_url)\n",
    "            print(f\"  ‚úì Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            \n",
    "            # Parse page for more links\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                # Remove fragments and query parameters for deduplication\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0]\n",
    "                \n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 5: LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Use LLM to intelligently select the most relevant pages for data extraction\n",
    "\"\"\"\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    \"\"\"\n",
    "    Use LLM to select the most relevant pages for business information extraction\n",
    "    \n",
    "    Args:\n",
    "        page_urls (list): List of all discovered page URLs\n",
    "        top_n (int): Number of top pages to select\n",
    "    \n",
    "    Returns:\n",
    "        list: List of selected page URLs\n",
    "    \"\"\"\n",
    "    print(f\"\\nü§ñ Using LLM to select top {top_n} most relevant pages...\")\n",
    "    \n",
    "    # Create a numbered list of URLs for the LLM\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your task is to identify the most relevant pages \n",
    "from a website that would contain business information such as company details, services, contact information, \n",
    "and business description.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"I have crawled a business website and found {len(page_urls)} pages. \n",
    "Please analyze the URLs and select the TOP {top_n} most relevant pages that would likely contain:\n",
    "- Company name and overview\n",
    "- About/Company information\n",
    "- Services or products offered\n",
    "- Contact information (email, phone, address)\n",
    "- Social media links\n",
    "- Business description\n",
    "\n",
    "Here are all the discovered page URLs:\n",
    "{url_list}\n",
    "\n",
    "Prioritize pages like:\n",
    "- Home/Index pages\n",
    "- About pages\n",
    "- Services/Products pages\n",
    "- Contact pages\n",
    "- Company/Team pages\n",
    "- Portfolio/Work pages\n",
    "\n",
    "Return ONLY a valid JSON array containing the numbers of the selected pages (1-indexed).\n",
    "Example format: [1, 3, 5, 7, 9, 12, 15]\n",
    "\n",
    "Your response must be ONLY the JSON array, nothing else.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        # Parse JSON array\n",
    "        selected_indices = json.loads(content)\n",
    "        \n",
    "        # Convert 1-indexed to 0-indexed and get URLs\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"‚úÖ LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        \n",
    "        return selected_urls\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in LLM page selection: {e}\")\n",
    "        print(\"‚ö†Ô∏è Falling back to heuristic selection...\")\n",
    "        \n",
    "        # Fallback: Use keyword-based heuristic\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company']\n",
    "        scored_pages = []\n",
    "        \n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(1 for keyword in priority_keywords if keyword in url_lower)\n",
    "            # Prioritize shorter URLs (often more important)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        \n",
    "        # Sort by score and take top N\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        \n",
    "        print(f\"‚úÖ Heuristically selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 6: CONTENT EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Extract clean text content from web pages\n",
    "\"\"\"\n",
    "\n",
    "def extract_page_content(url):\n",
    "    \"\"\"\n",
    "    Extract visible text content from a webpage\n",
    "    \n",
    "    Args:\n",
    "        url (str): Page URL\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 7: LLM-POWERED CONSOLIDATED DATA EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Use LLM to extract all business data from combined page content\n",
    "\"\"\"\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url):\n",
    "    \"\"\"\n",
    "    Use LLM to extract consolidated business information from multiple pages\n",
    "    \n",
    "    Args:\n",
    "        page_contents (dict): Dictionary mapping URLs to their text content\n",
    "        main_website_url (str): Main website URL\n",
    "    \n",
    "    Returns:\n",
    "        dict: Consolidated business information\n",
    "    \"\"\"\n",
    "    print(\"\\nü§ñ Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    # Combine all page contents with clear separation\n",
    "    combined_content = \"\"\n",
    "    for url, content in page_contents.items():\n",
    "        # Truncate individual pages if too long\n",
    "        truncated_content = content[:8000] if len(content) > 8000 else content\n",
    "        combined_content += f\"\\n\\n--- PAGE: {url} ---\\n{truncated_content}\\n\"\n",
    "    \n",
    "    # Limit total content size\n",
    "    max_total_chars = 40000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated due to length...]\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Analyze the provided webpage content \n",
    "from multiple pages of a business website and extract comprehensive, accurate business information. \n",
    "Consolidate information from all pages to provide the most complete picture.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze the content from {len(page_contents)} pages of this business website: {main_website_url}\n",
    "\n",
    "Extract and return a SINGLE consolidated JSON object with these exact fields:\n",
    "\n",
    "1. company_name: The official business/company name\n",
    "2. company_main_url: The main website URL ({main_website_url})\n",
    "3. emails: Array of ALL unique email addresses found across all pages\n",
    "4. contact_numbers: Array of ALL unique phone numbers found (include country code if present)\n",
    "5. social_media_links: Array of ALL social media profile URLs (Facebook, Instagram, LinkedIn, Twitter/X, YouTube, TikTok, etc.)\n",
    "6. summary: A comprehensive 3-5 sentence summary describing:\n",
    "   - What the business does\n",
    "   - Main services/products offered\n",
    "   - Key specialties or unique offerings\n",
    "   - Target market/audience if mentioned\n",
    "\n",
    "IMPORTANT:\n",
    "- Use null for fields where no information is found\n",
    "- For arrays, return empty array [] if no items found\n",
    "- Deduplicate all arrays (no repeated emails, phones, or social links)\n",
    "- The summary should be detailed and informative, covering all services/activities mentioned\n",
    "\n",
    "Combined website content:\n",
    "{combined_content}\n",
    "\n",
    "Return ONLY a valid JSON object with these exact field names. No additional text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"  ‚è≥ Sending request to LLM (this may take 10-20 seconds)...\")\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=2000\n",
    "        )\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        \n",
    "        # Remove markdown code blocks if present\n",
    "        if content.startswith('```'):\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        # Parse JSON\n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        # Ensure all required fields exist\n",
    "        required_fields = [\"company_name\", \"company_main_url\", \"emails\", \"contact_numbers\", \n",
    "                          \"social_media_links\", \"summary\"]\n",
    "        for field in required_fields:\n",
    "            if field not in extracted_data:\n",
    "                extracted_data[field] = None if field not in [\"emails\", \"contact_numbers\", \"social_media_links\"] else []\n",
    "        \n",
    "        print(\"‚úÖ Data extraction successful!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM extraction error: {e}\")\n",
    "        return {\n",
    "            \"company_name\": None,\n",
    "            \"company_main_url\": main_website_url,\n",
    "            \"emails\": [],\n",
    "            \"contact_numbers\": [],\n",
    "            \"social_media_links\": [],\n",
    "            \"summary\": None,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 8: MAIN ORCHESTRATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Main function to orchestrate the entire scraping pipeline\n",
    "\"\"\"\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    \"\"\"\n",
    "    Complete pipeline to scrape business data from Google Maps URL\n",
    "    \n",
    "    Args:\n",
    "        google_maps_url (str): Google Maps URL of the business\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (extracted_data dict, business_name str)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Extract website from Google Maps\n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    \n",
    "    if not website_url:\n",
    "        print(\"‚ùå Failed to extract website URL from Google Maps\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nüìç Main Website: {website_url}\\n\")\n",
    "    \n",
    "    # Step 2: Crawl website to discover all pages\n",
    "    all_pages = crawl_website(website_url)\n",
    "    \n",
    "    if not all_pages:\n",
    "        print(\"‚ùå No pages found to scrape\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"\\nüìÑ Discovered {len(all_pages)} total pages\")\n",
    "    \n",
    "    # Step 3: Use LLM to select most relevant pages\n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    \n",
    "    if not selected_pages:\n",
    "        print(\"‚ùå No pages selected for analysis\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 4: Extract content from selected pages\n",
    "    print(f\"\\nüì• Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    \n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        content = extract_page_content(page_url)\n",
    "        if content:\n",
    "            page_contents[page_url] = content\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    print(f\"‚úÖ Extracted content from {len(page_contents)} pages\")\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"‚ùå No content extracted from any page\")\n",
    "        return None, None\n",
    "    \n",
    "    # Step 5: Use LLM to extract consolidated business data\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url)\n",
    "    \n",
    "    # Extract business name for filename\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        # Clean business name for filename\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', business_name)\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return extracted_data, business_name\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 9: DATA SAVING AND DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Save and display extraction results\n",
    "\"\"\"\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    \"\"\"\n",
    "    Save extraction results to JSON file with business name\n",
    "    \n",
    "    Args:\n",
    "        extracted_data (dict): Extracted business data\n",
    "        business_name (str): Business name for filename\n",
    "        all_pages_count (int): Total pages discovered\n",
    "        selected_pages_count (int): Pages analyzed\n",
    "    \n",
    "    Returns:\n",
    "        str: Output filename\n",
    "    \"\"\"\n",
    "    # Create filename based on business name\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    \n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nüíæ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    \"\"\"Display a formatted summary of extracted data\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüè¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"üåê Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nüìß Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   ‚Ä¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nüì± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   ‚Ä¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nüîó Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            # Extract platform name\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link:\n",
    "                platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link:\n",
    "                platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link:\n",
    "                platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link:\n",
    "                platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link:\n",
    "                platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link:\n",
    "                platform = \"TikTok\"\n",
    "            print(f\"   ‚Ä¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nüìù Business Summary:\")\n",
    "    print(f\"   {summary}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# SECTION 10: EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Run the complete scraping pipeline\n",
    "\"\"\"\n",
    "\n",
    "# INPUT: Paste your Google Maps URL here\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/Tvdq57DwjeCz1w4V6\"\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Validate API key\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"‚ùå ERROR: Please set your OPENAI_API_KEY!\")\n",
    "        print(\"In Google Colab: Use Secrets (key icon) to add OPENAI_API_KEY\")\n",
    "        print(\"Locally: Set environment variable or update the code\")\n",
    "    else:\n",
    "        # Store initial counts\n",
    "        all_pages_discovered = 0\n",
    "        pages_analyzed = 0\n",
    "        \n",
    "        # Run the scraper\n",
    "        extracted_data, business_name = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            # Save results with business name in filename\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                TOP_PAGES_TO_ANALYZE\n",
    "            )\n",
    "            \n",
    "            # Display summary\n",
    "            display_summary(extracted_data)\n",
    "            \n",
    "            # Download file in Colab\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(f\"\\n‚¨áÔ∏è Downloading: {filename}\")\n",
    "            except:\n",
    "                print(f\"üìÅ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted. Please check the Google Maps URL and try again.\")\n",
    "\n",
    "print(\"\\n‚úÖ Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "================================================================================\n",
      "üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "üîç Extracting website from Google Maps URL...\n",
      "‚úÖ Found website: http://www.tasteoftexas.com/\n",
      "\n",
      "üìç Main Website: http://www.tasteoftexas.com/\n",
      "\n",
      "üï∑Ô∏è Starting website crawl from: http://www.tasteoftexas.com/\n",
      "  ‚úì Discovered [1/30]: http://www.tasteoftexas.com\n",
      "  ‚úì Discovered [2/30]: http://www.tasteoftexas.com/visit-us\n",
      "  ‚úì Discovered [3/30]: http://www.tasteoftexas.com/compare\n",
      "  ‚úì Discovered [4/30]: http://www.tasteoftexas.com/cart.php\n",
      "  ‚úì Discovered [5/30]: https://www.tasteoftexas.com\n",
      "  ‚úì Discovered [6/30]: https://www.tasteoftexas.com/menu\n",
      "  ‚úì Discovered [7/30]: https://www.tasteoftexas.com/wine-comparison\n",
      "  ‚úì Discovered [8/30]: https://www.tasteoftexas.com/visit-us\n",
      "  ‚úì Discovered [9/30]: https://www.tasteoftexas.com/private-events\n",
      "  ‚úì Discovered [10/30]: https://www.tasteoftexas.com/faqs\n",
      "  ‚úì Discovered [11/30]: https://www.tasteoftexas.com/to-go\n",
      "  ‚úì Discovered [12/30]: https://www.tasteoftexas.com/steak-gift-boxes\n",
      "  ‚úì Discovered [13/30]: https://www.tasteoftexas.com/about-us\n",
      "  ‚úì Discovered [14/30]: https://www.tasteoftexas.com/our-beef\n",
      "  ‚úì Discovered [15/30]: https://www.tasteoftexas.com/texas-history-museum\n",
      "  ‚úì Discovered [16/30]: https://www.tasteoftexas.com/grilling-101\n",
      "  ‚úì Discovered [17/30]: https://www.tasteoftexas.com/signature-dishes\n",
      "  ‚úì Discovered [18/30]: https://www.tasteoftexas.com/newsletters\n",
      "  ‚úì Discovered [19/30]: https://www.tasteoftexas.com/press-accolades\n",
      "  ‚úì Discovered [20/30]: https://www.tasteoftexas.com/school-tours\n",
      "  ‚úì Discovered [21/30]: https://www.tasteoftexas.com/contact-us\n",
      "  ‚úì Discovered [22/30]: http://www.tasteoftexas.com/holiday\n",
      "  ‚úì Discovered [23/30]: http://www.tasteoftexas.com/restaurant\n",
      "  ‚úì Discovered [24/30]: http://www.tasteoftexas.com/shop\n",
      "  ‚úì Discovered [25/30]: http://www.tasteoftexas.com/refunds-and-returns\n",
      "  ‚úì Discovered [26/30]: http://www.tasteoftexas.com/website-terms-of-use\n",
      "  ‚úì Discovered [27/30]: http://www.tasteoftexas.com/privacy-policy\n",
      "  ‚úì Discovered [28/30]: http://www.tasteoftexas.com/accessibility-statement\n",
      "  ‚úì Discovered [29/30]: http://www.tasteoftexas.com/menu\n",
      "  ‚úì Discovered [30/30]: http://www.tasteoftexas.com/signature-dishes\n",
      "‚úÖ Crawl complete! Discovered 30 pages\n",
      "\n",
      "ü§ñ Using LLM to select top 15 most relevant pages...\n",
      "‚ö†Ô∏è LLM selection error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üîÑ Using heuristic fallback...\n",
      "‚úÖ Heuristic selected 15 pages\n",
      "\n",
      "üì• Extracting content from 15 selected pages...\n",
      "  [1/15] Extracting: https://www.tasteoftexas.com/about-us\n",
      "      ‚úì Extracted 3880 characters\n",
      "  [2/15] Extracting: https://www.tasteoftexas.com/contact-us\n",
      "      ‚úì Extracted 267 characters\n",
      "  [3/15] Extracting: http://www.tasteoftexas.com\n",
      "      ‚úì Extracted 134 characters\n",
      "  [4/15] Extracting: https://www.tasteoftexas.com\n",
      "      ‚úì Extracted 134 characters\n",
      "  [5/15] Extracting: http://www.tasteoftexas.com/shop\n",
      "      ‚úì Extracted 1144 characters\n",
      "  [6/15] Extracting: http://www.tasteoftexas.com/menu\n",
      "      ‚úì Extracted 6687 characters\n",
      "  [7/15] Extracting: https://www.tasteoftexas.com/menu\n",
      "      ‚úì Extracted 6687 characters\n",
      "  [8/15] Extracting: https://www.tasteoftexas.com/faqs\n",
      "      ‚úì Extracted 122 characters\n",
      "  [9/15] Extracting: https://www.tasteoftexas.com/to-go\n",
      "      ‚úì Extracted 397 characters\n",
      "  [10/15] Extracting: http://www.tasteoftexas.com/compare\n",
      "      ‚úì Extracted 134 characters\n",
      "  [11/15] Extracting: http://www.tasteoftexas.com/holiday\n",
      "      ‚úì Extracted 1932 characters\n",
      "  [12/15] Extracting: http://www.tasteoftexas.com/visit-us\n",
      "      ‚úì Extracted 1213 characters\n",
      "  [13/15] Extracting: http://www.tasteoftexas.com/cart.php\n",
      "      ‚úì Extracted 133 characters\n",
      "  [14/15] Extracting: https://www.tasteoftexas.com/visit-us\n",
      "      ‚úì Extracted 1213 characters\n",
      "  [15/15] Extracting: https://www.tasteoftexas.com/our-beef\n",
      "      ‚úì Extracted 1855 characters\n",
      "\n",
      "ü§ñ Using LLM to extract consolidated business data...\n",
      "üìä Total content length: 25339 characters\n",
      "üîÑ Calling OpenAI API...\n",
      "‚ùå LLM extraction error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üîÑ Using enhanced fallback extraction...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üíæ Results saved to: tasteoftexas_20251115_000103.json\n",
      "\n",
      "================================================================================\n",
      "üìä EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üè¢ Company Name: Tasteoftexas\n",
      "üåê Website: http://www.tasteoftexas.com/\n",
      "\n",
      "üìß Emails (1 found):\n",
      "   ‚Ä¢ support@tasteoftexas.com\n",
      "\n",
      "üì± Phone Numbers (2 found):\n",
      "   ‚Ä¢ 713-932-6901\n",
      "   ‚Ä¢ 713.932.6901\n",
      "\n",
      "üîó Social Media (0 links):\n",
      "   None found\n",
      "\n",
      "üìù Business Summary:\n",
      "   Business information extracted using automated fallback method. Manual verification recommended for accuracy.\n",
      "\n",
      "‚öôÔ∏è Extraction Method: enhanced_regex_fallback\n",
      "\n",
      "================================================================================\n",
      "üìÅ File saved locally: tasteoftexas_20251115_000103.json\n",
      "\n",
      "‚úÖ Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# FIXED VERSION - Improved LLM extraction and fallback methods\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import openai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "MAX_CRAWL_DEPTH = 2\n",
    "MAX_PAGES = 30\n",
    "TOP_PAGES_TO_ANALYZE = 15  # Optimized for better token management\n",
    "REQUEST_TIMEOUT = 10\n",
    "RATE_LIMIT_DELAY = 1\n",
    "\n",
    "ua = UserAgent()\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    print(f\"üîç Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            time.sleep(3)\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"‚úÖ Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"‚úÖ Found website via fallback: {href}\")\n",
    "                        return href\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"‚úÖ Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with requests: {e}\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    print(f\"üï∑Ô∏è Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url.rstrip('/'), 0)])\n",
    "    pages = []\n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        current_url = current_url.rstrip('/')\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            pages.append(current_url)\n",
    "            print(f\"  ‚úì Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0].rstrip('/')\n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    print(f\"\\nü§ñ Using LLM to select top {top_n} most relevant pages...\")\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your job is to identify the most relevant pages that contain business information like company details, contact info, services, and about information.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these {len(page_urls)} URLs and select the top {top_n} most relevant pages for extracting company information (like contact details, about us, services, etc.).\n",
    "\n",
    "URLs:\n",
    "{url_list}\n",
    "\n",
    "Return ONLY a valid JSON array of numbers (1-indexed positions) like: [1, 3, 5, 7, 9]\n",
    "Do not include any other text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM raw response: {content[:200]}\")\n",
    "        \n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        selected_indices = json.loads(content)\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"‚úÖ LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        return selected_urls[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM selection error: {e}\")\n",
    "        print(\"üîÑ Using heuristic fallback...\")\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company', 'catering']\n",
    "        scored_pages = []\n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(2 for keyword in priority_keywords if keyword in url_lower)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        print(f\"‚úÖ Heuristic selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT EXTRACTOR - IMPROVED\n",
    "# ============================================================================\n",
    "\n",
    "def extract_page_content(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text with better formatting\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED LLM EXTRACTION WITH BETTER ERROR HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url):\n",
    "    print(\"\\nü§ñ Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    # Prepare content with smart truncation\n",
    "    combined_content = \"\"\n",
    "    max_chars_per_page = 6000  # Increased slightly for better context\n",
    "    \n",
    "    for url, content in page_contents.items():\n",
    "        truncated_content = content[:max_chars_per_page] if len(content) > max_chars_per_page else content\n",
    "        combined_content += f\"\\n\\n=== PAGE: {url} ===\\n{truncated_content}\\n\"\n",
    "    \n",
    "    # Limit total content to stay within token limits\n",
    "    max_total_chars = 40000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated for length]\"\n",
    "    \n",
    "    print(f\"üìä Total content length: {len(combined_content)} characters\")\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Extract comprehensive business information from website content and return it as valid JSON. Be thorough in finding all contact details, emails, phone numbers, and social media links.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Extract ALL business information from these web pages for: {main_website_url}\n",
    "\n",
    "Website Content:\n",
    "{combined_content}\n",
    "\n",
    "Return ONLY a valid JSON object (no markdown, no explanations) with this structure:\n",
    "{{\n",
    "  \"company_name\": \"full company name found on the website\",\n",
    "  \"company_main_url\": \"{main_website_url}\",\n",
    "  \"emails\": [\"list all unique email addresses found\"],\n",
    "  \"contact_numbers\": [\"list all phone numbers with country codes if available\"],\n",
    "  \"social_media_links\": [\"list all social media URLs - Facebook, Instagram, LinkedIn, Twitter, YouTube, etc.\"],\n",
    "  \"summary\": \"Write a comprehensive 5-10 line summary describing what the company does, their services/products, target audience, and unique value proposition based on ALL the content analyzed\"\n",
    "}}\n",
    "\n",
    "Important:\n",
    "- Find ALL emails and phone numbers across all pages\n",
    "- Include country codes in phone numbers when visible\n",
    "- Extract complete social media URLs\n",
    "- Write a detailed, informative summary that captures the essence of the business\n",
    "- Use empty arrays [] if nothing found, not null\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"üîÑ Calling OpenAI API...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=3000  # Increased for more detailed extraction\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM response received ({len(content)} chars)\")\n",
    "        \n",
    "        # Enhanced JSON extraction\n",
    "        json_str = content\n",
    "        \n",
    "        # Remove markdown code blocks\n",
    "        if '```json' in json_str:\n",
    "            json_str = json_str.split('```json')[1].split('```')[0].strip()\n",
    "        elif '```' in json_str:\n",
    "            json_str = json_str.split('```')[1].split('```')[0].strip()\n",
    "        \n",
    "        # Try to find JSON object if surrounded by text\n",
    "        if not json_str.startswith('{'):\n",
    "            match = re.search(r'\\{.*\\}', json_str, re.DOTALL)\n",
    "            if match:\n",
    "                json_str = match.group(0)\n",
    "        \n",
    "        # Parse JSON\n",
    "        extracted_data = json.loads(json_str)\n",
    "        \n",
    "        # Validate and clean data\n",
    "        extracted_data = validate_and_clean_data(extracted_data, main_website_url)\n",
    "        \n",
    "        print(\"‚úÖ Data extraction successful via LLM!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"üîç Attempting to parse response: {content[:300]}...\")\n",
    "        return create_fallback_data(main_website_url, page_contents)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM extraction error: {e}\")\n",
    "        return create_fallback_data(main_website_url, page_contents)\n",
    "\n",
    "def validate_and_clean_data(data, main_url):\n",
    "    \"\"\"Validate and clean extracted data\"\"\"\n",
    "    cleaned = {\n",
    "        \"company_name\": data.get(\"company_name\") or urlparse(main_url).netloc.replace('www.', '').split('.')[0].title(),\n",
    "        \"company_main_url\": main_url,\n",
    "        \"emails\": [],\n",
    "        \"contact_numbers\": [],\n",
    "        \"social_media_links\": [],\n",
    "        \"summary\": data.get(\"summary\") or \"No summary available\"\n",
    "    }\n",
    "    \n",
    "    # Clean emails\n",
    "    if data.get(\"emails\") and isinstance(data[\"emails\"], list):\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        cleaned[\"emails\"] = [e.strip() for e in data[\"emails\"] if re.match(email_pattern, e.strip())]\n",
    "    \n",
    "    # Clean phone numbers\n",
    "    if data.get(\"contact_numbers\") and isinstance(data[\"contact_numbers\"], list):\n",
    "        cleaned[\"contact_numbers\"] = [p.strip() for p in data[\"contact_numbers\"] if p and len(str(p).strip()) > 5]\n",
    "    \n",
    "    # Clean social media links\n",
    "    if data.get(\"social_media_links\") and isinstance(data[\"social_media_links\"], list):\n",
    "        social_domains = ['facebook.com', 'instagram.com', 'linkedin.com', 'twitter.com', 'x.com', \n",
    "                         'youtube.com', 'tiktok.com', 'pinterest.com']\n",
    "        cleaned[\"social_media_links\"] = [\n",
    "            s.strip() for s in data[\"social_media_links\"] \n",
    "            if s and any(domain in s.lower() for domain in social_domains)\n",
    "        ]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED FALLBACK DATA EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_fallback_data(main_website_url, page_contents):\n",
    "    \"\"\"Enhanced fallback extraction with better regex patterns\"\"\"\n",
    "    print(\"üîÑ Using enhanced fallback extraction...\")\n",
    "    \n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    \n",
    "    # Extract emails with better pattern\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "    emails = list(set(re.findall(email_pattern, all_text)))\n",
    "    emails = [e for e in emails if not e.endswith(('.png', '.jpg', '.gif'))]\n",
    "    \n",
    "    # Extract phone numbers with improved patterns\n",
    "    phone_patterns = [\n",
    "        r'\\+\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}',  # International\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # US format\n",
    "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # Simple format\n",
    "    ]\n",
    "    \n",
    "    phones = set()\n",
    "    for pattern in phone_patterns:\n",
    "        found = re.findall(pattern, all_text)\n",
    "        for phone in found:\n",
    "            # Clean up phone number\n",
    "            cleaned = re.sub(r'[^\\d+()-]', '', phone)\n",
    "            if len(re.sub(r'[^\\d]', '', cleaned)) >= 10:  # At least 10 digits\n",
    "                phones.add(phone.strip())\n",
    "    \n",
    "    # Extract social media links\n",
    "    social_patterns = {\n",
    "        'facebook.com': r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._-]+',\n",
    "        'instagram.com': r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._-]+',\n",
    "        'linkedin.com': r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._-]+',\n",
    "        'twitter.com': r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._-]+',\n",
    "        'youtube.com': r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user)/[a-zA-Z0-9._-]+',\n",
    "        'tiktok.com': r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._-]+'\n",
    "    }\n",
    "    \n",
    "    socials = []\n",
    "    for platform, pattern in social_patterns.items():\n",
    "        found = re.findall(pattern, all_text, re.IGNORECASE)\n",
    "        socials.extend(found)\n",
    "    socials = list(set(socials))\n",
    "    \n",
    "    # Generate better company name\n",
    "    company_name = urlparse(main_website_url).netloc.replace('www.', '').split('.')[0]\n",
    "    company_name = ' '.join(word.capitalize() for word in re.split(r'[-_]', company_name))\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_main_url\": main_website_url,\n",
    "        \"emails\": sorted(list(set(emails)))[:15],\n",
    "        \"contact_numbers\": sorted(list(phones))[:15],\n",
    "        \"social_media_links\": sorted(socials),\n",
    "        \"summary\": \"Business information extracted using automated fallback method. Manual verification recommended for accuracy.\",\n",
    "        \"extraction_method\": \"enhanced_regex_fallback\"\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    if not website_url:\n",
    "        print(\"‚ùå Failed to extract website URL from Google Maps\")\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    parsed = urlparse(website_url)\n",
    "    website_url = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    print(f\"\\nüìç Main Website: {website_url}\\n\")\n",
    "    \n",
    "    all_pages = crawl_website(website_url)\n",
    "    if not all_pages:\n",
    "        print(\"‚ùå No pages found to scrape\")\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    if not selected_pages:\n",
    "        print(\"‚ùå No pages selected for analysis\")\n",
    "        return None, None, len(all_pages), 0\n",
    "\n",
    "    print(f\"\\nüì• Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        content = extract_page_content(page_url)\n",
    "        if content:\n",
    "            page_contents[page_url] = content\n",
    "            print(f\"      ‚úì Extracted {len(content)} characters\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"‚ùå No content extracted from any page\")\n",
    "        return None, None, len(all_pages), len(selected_pages)\n",
    "\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url)\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', str(business_name))\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return extracted_data, business_name, len(all_pages), len(page_contents)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nüíæ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüè¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"üåê Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nüìß Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   ‚Ä¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nüì± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   ‚Ä¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nüîó Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link: platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link: platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link: platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link: platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link: platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link: platform = \"TikTok\"\n",
    "            print(f\"   ‚Ä¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nüìù Business Summary:\\n   {summary}\")\n",
    "    \n",
    "    if 'extraction_method' in extracted_data:\n",
    "        print(f\"\\n‚öôÔ∏è Extraction Method: {extracted_data['extraction_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/Tvdq57DwjeCz1w4V6\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"‚ùå ERROR: Please set your OPENAI_API_KEY!\")\n",
    "    else:\n",
    "        extracted_data, business_name, all_pages_discovered, pages_analyzed = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                pages_analyzed\n",
    "            )\n",
    "            display_summary(extracted_data)\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(\"üì• File download started!\")\n",
    "            except:\n",
    "                print(f\"üìÅ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted. Check the URL and API key, then try again.\")\n",
    "\n",
    "print(\"\\n‚úÖ Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "================================================================================\n",
      "üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "üîç Extracting website from Google Maps URL...\n",
      "‚úÖ Found website: http://www.tasteoftexas.com/\n",
      "\n",
      "üìç Main Website: http://www.tasteoftexas.com/\n",
      "\n",
      "üï∑Ô∏è Starting website crawl from: http://www.tasteoftexas.com/\n",
      "  ‚úì Discovered [1/15]: http://www.tasteoftexas.com\n",
      "  ‚úì Discovered [2/15]: http://www.tasteoftexas.com/visit-us\n",
      "  ‚úì Discovered [3/15]: http://www.tasteoftexas.com/compare\n",
      "  ‚úì Discovered [4/15]: http://www.tasteoftexas.com/cart.php\n",
      "  ‚úì Discovered [5/15]: https://www.tasteoftexas.com\n",
      "  ‚úì Discovered [6/15]: https://www.tasteoftexas.com/menu\n",
      "  ‚úì Discovered [7/15]: https://www.tasteoftexas.com/wine-comparison\n",
      "  ‚úì Discovered [8/15]: https://www.tasteoftexas.com/visit-us\n",
      "  ‚úì Discovered [9/15]: https://www.tasteoftexas.com/private-events\n",
      "  ‚úì Discovered [10/15]: https://www.tasteoftexas.com/faqs\n",
      "  ‚úì Discovered [11/15]: https://www.tasteoftexas.com/to-go\n",
      "  ‚úì Discovered [12/15]: https://www.tasteoftexas.com/steak-gift-boxes\n",
      "  ‚úì Discovered [13/15]: https://www.tasteoftexas.com/about-us\n",
      "  ‚úì Discovered [14/15]: https://www.tasteoftexas.com/our-beef\n",
      "  ‚úì Discovered [15/15]: https://www.tasteoftexas.com/texas-history-museum\n",
      "‚úÖ Crawl complete! Discovered 15 pages\n",
      "\n",
      "ü§ñ Using LLM to select top 10 most relevant pages...\n",
      "‚ö†Ô∏è LLM selection error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "üîÑ Using heuristic fallback...\n",
      "‚úÖ Heuristic selected 10 pages\n",
      "\n",
      "üì• Extracting content from 10 selected pages...\n",
      "  [1/10] Extracting: https://www.tasteoftexas.com/about-us\n",
      "      ‚úì Extracted 3880 characters\n",
      "  [2/10] Extracting: http://www.tasteoftexas.com\n",
      "      ‚úì Extracted 134 characters\n",
      "  [3/10] Extracting: https://www.tasteoftexas.com\n",
      "      ‚úì Extracted 134 characters\n",
      "  [4/10] Extracting: https://www.tasteoftexas.com/menu\n",
      "      ‚úì Extracted 6687 characters\n",
      "  [5/10] Extracting: https://www.tasteoftexas.com/faqs\n",
      "      ‚úì Extracted 122 characters\n",
      "  [6/10] Extracting: https://www.tasteoftexas.com/to-go\n",
      "      ‚úì Extracted 397 characters\n",
      "  [7/10] Extracting: http://www.tasteoftexas.com/compare\n",
      "      ‚úì Extracted 134 characters\n",
      "  [8/10] Extracting: http://www.tasteoftexas.com/visit-us\n",
      "      ‚úì Extracted 1213 characters\n",
      "  [9/10] Extracting: http://www.tasteoftexas.com/cart.php\n",
      "      ‚úì Extracted 133 characters\n",
      "  [10/10] Extracting: https://www.tasteoftexas.com/visit-us\n",
      "      ‚úì Extracted 1213 characters\n",
      "\n",
      "ü§ñ Using LLM to extract consolidated business data...\n",
      "üìä Total content length: 14563 characters\n",
      "üîÑ Calling OpenAI API...\n",
      "‚ùå LLM extraction error: AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "üîç Traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ABC\\AppData\\Local\\Temp\\ipykernel_33416\\2678400174.py\", line 309, in extract_business_data_with_llm\n",
      "    response = openai.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1156, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "\n",
      "üîÑ Using enhanced fallback extraction...\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üíæ Results saved to: tasteoftexas_20251115_001653.json\n",
      "\n",
      "================================================================================\n",
      "üìä EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üè¢ Company Name: Tasteoftexas\n",
      "üåê Website: http://www.tasteoftexas.com/\n",
      "\n",
      "üìß Emails (0 found):\n",
      "   None found\n",
      "\n",
      "üì± Phone Numbers (1 found):\n",
      "   ‚Ä¢ 713.932.6901\n",
      "\n",
      "üîó Social Media (0 links):\n",
      "   None found\n",
      "\n",
      "üìù Business Summary:\n",
      "   Business information extracted using automated fallback method. Manual verification recommended for accuracy.\n",
      "\n",
      "‚öôÔ∏è Extraction Method: enhanced_regex_fallback\n",
      "\n",
      "================================================================================\n",
      "üìÅ File saved locally: tasteoftexas_20251115_001653.json\n",
      "\n",
      "‚úÖ Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# FIXED VERSION - Improved LLM extraction and fallback methods\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import openai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "MAX_CRAWL_DEPTH = 2\n",
    "MAX_PAGES = 15\n",
    "TOP_PAGES_TO_ANALYZE = 10  # Optimized for better token management\n",
    "REQUEST_TIMEOUT = 10\n",
    "RATE_LIMIT_DELAY = 1\n",
    "\n",
    "ua = UserAgent()\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    print(f\"üîç Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            time.sleep(3)\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"‚úÖ Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"‚úÖ Found website via fallback: {href}\")\n",
    "                        return href\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"‚úÖ Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with requests: {e}\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    print(f\"üï∑Ô∏è Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url.rstrip('/'), 0)])\n",
    "    pages = []\n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        current_url = current_url.rstrip('/')\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            pages.append(current_url)\n",
    "            print(f\"  ‚úì Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0].rstrip('/')\n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    print(f\"\\nü§ñ Using LLM to select top {top_n} most relevant pages...\")\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your job is to identify the most relevant pages that contain business information like company details, contact info, services, and about information.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these {len(page_urls)} URLs and select the top {top_n} most relevant pages for extracting company information (like contact details, about us, services, etc.).\n",
    "\n",
    "URLs:\n",
    "{url_list}\n",
    "\n",
    "Return ONLY a valid JSON array of numbers (1-indexed positions) like: [1, 3, 5, 7, 9]\n",
    "Do not include any other text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM raw response: {content[:200]}\")\n",
    "        \n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        selected_indices = json.loads(content)\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"‚úÖ LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        return selected_urls[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM selection error: {e}\")\n",
    "        print(\"üîÑ Using heuristic fallback...\")\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company', 'catering']\n",
    "        scored_pages = []\n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(2 for keyword in priority_keywords if keyword in url_lower)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        print(f\"‚úÖ Heuristic selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT EXTRACTOR - IMPROVED\n",
    "# ============================================================================\n",
    "\n",
    "def extract_page_content(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for script in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Extract text with better formatting\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting content from {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED LLM EXTRACTION WITH BETTER ERROR HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url):\n",
    "    print(\"\\nü§ñ Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    # Prepare content with smart truncation\n",
    "    combined_content = \"\"\n",
    "    max_chars_per_page = 8000  # Increased for better context\n",
    "    \n",
    "    for url, content in page_contents.items():\n",
    "        truncated_content = content[:max_chars_per_page] if len(content) > max_chars_per_page else content\n",
    "        combined_content += f\"\\n\\n=== PAGE: {url} ===\\n{truncated_content}\\n\"\n",
    "    \n",
    "    # Limit total content to stay within token limits (more generous)\n",
    "    max_total_chars = 60000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated for length]\"\n",
    "    \n",
    "    print(f\"üìä Total content length: {len(combined_content)} characters\")\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Extract comprehensive business information from website content and return ONLY valid JSON with no additional text, markdown formatting, or explanations.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze the following website content and extract ALL business information for: {main_website_url}\n",
    "\n",
    "{combined_content}\n",
    "\n",
    "Extract and return ONLY a JSON object with this EXACT structure (no markdown, no text before or after):\n",
    "{{\n",
    "  \"company_name\": \"Full official company name\",\n",
    "  \"company_main_url\": \"{main_website_url}\",\n",
    "  \"emails\": [\"email1@domain.com\", \"email2@domain.com\"],\n",
    "  \"contact_numbers\": [\"+1-234-567-8900\", \"234-567-8900\"],\n",
    "  \"social_media_links\": [\"https://facebook.com/page\", \"https://instagram.com/profile\"],\n",
    "  \"summary\": \"A comprehensive 5-10 line summary describing: what the company does, main services/products offered, target audience/market, unique selling points, company values or mission, and any notable achievements or specializations. Base this on ALL analyzed pages.\"\n",
    "}}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Find ALL emails, phone numbers, and social media links across all pages\n",
    "2. Look for social media links in footer, header, contact pages, and inline content\n",
    "3. Include full URLs for social media (Facebook, Instagram, Twitter/X, LinkedIn, YouTube, TikTok, Pinterest)\n",
    "4. Write a detailed, informative summary that truly captures what the business does\n",
    "5. Use empty arrays [] for missing data, never null\n",
    "6. Return ONLY the JSON object - no explanation, no markdown backticks, no preamble\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"üîÑ Calling OpenAI API...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000,  # Increased significantly\n",
    "            response_format={\"type\": \"json_object\"}  # Force JSON response\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM response received ({len(content)} chars)\")\n",
    "        print(f\"üìù First 500 chars of response: {content[:500]}\")\n",
    "        \n",
    "        # Parse JSON directly (response_format ensures it's JSON)\n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        # Validate and clean data\n",
    "        extracted_data = validate_and_clean_data(extracted_data, main_website_url)\n",
    "        \n",
    "        # Additional check - if data looks empty, try fallback\n",
    "        if (not extracted_data.get('emails') and \n",
    "            not extracted_data.get('contact_numbers') and \n",
    "            not extracted_data.get('social_media_links')):\n",
    "            print(\"‚ö†Ô∏è LLM extraction returned empty data, trying fallback...\")\n",
    "            return create_fallback_data(main_website_url, page_contents)\n",
    "        \n",
    "        print(\"‚úÖ Data extraction successful via LLM!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"üîç Raw response: {content[:500]}\")\n",
    "        return create_fallback_data(main_website_url, page_contents)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM extraction error: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        print(f\"üîç Traceback: {traceback.format_exc()}\")\n",
    "        return create_fallback_data(main_website_url, page_contents)\n",
    "\n",
    "def validate_and_clean_data(data, main_url):\n",
    "    \"\"\"Validate and clean extracted data\"\"\"\n",
    "    cleaned = {\n",
    "        \"company_name\": data.get(\"company_name\") or urlparse(main_url).netloc.replace('www.', '').split('.')[0].title(),\n",
    "        \"company_main_url\": main_url,\n",
    "        \"emails\": [],\n",
    "        \"contact_numbers\": [],\n",
    "        \"social_media_links\": [],\n",
    "        \"summary\": data.get(\"summary\") or \"No summary available\"\n",
    "    }\n",
    "    \n",
    "    # Clean emails\n",
    "    if data.get(\"emails\") and isinstance(data[\"emails\"], list):\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        cleaned[\"emails\"] = [e.strip() for e in data[\"emails\"] if re.match(email_pattern, e.strip())]\n",
    "    \n",
    "    # Clean phone numbers\n",
    "    if data.get(\"contact_numbers\") and isinstance(data[\"contact_numbers\"], list):\n",
    "        cleaned[\"contact_numbers\"] = [p.strip() for p in data[\"contact_numbers\"] if p and len(str(p).strip()) > 5]\n",
    "    \n",
    "    # Clean social media links\n",
    "    if data.get(\"social_media_links\") and isinstance(data[\"social_media_links\"], list):\n",
    "        social_domains = ['facebook.com', 'instagram.com', 'linkedin.com', 'twitter.com', 'x.com', \n",
    "                         'youtube.com', 'tiktok.com', 'pinterest.com']\n",
    "        cleaned[\"social_media_links\"] = [\n",
    "            s.strip() for s in data[\"social_media_links\"] \n",
    "            if s and any(domain in s.lower() for domain in social_domains)\n",
    "        ]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED FALLBACK DATA EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_fallback_data(main_website_url, page_contents):\n",
    "    \"\"\"Enhanced fallback extraction with better regex patterns\"\"\"\n",
    "    print(\"üîÑ Using enhanced fallback extraction...\")\n",
    "    \n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    \n",
    "    # Extract emails with better pattern\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "    emails = list(set(re.findall(email_pattern, all_text)))\n",
    "    emails = [e for e in emails if not e.endswith(('.png', '.jpg', '.gif'))]\n",
    "    \n",
    "    # Extract phone numbers with improved patterns\n",
    "    phone_patterns = [\n",
    "        r'\\+\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}',  # International\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # US format\n",
    "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',  # Simple format\n",
    "    ]\n",
    "    \n",
    "    phones = set()\n",
    "    for pattern in phone_patterns:\n",
    "        found = re.findall(pattern, all_text)\n",
    "        for phone in found:\n",
    "            # Clean up phone number\n",
    "            cleaned = re.sub(r'[^\\d+()-]', '', phone)\n",
    "            if len(re.sub(r'[^\\d]', '', cleaned)) >= 10:  # At least 10 digits\n",
    "                phones.add(phone.strip())\n",
    "    \n",
    "    # Extract social media links\n",
    "    social_patterns = {\n",
    "        'facebook.com': r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._-]+',\n",
    "        'instagram.com': r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._-]+',\n",
    "        'linkedin.com': r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._-]+',\n",
    "        'twitter.com': r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._-]+',\n",
    "        'youtube.com': r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user)/[a-zA-Z0-9._-]+',\n",
    "        'tiktok.com': r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._-]+'\n",
    "    }\n",
    "    \n",
    "    socials = []\n",
    "    for platform, pattern in social_patterns.items():\n",
    "        found = re.findall(pattern, all_text, re.IGNORECASE)\n",
    "        socials.extend(found)\n",
    "    socials = list(set(socials))\n",
    "    \n",
    "    # Generate better company name\n",
    "    company_name = urlparse(main_website_url).netloc.replace('www.', '').split('.')[0]\n",
    "    company_name = ' '.join(word.capitalize() for word in re.split(r'[-_]', company_name))\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_main_url\": main_website_url,\n",
    "        \"emails\": sorted(list(set(emails)))[:15],\n",
    "        \"contact_numbers\": sorted(list(phones))[:15],\n",
    "        \"social_media_links\": sorted(socials),\n",
    "        \"summary\": \"Business information extracted using automated fallback method. Manual verification recommended for accuracy.\",\n",
    "        \"extraction_method\": \"enhanced_regex_fallback\"\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    if not website_url:\n",
    "        print(\"‚ùå Failed to extract website URL from Google Maps\")\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    parsed = urlparse(website_url)\n",
    "    website_url = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    print(f\"\\nüìç Main Website: {website_url}\\n\")\n",
    "    \n",
    "    all_pages = crawl_website(website_url)\n",
    "    if not all_pages:\n",
    "        print(\"‚ùå No pages found to scrape\")\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    if not selected_pages:\n",
    "        print(\"‚ùå No pages selected for analysis\")\n",
    "        return None, None, len(all_pages), 0\n",
    "\n",
    "    print(f\"\\nüì• Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        content = extract_page_content(page_url)\n",
    "        if content:\n",
    "            page_contents[page_url] = content\n",
    "            print(f\"      ‚úì Extracted {len(content)} characters\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"‚ùå No content extracted from any page\")\n",
    "        return None, None, len(all_pages), len(selected_pages)\n",
    "\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url)\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', str(business_name))\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return extracted_data, business_name, len(all_pages), len(page_contents)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND DISPLAY RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nüíæ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüè¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"üåê Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nüìß Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   ‚Ä¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nüì± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   ‚Ä¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nüîó Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link: platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link: platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link: platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link: platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link: platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link: platform = \"TikTok\"\n",
    "            print(f\"   ‚Ä¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nüìù Business Summary:\\n   {summary}\")\n",
    "    \n",
    "    if 'extraction_method' in extracted_data:\n",
    "        print(f\"\\n‚öôÔ∏è Extraction Method: {extracted_data['extraction_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/Tvdq57DwjeCz1w4V6\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"‚ùå ERROR: Please set your OPENAI_API_KEY!\")\n",
    "    else:\n",
    "        extracted_data, business_name, all_pages_discovered, pages_analyzed = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                pages_analyzed\n",
    "            )\n",
    "            display_summary(extracted_data)\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(\"üì• File download started!\")\n",
    "            except:\n",
    "                print(f\"üìÅ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted. Check the URL and API key, then try again.\")\n",
    "\n",
    "print(\"\\n‚úÖ Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully!\n",
      "================================================================================\n",
      "üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "üîç Extracting website from Google Maps URL...\n",
      "‚úÖ Found website: https://eatmila.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb-listing\n",
      "\n",
      "üìç Main Website: https://eatmila.com/\n",
      "\n",
      "üï∑Ô∏è Starting website crawl from: https://eatmila.com/\n",
      "  ‚úì Discovered [1/30]: https://eatmila.com\n",
      "  ‚úì Discovered [2/30]: https://eatmila.com/products/classic-pork-xiao-long-bao\n",
      "  ‚úì Discovered [3/30]: https://eatmila.com/products/potstickers\n",
      "  ‚úì Discovered [4/30]: https://eatmila.com/products/chocolate-black-sesame-lava-dumplings\n",
      "  ‚úì Discovered [5/30]: https://eatmila.com/products/noodles\n",
      "  ‚úì Discovered [6/30]: https://eatmila.com/products/braised-beef-noodles\n",
      "  ‚úì Discovered [7/30]: https://eatmila.com/collections/national\n",
      "  ‚úì Discovered [8/30]: https://eatmila.com/products/mila-signature-bundle \n",
      "  ‚úì Discovered [9/30]: https://eatmila.com/products/hulu-sauce-jars\n",
      "  ‚úì Discovered [10/30]: https://eatmila.com/products/bamboosteamer\n",
      "  ‚úì Discovered [11/30]: https://eatmila.com/products/dumpling-dipping-bowl-chopstick-set\n",
      "  ‚úì Discovered [12/30]: https://eatmila.com/products/rose-lychee-ice-cream\n",
      "  ‚úì Discovered [13/30]: https://eatmila.com/pages/build-your-own-bundle\n",
      "  ‚úì Discovered [14/30]: https://eatmila.com/pages/general-store-locator\n",
      "  ‚úì Discovered [15/30]: https://eatmila.com/pages/founders\n",
      "  ‚úì Discovered [16/30]: https://eatmila.com/pages/delivery\n",
      "  ‚úì Discovered [17/30]: https://eatmila.com/pages/press\n",
      "  ‚úì Discovered [18/30]: https://eatmila.com/pages/faqs\n",
      "  ‚úì Discovered [19/30]: https://eatmila.com/a/account/login\n",
      "  ‚úì Discovered [20/30]: https://eatmila.com/blogs/news\n",
      "  ‚úì Discovered [21/30]: https://eatmila.com/products/soup-dumplings-50-pc-exp\n",
      "  ‚úì Discovered [22/30]: https://eatmila.com/products/pork-and-shrimp-xiao-long-bao\n",
      "  ‚úì Discovered [23/30]: https://eatmila.com/products/savory-chicken-xiao-long-bao\n",
      "  ‚úì Discovered [24/30]: https://eatmila.com/products/pho-beef-xiao-long-bao\n",
      "  ‚úì Discovered [25/30]: https://eatmila.com/products/chinese-noodles-kit-ground-pork\n",
      "  ‚úì Discovered [26/30]: https://eatmila.com/products/ground-pork-spicy-dan-dan-noodles\n",
      "  ‚úì Discovered [27/30]: https://eatmila.com/products/impossible-plant-based-spicy-dan-dan-noodle\n",
      "  ‚úì Discovered [28/30]: https://eatmila.com/products/caramelized-scallion-oil-noodle\n",
      "  ‚úì Discovered [29/30]: https://eatmila.com/products/ground-pork-sweet-and-savory-noodle\n",
      "  ‚úì Discovered [30/30]: https://eatmila.com/products/chili-crunch-dipping-sauce\n",
      "‚úÖ Crawl complete! Discovered 30 pages\n",
      "\n",
      "ü§ñ Using LLM to select top 10 most relevant pages...\n",
      "‚ö†Ô∏è LLM selection error: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "üîÑ Using heuristic fallback...\n",
      "‚úÖ Heuristic selected 10 pages\n",
      "\n",
      "üì• Extracting content from 10 selected pages...\n",
      "  [1/10] Extracting: https://eatmila.com/products/bamboosteamer\n",
      "      ‚úì Extracted 3520 characters\n",
      "  [2/10] Extracting: https://eatmila.com/products/noodles\n",
      "      ‚úì Extracted 7039 characters\n",
      "  [3/10] Extracting: https://eatmila.com/products/potstickers\n",
      "      ‚úì Extracted 3613 characters\n",
      "  [4/10] Extracting: https://eatmila.com/products/hulu-sauce-jars\n",
      "      ‚úì Extracted 4263 characters\n",
      "  [5/10] Extracting: https://eatmila.com/products/braised-beef-noodles\n",
      "      ‚úì Extracted 7570 characters\n",
      "  [6/10] Extracting: https://eatmila.com/products/rose-lychee-ice-cream\n",
      "      ‚úì Extracted 4787 characters\n",
      "  [7/10] Extracting: https://eatmila.com/products/mila-signature-bundle \n",
      "      ‚úì Extracted 4283 characters\n",
      "  [8/10] Extracting: https://eatmila.com/products/pho-beef-xiao-long-bao\n",
      "      ‚úì Extracted 6186 characters\n",
      "  [9/10] Extracting: https://eatmila.com/products/soup-dumplings-50-pc-exp\n",
      "      ‚úì Extracted 5800 characters\n",
      "  [10/10] Extracting: https://eatmila.com/products/classic-pork-xiao-long-bao\n",
      "      ‚úì Extracted 5800 characters\n",
      "\n",
      "ü§ñ Using LLM to extract consolidated business data...\n",
      "üîó Found 2 social media links in HTML\n",
      "üìä Total content length: 53512 characters\n",
      "üîÑ Calling OpenAI API...\n",
      "‚ùå LLM extraction error: AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "üîç Traceback: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ABC\\AppData\\Local\\Temp\\ipykernel_33416\\4174679287.py\", line 339, in extract_business_data_with_llm\n",
      "    response = openai.chat.completions.create(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 286, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 1156, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ABC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n",
      "\n",
      "üîÑ Using enhanced fallback extraction...\n",
      "‚ö†Ô∏è Could not generate LLM summary: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-.... **********keys. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'code': 'invalid_api_key', 'param': None}}\n",
      "\n",
      "================================================================================\n",
      "‚úÖ EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üíæ Results saved to: eatmila_20251115_003518.json\n",
      "\n",
      "================================================================================\n",
      "üìä EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üè¢ Company Name: Eatmila\n",
      "üåê Website: https://eatmila.com/\n",
      "\n",
      "üìß Emails (0 found):\n",
      "   None found\n",
      "\n",
      "üì± Phone Numbers (0 found):\n",
      "   None found\n",
      "\n",
      "üîó Social Media (2 links):\n",
      "   ‚Ä¢ Facebook: https://www.facebook.com/eat.MiLa\n",
      "   ‚Ä¢ Instagram: https://www.instagram.com/eat.mila\n",
      "\n",
      "üìù Business Summary:\n",
      "   Business information extracted using automated method. Unable to generate detailed summary. Please visit the website for more information.\n",
      "\n",
      "‚öôÔ∏è Extraction Method: enhanced_regex_fallback_with_llm_summary\n",
      "\n",
      "================================================================================\n",
      "üìÅ File saved locally: eatmila_20251115_003518.json\n",
      "\n",
      "‚úÖ Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# üîç Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# COMPLETE WORKING VERSION - All syntax errors fixed\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import openai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "MAX_CRAWL_DEPTH = 2\n",
    "MAX_PAGES = 30\n",
    "TOP_PAGES_TO_ANALYZE = 10\n",
    "REQUEST_TIMEOUT = 10\n",
    "RATE_LIMIT_DELAY = 1\n",
    "\n",
    "ua = UserAgent()\n",
    "print(\"‚úÖ Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    print(f\"üîç Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            time.sleep(3)\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"‚úÖ Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"‚úÖ Found website via fallback: {href}\")\n",
    "                        return href\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"‚úÖ Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with requests: {e}\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    print(f\"üï∑Ô∏è Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url.rstrip('/'), 0)])\n",
    "    pages = []\n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        current_url = current_url.rstrip('/')\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            pages.append(current_url)\n",
    "            print(f\"  ‚úì Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0].rstrip('/')\n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"‚úÖ Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    print(f\"\\nü§ñ Using LLM to select top {top_n} most relevant pages...\")\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your job is to identify the most relevant pages that contain business information like company details, contact info, services, and about information.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these {len(page_urls)} URLs and select the top {top_n} most relevant pages for extracting company information (like contact details, about us, services, etc.).\n",
    "\n",
    "URLs:\n",
    "{url_list}\n",
    "\n",
    "Return ONLY a valid JSON array of numbers (1-indexed positions) like: [1, 3, 5, 7, 9]\n",
    "Do not include any other text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM raw response: {content[:200]}\")\n",
    "        \n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        selected_indices = json.loads(content)\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"‚úÖ LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        return selected_urls[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM selection error: {e}\")\n",
    "        print(\"üîÑ Using heuristic fallback...\")\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company']\n",
    "        scored_pages = []\n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(2 for keyword in priority_keywords if keyword in url_lower)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        print(f\"‚úÖ Heuristic selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "def extract_page_content(url):\n",
    "    \"\"\"Extract both text content and raw HTML for better social media detection\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        if response.status_code != 200:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        for script in soup(['script', 'style', 'iframe', 'noscript']):\n",
    "            script.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text, html_content\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚úó Error extracting content from {url}: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "def extract_social_from_html(html_list):\n",
    "    \"\"\"Extract social media links directly from raw HTML\"\"\"\n",
    "    if not html_list:\n",
    "        return []\n",
    "    \n",
    "    all_html = \" \".join(html_list)\n",
    "    socials = set()\n",
    "    \n",
    "    patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        found = re.findall(pattern, all_html, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'[\"\\'>].*$', '', link)\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            if len(link) > 20:\n",
    "                socials.add(link)\n",
    "    \n",
    "    return list(socials)\n",
    "\n",
    "# ============================================================================\n",
    "# LLM EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url, all_html=None):\n",
    "    print(\"\\nü§ñ Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    social_media_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    print(f\"üîó Found {len(social_media_from_html)} social media links in HTML\")\n",
    "    \n",
    "    combined_content = \"\"\n",
    "    max_chars_per_page = 8000\n",
    "    \n",
    "    for url, content in page_contents.items():\n",
    "        truncated_content = content[:max_chars_per_page] if len(content) > max_chars_per_page else content\n",
    "        combined_content += f\"\\n\\n=== PAGE: {url} ===\\n{truncated_content}\\n\"\n",
    "    \n",
    "    max_total_chars = 60000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated for length]\"\n",
    "    \n",
    "    print(f\"üìä Total content length: {len(combined_content)} characters\")\n",
    "\n",
    "    social_hint = \"\"\n",
    "    if social_media_from_html:\n",
    "        social_hint = f\"\\n\\nNOTE: The following social media links were found in the HTML: {', '.join(social_media_from_html[:5])}\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Extract comprehensive business information from website content and return ONLY valid JSON with no additional text, markdown formatting, or explanations.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze the following website content and extract ALL business information for: {main_website_url}\n",
    "\n",
    "{combined_content}{social_hint}\n",
    "\n",
    "Extract and return ONLY a JSON object with this EXACT structure (no markdown, no text before or after):\n",
    "{{\n",
    "  \"company_name\": \"Full official company name\",\n",
    "  \"company_main_url\": \"{main_website_url}\",\n",
    "  \"emails\": [\"email1@domain.com\", \"email2@domain.com\"],\n",
    "  \"contact_numbers\": [\"+12345678900\", \"2345678900\"],\n",
    "  \"social_media_links\": [\"https://facebook.com/page\", \"https://instagram.com/profile\"],\n",
    "  \"summary\": \"A comprehensive 5-10 line summary describing: what the company does, short introduction main services/products offered, Base this on ALL analyzed pages.\"\n",
    "}}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Find ALL emails, phone numbers, and social media links across all pages\n",
    "2. Look for social media links in footer, header, contact pages, and inline content\n",
    "3. Include full URLs for social media (Facebook, Instagram, Twitter/X, LinkedIn, YouTube, TikTok, Pinterest)\n",
    "4. Write a detailed, informative summary that truly captures what the business does\n",
    "5. Use empty arrays [] for missing data, never null\n",
    "6. Return ONLY the JSON object - no explanation, no markdown backticks, no preamble\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"üîÑ Calling OpenAI API...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"üîç LLM response received ({len(content)} chars)\")\n",
    "        print(f\"üìù First 500 chars of response: {content[:500]}\")\n",
    "        \n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        if social_media_from_html:\n",
    "            existing_socials = set(extracted_data.get('social_media_links', []))\n",
    "            all_socials = existing_socials.union(set(social_media_from_html))\n",
    "            extracted_data['social_media_links'] = list(all_socials)\n",
    "        \n",
    "        extracted_data = validate_and_clean_data(extracted_data, main_website_url)\n",
    "        \n",
    "        if (not extracted_data.get('emails') and \n",
    "            not extracted_data.get('contact_numbers') and \n",
    "            not extracted_data.get('social_media_links')):\n",
    "            print(\"‚ö†Ô∏è LLM extraction returned empty data, trying fallback...\")\n",
    "            return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "        \n",
    "        print(\"‚úÖ Data extraction successful via LLM!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON parsing error: {e}\")\n",
    "        print(f\"üîç Raw response: {content[:500]}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM extraction error: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        print(f\"üîç Traceback: {traceback.format_exc()}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "\n",
    "def validate_and_clean_data(data, main_url):\n",
    "    \"\"\"Validate and clean extracted data\"\"\"\n",
    "    cleaned = {\n",
    "        \"company_name\": data.get(\"company_name\") or urlparse(main_url).netloc.replace('www.', '').split('.')[0].title(),\n",
    "        \"company_main_url\": main_url,\n",
    "        \"emails\": [],\n",
    "        \"contact_numbers\": [],\n",
    "        \"social_media_links\": [],\n",
    "        \"summary\": data.get(\"summary\") or \"No summary available\"\n",
    "    }\n",
    "    \n",
    "    if data.get(\"emails\") and isinstance(data[\"emails\"], list):\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        cleaned[\"emails\"] = [e.strip() for e in data[\"emails\"] if re.match(email_pattern, e.strip())]\n",
    "    \n",
    "    if data.get(\"contact_numbers\") and isinstance(data[\"contact_numbers\"], list):\n",
    "        cleaned[\"contact_numbers\"] = [p.strip() for p in data[\"contact_numbers\"] if p and len(str(p).strip()) > 5]\n",
    "    \n",
    "    if data.get(\"social_media_links\") and isinstance(data[\"social_media_links\"], list):\n",
    "        social_domains = ['facebook.com', 'instagram.com', 'linkedin.com', 'twitter.com', 'x.com', \n",
    "                         'youtube.com', 'tiktok.com', 'pinterest.com']\n",
    "        cleaned[\"social_media_links\"] = [\n",
    "            s.strip() for s in data[\"social_media_links\"] \n",
    "            if s and any(domain in s.lower() for domain in social_domains)\n",
    "        ]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# ============================================================================\n",
    "# FALLBACK EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_fallback_data(main_website_url, page_contents, all_html=None):\n",
    "    \"\"\"Enhanced fallback extraction with better regex patterns and LLM summary\"\"\"\n",
    "    print(\"üîÑ Using enhanced fallback extraction...\")\n",
    "    \n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    social_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    \n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "    emails = list(set(re.findall(email_pattern, all_text)))\n",
    "    emails = [e for e in emails if not e.endswith(('.png', '.jpg', '.gif', '.svg'))]\n",
    "    \n",
    "    phone_patterns = [\n",
    "        r'\\+\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}',\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "    ]\n",
    "    \n",
    "    phones = set()\n",
    "    for pattern in phone_patterns:\n",
    "        found = re.findall(pattern, all_text)\n",
    "        for phone in found:\n",
    "            cleaned = re.sub(r'[^\\d+()-]', '', phone)\n",
    "            if len(re.sub(r'[^\\d]', '', cleaned)) >= 10:\n",
    "                phones.add(phone.strip())\n",
    "    \n",
    "    socials_from_text = set()\n",
    "    social_patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in social_patterns:\n",
    "        found = re.findall(pattern, all_text, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            socials_from_text.add(link)\n",
    "    \n",
    "    all_socials = set(social_from_html).union(socials_from_text)\n",
    "    \n",
    "    company_name = urlparse(main_website_url).netloc.replace('www.', '').split('.')[0]\n",
    "    company_name = ' '.join(word.capitalize() for word in re.split(r'[-_]', company_name))\n",
    "    \n",
    "    summary = generate_fallback_summary(all_text, company_name, main_website_url)\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_main_url\": main_website_url,\n",
    "        \"emails\": sorted(list(set(emails)))[:15],\n",
    "        \"contact_numbers\": sorted(list(phones))[:15],\n",
    "        \"social_media_links\": sorted(list(all_socials)),\n",
    "        \"summary\": summary,\n",
    "        \"extraction_method\": \"enhanced_regex_fallback_with_llm_summary\"\n",
    "    }\n",
    "\n",
    "def generate_fallback_summary(content, company_name, website_url):\n",
    "    \"\"\"Generate a summary using LLM even when structured extraction fails\"\"\"\n",
    "    try:\n",
    "        truncated = content[:15000] if len(content) > 15000 else content\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a business analyst. Write concise, informative summaries.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Based on this website content for {company_name} ({website_url}), write a 5-10 line summary describing:\n",
    "- What the company does\n",
    "- Main services/products\n",
    "- Target audience\n",
    "- Unique aspects\n",
    "\n",
    "Content:\n",
    "{truncated}\n",
    "\n",
    "Write only the summary, no preamble.\"\"\"}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(\"‚úÖ Generated summary using LLM\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not generate LLM summary: {e}\")\n",
    "        return \"Business information extracted using automated method. Unable to generate detailed summary. Please visit the website for more information.\"\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    print(\"=\"*80)\n",
    "    print(\"üöÄ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    if not website_url:\n",
    "        print(\"‚ùå Failed to extract website URL from Google Maps\")\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    parsed = urlparse(website_url)\n",
    "    website_url = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    print(f\"\\nüìç Main Website: {website_url}\\n\")\n",
    "    \n",
    "    all_pages = crawl_website(website_url)\n",
    "    if not all_pages:\n",
    "        print(\"‚ùå No pages found to scrape\")\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    if not selected_pages:\n",
    "        print(\"‚ùå No pages selected for analysis\")\n",
    "        return None, None, len(all_pages), 0\n",
    "\n",
    "    print(f\"\\nüì• Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    all_html = []\n",
    "    \n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        text_content, html_content = extract_page_content(page_url)\n",
    "        if text_content:\n",
    "            page_contents[page_url] = text_content\n",
    "            all_html.append(html_content)\n",
    "            print(f\"      ‚úì Extracted {len(text_content)} characters\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"‚ùå No content extracted from any page\")\n",
    "        return None, None, len(all_pages), len(selected_pages)\n",
    "\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url, all_html)\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', str(business_name))\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return extracted_data, business_name, len(all_pages), len(page_contents)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nüíæ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüè¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"üåê Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nüìß Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   ‚Ä¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nüì± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   ‚Ä¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nüîó Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link: platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link: platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link: platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link: platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link: platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link: platform = \"TikTok\"\n",
    "            print(f\"   ‚Ä¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nüìù Business Summary:\\n   {summary}\")\n",
    "    \n",
    "    if 'extraction_method' in extracted_data:\n",
    "        print(f\"\\n‚öôÔ∏è Extraction Method: {extracted_data['extraction_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/xjfuyKPZsg8tbTP68\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"‚ùå ERROR: Please set your OPENAI_API_KEY!\")\n",
    "    else:\n",
    "        extracted_data, business_name, all_pages_discovered, pages_analyzed = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                pages_analyzed\n",
    "            )\n",
    "            display_summary(extracted_data)\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(\"üì• File download started!\")\n",
    "            except:\n",
    "                print(f\"üìÅ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"‚ùå No data extracted. Check the URL and API key, then try again.\")\n",
    "\n",
    "print(\"\\n‚úÖ Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
