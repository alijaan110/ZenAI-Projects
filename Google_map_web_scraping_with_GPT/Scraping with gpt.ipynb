{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "================================================================================\n",
      "ğŸš€ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "ğŸ” Extracting website from Google Maps URL...\n",
      "âœ… Found website: https://eatmila.com/?utm_source=google&utm_medium=organic&utm_campaign=gmb-listing\n",
      "\n",
      "ğŸ“ Main Website: https://eatmila.com/\n",
      "\n",
      "ğŸ•·ï¸ Starting website crawl from: https://eatmila.com/\n",
      "  âœ“ Discovered [1/30]: https://eatmila.com\n",
      "  âœ“ Discovered [2/30]: https://eatmila.com/products/classic-pork-xiao-long-bao\n",
      "  âœ“ Discovered [3/30]: https://eatmila.com/products/potstickers\n",
      "  âœ“ Discovered [4/30]: https://eatmila.com/products/chocolate-black-sesame-lava-dumplings\n",
      "  âœ“ Discovered [5/30]: https://eatmila.com/products/noodles\n",
      "  âœ“ Discovered [6/30]: https://eatmila.com/products/braised-beef-noodles\n",
      "  âœ“ Discovered [7/30]: https://eatmila.com/collections/national\n",
      "  âœ“ Discovered [8/30]: https://eatmila.com/products/mila-signature-bundle \n",
      "  âœ“ Discovered [9/30]: https://eatmila.com/products/hulu-sauce-jars\n",
      "  âœ“ Discovered [10/30]: https://eatmila.com/products/bamboosteamer\n",
      "  âœ“ Discovered [11/30]: https://eatmila.com/products/dumpling-dipping-bowl-chopstick-set\n",
      "  âœ“ Discovered [12/30]: https://eatmila.com/products/rose-lychee-ice-cream\n",
      "  âœ“ Discovered [13/30]: https://eatmila.com/pages/build-your-own-bundle\n",
      "  âœ“ Discovered [14/30]: https://eatmila.com/pages/general-store-locator\n",
      "  âœ“ Discovered [15/30]: https://eatmila.com/pages/founders\n",
      "  âœ“ Discovered [16/30]: https://eatmila.com/pages/delivery\n",
      "  âœ“ Discovered [17/30]: https://eatmila.com/pages/press\n",
      "  âœ“ Discovered [18/30]: https://eatmila.com/pages/faqs\n",
      "  âœ“ Discovered [19/30]: https://eatmila.com/a/account/login\n",
      "  âœ“ Discovered [20/30]: https://eatmila.com/blogs/news\n",
      "  âœ“ Discovered [21/30]: https://eatmila.com/products/soup-dumplings-50-pc-exp\n",
      "  âœ“ Discovered [22/30]: https://eatmila.com/products/pork-and-shrimp-xiao-long-bao\n",
      "  âœ“ Discovered [23/30]: https://eatmila.com/products/savory-chicken-xiao-long-bao\n",
      "  âœ“ Discovered [24/30]: https://eatmila.com/products/pho-beef-xiao-long-bao\n",
      "  âœ“ Discovered [25/30]: https://eatmila.com/products/chinese-noodles-kit-ground-pork\n",
      "  âœ“ Discovered [26/30]: https://eatmila.com/products/ground-pork-spicy-dan-dan-noodles\n",
      "  âœ“ Discovered [27/30]: https://eatmila.com/products/impossible-plant-based-spicy-dan-dan-noodle\n",
      "  âœ“ Discovered [28/30]: https://eatmila.com/products/caramelized-scallion-oil-noodle\n",
      "  âœ“ Discovered [29/30]: https://eatmila.com/products/ground-pork-sweet-and-savory-noodle\n",
      "  âœ“ Discovered [30/30]: https://eatmila.com/products/chili-crunch-dipping-sauce\n",
      "âœ… Crawl complete! Discovered 30 pages\n",
      "\n",
      "ğŸ¤– Using LLM to select top 10 most relevant pages...\n",
      "ğŸ” LLM raw response: [1, 15, 16, 18, 20, 14, 17, 8, 7, 4]\n",
      "âœ… LLM selected 10 pages:\n",
      "   1. https://eatmila.com\n",
      "   2. https://eatmila.com/pages/founders\n",
      "   3. https://eatmila.com/pages/delivery\n",
      "   4. https://eatmila.com/pages/faqs\n",
      "   5. https://eatmila.com/blogs/news\n",
      "   6. https://eatmila.com/pages/general-store-locator\n",
      "   7. https://eatmila.com/pages/press\n",
      "   8. https://eatmila.com/products/mila-signature-bundle \n",
      "   9. https://eatmila.com/collections/national\n",
      "   10. https://eatmila.com/products/chocolate-black-sesame-lava-dumplings\n",
      "\n",
      "ğŸ“¥ Extracting content from 10 selected pages...\n",
      "  [1/10] Extracting: https://eatmila.com\n",
      "      âœ“ Extracted 6179 characters\n",
      "  [2/10] Extracting: https://eatmila.com/pages/founders\n",
      "      âœ“ Extracted 13325 characters\n",
      "  [3/10] Extracting: https://eatmila.com/pages/delivery\n",
      "      âœ“ Extracted 4204 characters\n",
      "  [4/10] Extracting: https://eatmila.com/pages/faqs\n",
      "      âœ“ Extracted 9576 characters\n",
      "  [5/10] Extracting: https://eatmila.com/blogs/news\n",
      "      âœ“ Extracted 5118 characters\n",
      "  [6/10] Extracting: https://eatmila.com/pages/general-store-locator\n",
      "      âœ“ Extracted 3888 characters\n",
      "  [7/10] Extracting: https://eatmila.com/pages/press\n",
      "      âœ“ Extracted 4329 characters\n",
      "  [8/10] Extracting: https://eatmila.com/products/mila-signature-bundle \n",
      "      âœ“ Extracted 4283 characters\n",
      "  [9/10] Extracting: https://eatmila.com/collections/national\n",
      "      âœ“ Extracted 5033 characters\n",
      "  [10/10] Extracting: https://eatmila.com/products/chocolate-black-sesame-lava-dumplings\n",
      "      âœ“ Extracted 10152 characters\n",
      "\n",
      "ğŸ¤– Using LLM to extract consolidated business data...\n",
      "ğŸ”— Found 2 social media links in HTML\n",
      "ğŸ“Š Total content length: 57596 characters\n",
      "ğŸ”„ Calling OpenAI API...\n",
      "ğŸ” LLM response received (847 chars)\n",
      "ğŸ“ First 500 chars of response: {\n",
      "  \"company_name\": \"MÃ¬LÃ \",\n",
      "  \"company_main_url\": \"https://eatmila.com/\",\n",
      "  \"emails\": [\"press@eatmila.com\"],\n",
      "  \"contact_numbers\": [],\n",
      "  \"social_media_links\": [\"https://www.facebook.com/eat.MiLa\", \"https://www.instagram.com/eat.mila\"],\n",
      "  \"summary\": \"MÃ¬LÃ  is a frozen food company specializing in authentic Chinese dumplings, noodles, and sauces, founded by Jen Liao and Caleb Wang in 2018. The company aims to bring restaurant-quality Chinese cuisine to homes across the United States, offering a vari\n",
      "âœ… Data extraction successful via LLM!\n",
      "\n",
      "================================================================================\n",
      "âœ… EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¾ Results saved to: mÃ¬lÃ _20251115_004240.json\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ¢ Company Name: MÃ¬LÃ \n",
      "ğŸŒ Website: https://eatmila.com/\n",
      "\n",
      "ğŸ“§ Emails (1 found):\n",
      "   â€¢ press@eatmila.com\n",
      "\n",
      "ğŸ“± Phone Numbers (0 found):\n",
      "   None found\n",
      "\n",
      "ğŸ”— Social Media (2 links):\n",
      "   â€¢ Instagram: https://www.instagram.com/eat.mila\n",
      "   â€¢ Facebook: https://www.facebook.com/eat.MiLa\n",
      "\n",
      "ğŸ“ Business Summary:\n",
      "   MÃ¬LÃ  is a frozen food company specializing in authentic Chinese dumplings, noodles, and sauces, founded by Jen Liao and Caleb Wang in 2018. The company aims to bring restaurant-quality Chinese cuisine to homes across the United States, offering a variety of products including soup dumplings, potstickers, and vegan options. MÃ¬LÃ  prides itself on using high-quality, locally sourced ingredients and provides a melt-free guarantee for its frozen products. Customers can shop online with free shipping on orders over $99 and explore various bundles and subscription options for regular deliveries.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ File saved locally: mÃ¬lÃ _20251115_004240.json\n",
      "\n",
      "âœ… Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” Intelligent Web Scraper: Google Maps + LLM-Powered Page Selection\n",
    "# COMPLETE WORKING VERSION - All syntax errors fixed\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import openai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "MAX_CRAWL_DEPTH = 2\n",
    "MAX_PAGES = 30\n",
    "TOP_PAGES_TO_ANALYZE = 10\n",
    "REQUEST_TIMEOUT = 10\n",
    "RATE_LIMIT_DELAY = 1\n",
    "\n",
    "ua = UserAgent()\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    print(f\"ğŸ” Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            time.sleep(3)\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"âœ… Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"âœ… Found website via fallback: {href}\")\n",
    "                        return href\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"âœ… Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with requests: {e}\")\n",
    "    \n",
    "    print(\"âš ï¸ Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    print(f\"ğŸ•·ï¸ Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url.rstrip('/'), 0)])\n",
    "    pages = []\n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        current_url = current_url.rstrip('/')\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            pages.append(current_url)\n",
    "            print(f\"  âœ“ Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0].rstrip('/')\n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"âœ… Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    print(f\"\\nğŸ¤– Using LLM to select top {top_n} most relevant pages...\")\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your job is to identify the most relevant pages that contain business information like company details, contact info, services, and about information.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these {len(page_urls)} URLs and select the top {top_n} most relevant pages for extracting company information (like contact details, about us, services, etc.).\n",
    "\n",
    "URLs:\n",
    "{url_list}\n",
    "\n",
    "Return ONLY a valid JSON array of numbers (1-indexed positions) like: [1, 3, 5, 7, 9]\n",
    "Do not include any other text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"ğŸ” LLM raw response: {content[:200]}\")\n",
    "        \n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        selected_indices = json.loads(content)\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"âœ… LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        return selected_urls[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LLM selection error: {e}\")\n",
    "        print(\"ğŸ”„ Using heuristic fallback...\")\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company']\n",
    "        scored_pages = []\n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(2 for keyword in priority_keywords if keyword in url_lower)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        print(f\"âœ… Heuristic selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "def extract_page_content(url):\n",
    "    \"\"\"Extract both text content and raw HTML for better social media detection\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        if response.status_code != 200:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        for script in soup(['script', 'style', 'iframe', 'noscript']):\n",
    "            script.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text, html_content\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error extracting content from {url}: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "def extract_social_from_html(html_list):\n",
    "    \"\"\"Extract social media links directly from raw HTML\"\"\"\n",
    "    if not html_list:\n",
    "        return []\n",
    "    \n",
    "    all_html = \" \".join(html_list)\n",
    "    socials = set()\n",
    "    \n",
    "    patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        found = re.findall(pattern, all_html, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'[\"\\'>].*$', '', link)\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            if len(link) > 20:\n",
    "                socials.add(link)\n",
    "    \n",
    "    return list(socials)\n",
    "\n",
    "# ============================================================================\n",
    "# LLM EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url, all_html=None):\n",
    "    print(\"\\nğŸ¤– Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    social_media_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    print(f\"ğŸ”— Found {len(social_media_from_html)} social media links in HTML\")\n",
    "    \n",
    "    combined_content = \"\"\n",
    "    max_chars_per_page = 8000\n",
    "    \n",
    "    for url, content in page_contents.items():\n",
    "        truncated_content = content[:max_chars_per_page] if len(content) > max_chars_per_page else content\n",
    "        combined_content += f\"\\n\\n=== PAGE: {url} ===\\n{truncated_content}\\n\"\n",
    "    \n",
    "    max_total_chars = 60000\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated for length]\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Total content length: {len(combined_content)} characters\")\n",
    "\n",
    "    social_hint = \"\"\n",
    "    if social_media_from_html:\n",
    "        social_hint = f\"\\n\\nNOTE: The following social media links were found in the HTML: {', '.join(social_media_from_html[:5])}\"\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert business data extraction assistant. Extract comprehensive business information from website content and return ONLY valid JSON with no additional text, markdown formatting, or explanations.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze the following website content and extract ALL business information for: {main_website_url}\n",
    "\n",
    "{combined_content}{social_hint}\n",
    "\n",
    "Extract and return ONLY a JSON object with this EXACT structure (no markdown, no text before or after):\n",
    "{{\n",
    "  \"company_name\": \"Full official company name\",\n",
    "  \"company_main_url\": \"{main_website_url}\",\n",
    "  \"emails\": [\"email1@domain.com\", \"email2@domain.com\"],\n",
    "  \"contact_numbers\": [\"+12345678900\", \"2345678900\"],\n",
    "  \"social_media_links\": [\"https://facebook.com/page\", \"https://instagram.com/profile\"],\n",
    "  \"summary\": \"A comprehensive 5-10 line summary describing: what the company does, short introduction main services/products offered, Base this on ALL analyzed pages.\"\n",
    "}}\n",
    "\n",
    "CRITICAL INSTRUCTIONS:\n",
    "1. Find ALL emails, phone numbers, and social media links across all pages\n",
    "2. Look for social media links in footer, header, contact pages, and inline content\n",
    "3. Include full URLs for social media (Facebook, Instagram, Twitter/X, LinkedIn, YouTube, TikTok, Pinterest)\n",
    "4. Write a detailed, informative summary that truly captures what the business does\n",
    "5. Use empty arrays [] for missing data, never null\n",
    "6. Return ONLY the JSON object - no explanation, no markdown backticks, no preamble\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸ”„ Calling OpenAI API...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"ğŸ” LLM response received ({len(content)} chars)\")\n",
    "        print(f\"ğŸ“ First 500 chars of response: {content[:500]}\")\n",
    "        \n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        if social_media_from_html:\n",
    "            existing_socials = set(extracted_data.get('social_media_links', []))\n",
    "            all_socials = existing_socials.union(set(social_media_from_html))\n",
    "            extracted_data['social_media_links'] = list(all_socials)\n",
    "        \n",
    "        extracted_data = validate_and_clean_data(extracted_data, main_website_url)\n",
    "        \n",
    "        if (not extracted_data.get('emails') and \n",
    "            not extracted_data.get('contact_numbers') and \n",
    "            not extracted_data.get('social_media_links')):\n",
    "            print(\"âš ï¸ LLM extraction returned empty data, trying fallback...\")\n",
    "            return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "        \n",
    "        print(\"âœ… Data extraction successful via LLM!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ JSON parsing error: {e}\")\n",
    "        print(f\"ğŸ” Raw response: {content[:500]}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LLM extraction error: {type(e).__name__}: {e}\")\n",
    "        import traceback\n",
    "        print(f\"ğŸ” Traceback: {traceback.format_exc()}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "\n",
    "def validate_and_clean_data(data, main_url):\n",
    "    \"\"\"Validate and clean extracted data\"\"\"\n",
    "    cleaned = {\n",
    "        \"company_name\": data.get(\"company_name\") or urlparse(main_url).netloc.replace('www.', '').split('.')[0].title(),\n",
    "        \"company_main_url\": main_url,\n",
    "        \"emails\": [],\n",
    "        \"contact_numbers\": [],\n",
    "        \"social_media_links\": [],\n",
    "        \"summary\": data.get(\"summary\") or \"No summary available\"\n",
    "    }\n",
    "    \n",
    "    if data.get(\"emails\") and isinstance(data[\"emails\"], list):\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        cleaned[\"emails\"] = [e.strip() for e in data[\"emails\"] if re.match(email_pattern, e.strip())]\n",
    "    \n",
    "    if data.get(\"contact_numbers\") and isinstance(data[\"contact_numbers\"], list):\n",
    "        cleaned[\"contact_numbers\"] = [p.strip() for p in data[\"contact_numbers\"] if p and len(str(p).strip()) > 5]\n",
    "    \n",
    "    if data.get(\"social_media_links\") and isinstance(data[\"social_media_links\"], list):\n",
    "        social_domains = ['facebook.com', 'instagram.com', 'linkedin.com', 'twitter.com', 'x.com', \n",
    "                         'youtube.com', 'tiktok.com', 'pinterest.com']\n",
    "        cleaned[\"social_media_links\"] = [\n",
    "            s.strip() for s in data[\"social_media_links\"] \n",
    "            if s and any(domain in s.lower() for domain in social_domains)\n",
    "        ]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# ============================================================================\n",
    "# FALLBACK EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_fallback_data(main_website_url, page_contents, all_html=None):\n",
    "    \"\"\"Enhanced fallback extraction with better regex patterns and LLM summary\"\"\"\n",
    "    print(\"ğŸ”„ Using enhanced fallback extraction...\")\n",
    "    \n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    social_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    \n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "    emails = list(set(re.findall(email_pattern, all_text)))\n",
    "    emails = [e for e in emails if not e.endswith(('.png', '.jpg', '.gif', '.svg'))]\n",
    "    \n",
    "    phone_patterns = [\n",
    "        r'\\+\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}',\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "    ]\n",
    "    \n",
    "    phones = set()\n",
    "    for pattern in phone_patterns:\n",
    "        found = re.findall(pattern, all_text)\n",
    "        for phone in found:\n",
    "            cleaned = re.sub(r'[^\\d+()-]', '', phone)\n",
    "            if len(re.sub(r'[^\\d]', '', cleaned)) >= 10:\n",
    "                phones.add(phone.strip())\n",
    "    \n",
    "    socials_from_text = set()\n",
    "    social_patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in social_patterns:\n",
    "        found = re.findall(pattern, all_text, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            socials_from_text.add(link)\n",
    "    \n",
    "    all_socials = set(social_from_html).union(socials_from_text)\n",
    "    \n",
    "    company_name = urlparse(main_website_url).netloc.replace('www.', '').split('.')[0]\n",
    "    company_name = ' '.join(word.capitalize() for word in re.split(r'[-_]', company_name))\n",
    "    \n",
    "    summary = generate_fallback_summary(all_text, company_name, main_website_url)\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_main_url\": main_website_url,\n",
    "        \"emails\": sorted(list(set(emails)))[:15],\n",
    "        \"contact_numbers\": sorted(list(phones))[:15],\n",
    "        \"social_media_links\": sorted(list(all_socials)),\n",
    "        \"summary\": summary,\n",
    "        \"extraction_method\": \"enhanced_regex_fallback_with_llm_summary\"\n",
    "    }\n",
    "\n",
    "def generate_fallback_summary(content, company_name, website_url):\n",
    "    \"\"\"Generate a summary using LLM even when structured extraction fails\"\"\"\n",
    "    try:\n",
    "        truncated = content[:15000] if len(content) > 15000 else content\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a business analyst. Write concise, informative summaries.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Based on this website content for {company_name} ({website_url}), write a 5-10 line summary describing:\n",
    "- What the company does\n",
    "- Main services/products\n",
    "- Target audience\n",
    "- Unique aspects\n",
    "\n",
    "Content:\n",
    "{truncated}\n",
    "\n",
    "Write only the summary, no preamble.\"\"\"}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(\"âœ… Generated summary using LLM\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not generate LLM summary: {e}\")\n",
    "        return \"Business information extracted using automated method. Unable to generate detailed summary. Please visit the website for more information.\"\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    if not website_url:\n",
    "        print(\"âŒ Failed to extract website URL from Google Maps\")\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    parsed = urlparse(website_url)\n",
    "    website_url = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    print(f\"\\nğŸ“ Main Website: {website_url}\\n\")\n",
    "    \n",
    "    all_pages = crawl_website(website_url)\n",
    "    if not all_pages:\n",
    "        print(\"âŒ No pages found to scrape\")\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    if not selected_pages:\n",
    "        print(\"âŒ No pages selected for analysis\")\n",
    "        return None, None, len(all_pages), 0\n",
    "\n",
    "    print(f\"\\nğŸ“¥ Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    all_html = []\n",
    "    \n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        text_content, html_content = extract_page_content(page_url)\n",
    "        if text_content:\n",
    "            page_contents[page_url] = text_content\n",
    "            all_html.append(html_content)\n",
    "            print(f\"      âœ“ Extracted {len(text_content)} characters\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"âŒ No content extracted from any page\")\n",
    "        return None, None, len(all_pages), len(selected_pages)\n",
    "\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url, all_html)\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', str(business_name))\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return extracted_data, business_name, len(all_pages), len(page_contents)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent page selection\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nğŸ’¾ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"ğŸŒ Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nğŸ“§ Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   â€¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nğŸ“± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   â€¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nğŸ”— Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link: platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link: platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link: platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link: platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link: platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link: platform = \"TikTok\"\n",
    "            print(f\"   â€¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nğŸ“ Business Summary:\\n   {summary}\")\n",
    "    \n",
    "    if 'extraction_method' in extracted_data:\n",
    "        print(f\"\\nâš™ï¸ Extraction Method: {extracted_data['extraction_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/xjfuyKPZsg8tbTP68\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"âŒ ERROR: Please set your OPENAI_API_KEY!\")\n",
    "    else:\n",
    "        extracted_data, business_name, all_pages_discovered, pages_analyzed = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                pages_analyzed\n",
    "            )\n",
    "            display_summary(extracted_data)\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(\"ğŸ“¥ File download started!\")\n",
    "            except:\n",
    "                print(f\"ğŸ“ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"âŒ No data extracted. Check the URL and API key, then try again.\")\n",
    "\n",
    "print(\"\\nâœ… Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configuration loaded successfully!\n",
      "================================================================================\n",
      "ğŸš€ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\n",
      "================================================================================\n",
      "ğŸ” Extracting website from Google Maps URL...\n",
      "âœ… Found website: http://thesintrahotel.com/\n",
      "\n",
      "ğŸ“ Main Website: http://thesintrahotel.com/\n",
      "\n",
      "ğŸ•·ï¸ Starting website crawl from: http://thesintrahotel.com/\n",
      "  âœ“ Discovered [1/30]: http://thesintrahotel.com\n",
      "  âœ“ Discovered [2/30]: http://thesintrahotel.com/about-online-hotel-booking-islamabad\n",
      "  âœ“ Discovered [3/30]: http://thesintrahotel.com/executive-room-at-hotel-islamabad\n",
      "  âœ“ Discovered [4/30]: http://thesintrahotel.com/twin-executive-room-at-hotel-islamabad\n",
      "  âœ“ Discovered [5/30]: http://thesintrahotel.com/super-deluxe-room-at-hotel-islamabad\n",
      "  âœ“ Discovered [6/30]: http://thesintrahotel.com/sintra-hotel-islamabad-pictures\n",
      "  âœ“ Discovered [7/30]: http://thesintrahotel.com/room-decor-packages\n",
      "  âœ“ Discovered [8/30]: http://thesintrahotel.com/places-to-visit-in-islamabad-and-fun-activities\n",
      "  âœ“ Discovered [9/30]: http://thesintrahotel.com/spice-fusion-top-restaurant-in-islamabad\n",
      "  âœ“ Discovered [10/30]: http://thesintrahotel.com/soir-top-cafe-in-islamabad\n",
      "  âœ“ Discovered [11/30]: http://thesintrahotel.com/blog-hotel-islamabad-updates\n",
      "  âœ“ Discovered [12/30]: http://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "  âœ“ Discovered [13/30]: http://thesintrahotel.com/post/1024/staycation-at-sintra-hotel-in-islamabad-weekend-escape\n",
      "  âœ“ Discovered [14/30]: http://thesintrahotel.com/post/24/birthday-room-decor-package-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [15/30]: http://thesintrahotel.com/post/23/melody-market-food-shopping-guide-by-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [16/30]: http://thesintrahotel.com/post/22/family-hotel-islamabad-weekend-itinerary\n",
      "  âœ“ Discovered [17/30]: http://thesintrahotel.com/post/20/ultimate-business-travellers-guide-staying-at-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [18/30]: http://thesintrahotel.com/post/21/ultimate-business-travellers-guide-staying-at-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [19/30]: http://thesintrahotel.com/post/19/perfect-choice-for-luxurious-hotel-stay-in-islamabad\n",
      "  âœ“ Discovered [20/30]: http://thesintrahotel.com/terms-and-conditions-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [21/30]: http://thesintrahotel.com/privacy-policy-sintra-hotel-islamabad\n",
      "  âœ“ Discovered [22/30]: https://thesintrahotel.com/about-online-hotel-booking-islamabad\n",
      "  âœ“ Discovered [23/30]: https://thesintrahotel.com/post/19/perfect-choice-for-luxurious-hotel-stay-in-islamabad\n",
      "  âœ“ Discovered [24/30]: https://thesintrahotel.com/spice-fusion-top-restaurant-in-islamabad\n",
      "  âœ“ Discovered [25/30]: https://thesintrahotel.com/soir-top-cafe-in-islamabad\n",
      "  âœ“ Discovered [26/30]: https://thesintrahotel.com/room-decor-packages\n",
      "  âœ“ Discovered [27/30]: https://thesintrahotel.com/places-to-visit-in-islamabad-and-fun-activities\n",
      "  âœ“ Discovered [28/30]: https://thesintrahotel.com/blog-hotel-islamabad-updates\n",
      "  âœ“ Discovered [29/30]: https://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "  âœ“ Discovered [30/30]: https://thesintrahotel.com\n",
      "âœ… Crawl complete! Discovered 30 pages\n",
      "\n",
      "ğŸ¤– Using LLM to select top 10 most relevant pages...\n",
      "ğŸ” LLM raw response: [1, 2, 12, 29, 9, 10, 8, 20, 21, 3]\n",
      "âœ… LLM selected 10 pages:\n",
      "   1. http://thesintrahotel.com\n",
      "   2. http://thesintrahotel.com/about-online-hotel-booking-islamabad\n",
      "   3. http://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "   4. https://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "   5. http://thesintrahotel.com/spice-fusion-top-restaurant-in-islamabad\n",
      "   6. http://thesintrahotel.com/soir-top-cafe-in-islamabad\n",
      "   7. http://thesintrahotel.com/places-to-visit-in-islamabad-and-fun-activities\n",
      "   8. http://thesintrahotel.com/terms-and-conditions-sintra-hotel-islamabad\n",
      "   9. http://thesintrahotel.com/privacy-policy-sintra-hotel-islamabad\n",
      "   10. http://thesintrahotel.com/executive-room-at-hotel-islamabad\n",
      "\n",
      "ğŸ“¥ Extracting content from 10 selected pages...\n",
      "  [1/10] Extracting: http://thesintrahotel.com\n",
      "      âœ“ Extracted 9102 characters\n",
      "  [2/10] Extracting: http://thesintrahotel.com/about-online-hotel-booking-islamabad\n",
      "      âœ“ Extracted 13800 characters\n",
      "  [3/10] Extracting: http://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "      âœ“ Extracted 8008 characters\n",
      "  [4/10] Extracting: https://thesintrahotel.com/sintra-hotel-islamabad-contact-number\n",
      "      âœ“ Extracted 8008 characters\n",
      "  [5/10] Extracting: http://thesintrahotel.com/spice-fusion-top-restaurant-in-islamabad\n",
      "      âœ“ Extracted 10815 characters\n",
      "  [6/10] Extracting: http://thesintrahotel.com/soir-top-cafe-in-islamabad\n",
      "      âœ“ Extracted 10887 characters\n",
      "  [7/10] Extracting: http://thesintrahotel.com/places-to-visit-in-islamabad-and-fun-activities\n",
      "      âœ“ Extracted 14095 characters\n",
      "  [8/10] Extracting: http://thesintrahotel.com/terms-and-conditions-sintra-hotel-islamabad\n",
      "      âœ“ Extracted 11606 characters\n",
      "  [9/10] Extracting: http://thesintrahotel.com/privacy-policy-sintra-hotel-islamabad\n",
      "      âœ“ Extracted 14142 characters\n",
      "  [10/10] Extracting: http://thesintrahotel.com/executive-room-at-hotel-islamabad\n",
      "      âœ“ Extracted 9800 characters\n",
      "\n",
      "ğŸ¤– Using LLM to extract consolidated business data...\n",
      "ğŸ”— Found 3 social media links in HTML\n",
      "ğŸ“§ Found 3 email hints in text\n",
      "ğŸ“± Found 4 phone hints in text\n",
      "ğŸ“Š Total content length: 80032 characters\n",
      "ğŸ”„ Calling OpenAI API with improved prompts...\n",
      "ğŸ” LLM response received (1995 chars)\n",
      "âœ… Data extraction successful via LLM!\n",
      "\n",
      "================================================================================\n",
      "âœ… EXTRACTION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ’¾ Results saved to: sintra_hotel_islamabad_20251128_184500.json\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š EXTRACTION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ¢ Company Name: Sintra Hotel Islamabad\n",
      "ğŸŒ Website: http://thesintrahotel.com/\n",
      "\n",
      "ğŸ“§ Emails (3 found):\n",
      "   â€¢ info@thesintrahotel.com\n",
      "   â€¢ Info@thesintrahotel.com\n",
      "   â€¢ info@sintrahotel.com\n",
      "\n",
      "ğŸ“± Phone Numbers (2 found):\n",
      "   â€¢ +92 51-8736313\n",
      "   â€¢ +92 333-4870007\n",
      "\n",
      "ğŸ”— Social Media (3 links):\n",
      "   â€¢ Facebook: https://www.facebook.com/profile.php\n",
      "   â€¢ Instagram: https://www.instagram.com/sintrahotel\n",
      "   â€¢ TikTok: https://www.tiktok.com/@sintrahotelislamabad\n",
      "\n",
      "ğŸ“ Business Summary:\n",
      "   Sintra Hotel Islamabad is a luxury hotel located in the heart of Islamabad, specifically in the vibrant Melody area near the Blue Area, making it an ideal choice for both business and leisure travelers. The hotel offers a range of elegantly designed accommodations, including executive rooms, twin executive rooms, and super deluxe rooms, all equipped with modern amenities such as high-speed Wi-Fi, flat-screen TVs, and 24-hour room service. Sintra Hotel caters to families, tourists, and business professionals, providing exceptional hospitality and personalized services to ensure a memorable stay. The hotel features two dining options: Spice Fusion Restaurant, which serves a blend of traditional Pakistani and international cuisines, and CafÃ© SOIR, known for its exquisite coffee and light snacks. Sintra Hotel stands out for its prime location, offering easy access to major tourist attractions like Faisal Mosque and Daman-e-Koh, as well as shopping centers like Centaurus Mall. The hotel also provides business services, including fully equipped conference rooms and customized catering options, making it suitable for corporate events. With a commitment to guest comfort and convenience, Sintra Hotel offers complimentary amenities such as breakfast, free parking, and a pick-up and drop-off service. The hotel values customer satisfaction and aims to create a relaxing and rejuvenating environment for all guests. Overall, Sintra Hotel Islamabad combines luxury, comfort, and convenience, making it one of the top choices for travelers visiting the capital city.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ File saved locally: sintra_hotel_islamabad_20251128_184500.json\n",
      "\n",
      "âœ… Script execution complete!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” Enhanced Intelligent Web Scraper - Fixed Version\n",
    "# Improved LLM extraction + Better fallback handling\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import openai\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
    "except:\n",
    "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'your-api-key-here')\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "MAX_CRAWL_DEPTH = 2\n",
    "MAX_PAGES = 30\n",
    "TOP_PAGES_TO_ANALYZE = 10\n",
    "REQUEST_TIMEOUT = 10\n",
    "RATE_LIMIT_DELAY = 1\n",
    "\n",
    "ua = UserAgent()\n",
    "print(\"âœ… Configuration loaded successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# GOOGLE MAPS URL PROCESSOR\n",
    "# ============================================================================\n",
    "\n",
    "def setup_selenium_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument(f'user-agent={ua.random}')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def extract_website_from_google_maps(maps_url, use_selenium=True):\n",
    "    print(f\"ğŸ” Extracting website from Google Maps URL...\")\n",
    "    \n",
    "    if use_selenium:\n",
    "        driver = None\n",
    "        try:\n",
    "            driver = setup_selenium_driver()\n",
    "            driver.get(maps_url)\n",
    "            time.sleep(3)\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[data-tooltip='Open website']\",\n",
    "                \"button[data-item-id='authority']\"\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    elements = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                    for element in elements:\n",
    "                        href = element.get_attribute('href')\n",
    "                        if href and 'google.com' not in href:\n",
    "                            print(f\"âœ… Found website: {href}\")\n",
    "                            return href\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('http') and 'google.com' not in href and 'gstatic.com' not in href:\n",
    "                    if not any(x in href for x in ['/maps/', '/search?', 'youtube.com', 'facebook.com']):\n",
    "                        print(f\"âœ… Found website via fallback: {href}\")\n",
    "                        return href\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with Selenium: {e}\")\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(maps_url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('http') and 'google.com' not in href:\n",
    "                print(f\"âœ… Found website: {href}\")\n",
    "                return href\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with requests: {e}\")\n",
    "    \n",
    "    print(\"âš ï¸ Could not extract website URL\")\n",
    "    return None\n",
    "\n",
    "# ============================================================================\n",
    "# WEBSITE CRAWLER\n",
    "# ============================================================================\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        base_parsed = urlparse(base_domain)\n",
    "        if parsed.netloc != base_parsed.netloc:\n",
    "            return False\n",
    "        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.zip', '.exe', '.mp4', '.mp3']\n",
    "        if any(url.lower().endswith(ext) for ext in skip_extensions):\n",
    "            return False\n",
    "        skip_patterns = ['#', 'javascript:', 'mailto:', 'tel:', '/cdn-cgi/', '/wp-admin/']\n",
    "        if any(pattern in url.lower() for pattern in skip_patterns):\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def crawl_website(start_url, max_depth=MAX_CRAWL_DEPTH, max_pages=MAX_PAGES):\n",
    "    print(f\"ğŸ•·ï¸ Starting website crawl from: {start_url}\")\n",
    "    \n",
    "    visited = set()\n",
    "    to_visit = deque([(start_url.rstrip('/'), 0)])\n",
    "    pages = []\n",
    "    base_domain = f\"{urlparse(start_url).scheme}://{urlparse(start_url).netloc}\"\n",
    "    \n",
    "    while to_visit and len(pages) < max_pages:\n",
    "        current_url, depth = to_visit.popleft()\n",
    "        current_url = current_url.rstrip('/')\n",
    "        if current_url in visited or depth > max_depth:\n",
    "            continue\n",
    "        visited.add(current_url)\n",
    "        try:\n",
    "            headers = {'User-Agent': ua.random}\n",
    "            response = requests.get(current_url, headers=headers, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                continue\n",
    "            pages.append(current_url)\n",
    "            print(f\"  âœ“ Discovered [{len(pages)}/{max_pages}]: {current_url}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(current_url, link['href'])\n",
    "                clean_url = absolute_url.split('#')[0].split('?')[0].rstrip('/')\n",
    "                if is_valid_url(clean_url, base_domain) and clean_url not in visited:\n",
    "                    to_visit.append((clean_url, depth + 1))\n",
    "            time.sleep(RATE_LIMIT_DELAY)\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— Error crawling {current_url}: {str(e)[:50]}\")\n",
    "    \n",
    "    print(f\"âœ… Crawl complete! Discovered {len(pages)} pages\")\n",
    "    return pages\n",
    "\n",
    "# ============================================================================\n",
    "# LLM-POWERED PAGE SELECTION\n",
    "# ============================================================================\n",
    "\n",
    "def select_relevant_pages_with_llm(page_urls, top_n=TOP_PAGES_TO_ANALYZE):\n",
    "    print(f\"\\nğŸ¤– Using LLM to select top {top_n} most relevant pages...\")\n",
    "    url_list = \"\\n\".join([f\"{i+1}. {url}\" for i, url in enumerate(page_urls)])\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert web analyst. Your job is to identify the most relevant pages that contain business information like company details, contact info, services, and about information.\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"Analyze these {len(page_urls)} URLs and select the top {top_n} most relevant pages for extracting company information (like contact details, about us, services, etc.).\n",
    "\n",
    "URLs:\n",
    "{url_list}\n",
    "\n",
    "Return ONLY a valid JSON array of numbers (1-indexed positions) like: [1, 3, 5, 7, 9]\n",
    "Do not include any other text or explanation.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"ğŸ” LLM raw response: {content[:200]}\")\n",
    "        \n",
    "        if '```' in content:\n",
    "            content = content.split('```')[1]\n",
    "            if content.startswith('json'):\n",
    "                content = content[4:]\n",
    "            content = content.strip()\n",
    "        \n",
    "        selected_indices = json.loads(content)\n",
    "        selected_urls = [page_urls[i-1] for i in selected_indices if 0 < i <= len(page_urls)]\n",
    "        \n",
    "        print(f\"âœ… LLM selected {len(selected_urls)} pages:\")\n",
    "        for i, url in enumerate(selected_urls, 1):\n",
    "            print(f\"   {i}. {url}\")\n",
    "        return selected_urls[:top_n]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LLM selection error: {e}\")\n",
    "        print(\"ğŸ”„ Using heuristic fallback...\")\n",
    "        priority_keywords = ['home', 'about', 'contact', 'service', 'product', 'portfolio', 'team', 'company']\n",
    "        scored_pages = []\n",
    "        for url in page_urls:\n",
    "            url_lower = url.lower()\n",
    "            score = sum(2 for keyword in priority_keywords if keyword in url_lower)\n",
    "            score += (100 - len(url)) / 100\n",
    "            scored_pages.append((score, url))\n",
    "        scored_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        selected = [url for _, url in scored_pages[:top_n]]\n",
    "        print(f\"âœ… Heuristic selected {len(selected)} pages\")\n",
    "        return selected\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT EXTRACTOR\n",
    "# ============================================================================\n",
    "\n",
    "def extract_page_content(url):\n",
    "    \"\"\"Extract both text content and raw HTML for better social media detection\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        response = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        if response.status_code != 200:\n",
    "            return \"\", \"\"\n",
    "        \n",
    "        html_content = response.text\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        for script in soup(['script', 'style', 'iframe', 'noscript']):\n",
    "            script.decompose()\n",
    "        \n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        cleaned_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return cleaned_text, html_content\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error extracting content from {url}: {e}\")\n",
    "        return \"\", \"\"\n",
    "\n",
    "def extract_social_from_html(html_list):\n",
    "    \"\"\"Extract social media links directly from raw HTML\"\"\"\n",
    "    if not html_list:\n",
    "        return []\n",
    "    \n",
    "    all_html = \" \".join(html_list)\n",
    "    socials = set()\n",
    "    \n",
    "    patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        found = re.findall(pattern, all_html, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'[\"\\'>].*$', '', link)\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            if len(link) > 20:\n",
    "                socials.add(link)\n",
    "    \n",
    "    return list(socials)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED LLM EXTRACTION WITH BETTER PROMPTS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_business_data_with_llm(page_contents, main_website_url, all_html=None):\n",
    "    print(\"\\nğŸ¤– Using LLM to extract consolidated business data...\")\n",
    "    \n",
    "    # Extract social media from HTML first\n",
    "    social_media_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    print(f\"ğŸ”— Found {len(social_media_from_html)} social media links in HTML\")\n",
    "    \n",
    "    # Also extract emails and phones from raw content as hints\n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    email_hints = list(set(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', all_text)))\n",
    "    phone_hints = list(set(re.findall(r'\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d', all_text)))\n",
    "    \n",
    "    print(f\"ğŸ“§ Found {len(email_hints)} email hints in text\")\n",
    "    print(f\"ğŸ“± Found {len(phone_hints)} phone hints in text\")\n",
    "    \n",
    "    # Prepare content with smart truncation\n",
    "    combined_content = \"\"\n",
    "    max_chars_per_page = 10000  # Increased for better context\n",
    "    \n",
    "    for url, content in page_contents.items():\n",
    "        truncated_content = content[:max_chars_per_page] if len(content) > max_chars_per_page else content\n",
    "        combined_content += f\"\\n\\n=== PAGE: {url} ===\\n{truncated_content}\\n\"\n",
    "    \n",
    "    max_total_chars = 80000  # Increased limit\n",
    "    if len(combined_content) > max_total_chars:\n",
    "        combined_content = combined_content[:max_total_chars] + \"\\n\\n[Content truncated for length]\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Total content length: {len(combined_content)} characters\")\n",
    "\n",
    "    # Build hints for the LLM\n",
    "    hints = \"\"\n",
    "    if social_media_from_html:\n",
    "        hints += f\"\\n\\nSOCIAL MEDIA FOUND IN HTML: {', '.join(social_media_from_html[:5])}\"\n",
    "    if email_hints:\n",
    "        hints += f\"\\n\\nEMAIL HINTS FOUND: {', '.join(email_hints[:5])}\"\n",
    "    if phone_hints:\n",
    "        hints += f\"\\n\\nPHONE HINTS FOUND: {', '.join(phone_hints[:5])}\"\n",
    "\n",
    "    # IMPROVED SYSTEM PROMPT\n",
    "    system_prompt = \"\"\"You are an expert business intelligence analyst specializing in extracting comprehensive company information from websites. \n",
    "\n",
    "Your task is to:\n",
    "1. Carefully analyze ALL provided content from multiple pages\n",
    "2. Extract EVERY piece of contact information (emails, phones, social media)\n",
    "3. Write a detailed, informative business summary (8-12 sentences minimum)\n",
    "4. Return clean, valid JSON with no markdown formatting\n",
    "\n",
    "Be thorough and precise. The summary should give readers a complete understanding of what the business does.\"\"\"\n",
    "    \n",
    "    # IMPROVED USER PROMPT with better instructions\n",
    "    user_prompt = f\"\"\"Analyze this complete website content for: {main_website_url}\n",
    "\n",
    "{combined_content}{hints}\n",
    "\n",
    "Extract comprehensive business information and return ONLY a JSON object (no markdown, no explanation):\n",
    "\n",
    "{{\n",
    "  \"company_name\": \"Full official company name\",\n",
    "  \"company_main_url\": \"{main_website_url}\",\n",
    "  \"emails\": [\"all emails found\"],\n",
    "  \"contact_numbers\": [\"all phone numbers with country codes if available\"],\n",
    "  \"social_media_links\": [\"all social media URLs\"],\n",
    "  \"summary\": \"DETAILED 8-12 sentence summary covering: (1) What the company does and its main business focus, (2) Primary products/services offered with specifics, (3) Target market or customer base, (4) Unique selling points or competitive advantages, (5) Company values or mission if mentioned, (6) Notable achievements or credentials, (7) Geographic service area if mentioned, (8) Any other relevant business details\"\n",
    "}}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- EMAILS: Search for email addresses in contact pages, footers, about pages. Look for patterns like name@domain.com\n",
    "- PHONES: Find ALL phone numbers including those in \"Contact Us\", \"Call Us\", footers, headers\n",
    "- SOCIAL MEDIA: Include Facebook, Instagram, LinkedIn, Twitter/X, YouTube, TikTok, Pinterest links\n",
    "- SUMMARY: Must be 8-12 sentences minimum. Be specific about what they do, not generic. Use actual details from the content.\n",
    "- Use empty arrays [] for missing data, never null or omit fields\n",
    "- Return ONLY valid JSON, no markdown backticks or preamble\"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"ğŸ”„ Calling OpenAI API with improved prompts...\")\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1,  # Low temperature for consistency\n",
    "            max_tokens=6000,  # Increased for detailed summaries\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        \n",
    "        content = response.choices[0].message.content.strip()\n",
    "        print(f\"ğŸ” LLM response received ({len(content)} chars)\")\n",
    "        \n",
    "        # Parse and validate\n",
    "        extracted_data = json.loads(content)\n",
    "        \n",
    "        # Merge with HTML-extracted social media\n",
    "        if social_media_from_html:\n",
    "            existing_socials = set(extracted_data.get('social_media_links', []))\n",
    "            all_socials = existing_socials.union(set(social_media_from_html))\n",
    "            extracted_data['social_media_links'] = list(all_socials)\n",
    "        \n",
    "        # Validate and clean\n",
    "        extracted_data = validate_and_clean_data(extracted_data, main_website_url)\n",
    "        \n",
    "        # Check if summary is too generic or short\n",
    "        summary = extracted_data.get('summary', '')\n",
    "        if len(summary) < 200 or 'unable to generate' in summary.lower():\n",
    "            print(\"âš ï¸ Summary too short or generic, regenerating...\")\n",
    "            extracted_data['summary'] = generate_better_summary(all_text, extracted_data['company_name'])\n",
    "        \n",
    "        # If still no contact info after LLM, try regex fallback\n",
    "        if not extracted_data.get('emails'):\n",
    "            print(\"âš ï¸ No emails from LLM, using regex fallback...\")\n",
    "            extracted_data['emails'] = email_hints[:10]\n",
    "        \n",
    "        if not extracted_data.get('contact_numbers'):\n",
    "            print(\"âš ï¸ No phones from LLM, using regex fallback...\")\n",
    "            extracted_data['contact_numbers'] = phone_hints[:10]\n",
    "        \n",
    "        print(\"âœ… Data extraction successful via LLM!\")\n",
    "        return extracted_data\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âŒ JSON parsing error: {e}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ LLM extraction error: {type(e).__name__}: {e}\")\n",
    "        return create_fallback_data(main_website_url, page_contents, all_html)\n",
    "\n",
    "def generate_better_summary(content, company_name):\n",
    "    \"\"\"Generate a more detailed summary with specific instructions\"\"\"\n",
    "    try:\n",
    "        truncated = content[:20000]  # More content for better summary\n",
    "        \n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a business analyst who writes detailed, informative company summaries. Never write generic summaries.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"\"\"Write a comprehensive 8-12 sentence business summary for {company_name} based on this website content:\n",
    "\n",
    "{truncated}\n",
    "\n",
    "Your summary MUST include:\n",
    "1. What specific products/services they offer (be detailed, not generic)\n",
    "2. Who their target customers are\n",
    "3. What makes them unique or different\n",
    "4. Their business approach or values\n",
    "5. Any notable achievements, credentials, or experience mentioned\n",
    "6. Geographic area they serve if mentioned\n",
    "7. Specific details about their offerings (menu items, services, specialties, etc.)\n",
    "\n",
    "Write in a professional but engaging tone. Be specific and use actual details from the content. Start directly with the summary.\"\"\"}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=800\n",
    "        )\n",
    "        \n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        print(f\"âœ… Generated detailed summary ({len(summary)} chars)\")\n",
    "        return summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not generate detailed summary: {e}\")\n",
    "        return f\"{company_name} is a business operating at the provided website. Due to technical limitations, a detailed summary could not be generated. Please visit their website directly for comprehensive information about their products, services, and business offerings.\"\n",
    "\n",
    "def validate_and_clean_data(data, main_url):\n",
    "    \"\"\"Validate and clean extracted data\"\"\"\n",
    "    cleaned = {\n",
    "        \"company_name\": data.get(\"company_name\") or urlparse(main_url).netloc.replace('www.', '').split('.')[0].title(),\n",
    "        \"company_main_url\": main_url,\n",
    "        \"emails\": [],\n",
    "        \"contact_numbers\": [],\n",
    "        \"social_media_links\": [],\n",
    "        \"summary\": data.get(\"summary\") or \"No summary available\"\n",
    "    }\n",
    "    \n",
    "    # Validate emails\n",
    "    if data.get(\"emails\") and isinstance(data[\"emails\"], list):\n",
    "        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "        cleaned[\"emails\"] = [e.strip() for e in data[\"emails\"] if re.match(email_pattern, e.strip())]\n",
    "    \n",
    "    # Validate phone numbers\n",
    "    if data.get(\"contact_numbers\") and isinstance(data[\"contact_numbers\"], list):\n",
    "        cleaned[\"contact_numbers\"] = [p.strip() for p in data[\"contact_numbers\"] if p and len(str(p).strip()) > 5]\n",
    "    \n",
    "    # Validate social media\n",
    "    if data.get(\"social_media_links\") and isinstance(data[\"social_media_links\"], list):\n",
    "        social_domains = ['facebook.com', 'instagram.com', 'linkedin.com', 'twitter.com', 'x.com', \n",
    "                         'youtube.com', 'tiktok.com', 'pinterest.com']\n",
    "        cleaned[\"social_media_links\"] = [\n",
    "            s.strip() for s in data[\"social_media_links\"] \n",
    "            if s and any(domain in s.lower() for domain in social_domains)\n",
    "        ]\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# ============================================================================\n",
    "# IMPROVED FALLBACK EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def create_fallback_data(main_website_url, page_contents, all_html=None):\n",
    "    \"\"\"Enhanced fallback extraction with better patterns\"\"\"\n",
    "    print(\"ğŸ”„ Using enhanced fallback extraction...\")\n",
    "    \n",
    "    all_text = \" \".join(page_contents.values())\n",
    "    social_from_html = extract_social_from_html(all_html) if all_html else []\n",
    "    \n",
    "    # Email extraction\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
    "    emails = list(set(re.findall(email_pattern, all_text)))\n",
    "    emails = [e for e in emails if not e.endswith(('.png', '.jpg', '.gif', '.svg'))]\n",
    "    \n",
    "    # Phone extraction with multiple patterns\n",
    "    phone_patterns = [\n",
    "        r'\\+\\d{1,3}[-.\\s]?\\(?\\d{1,4}\\)?[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,4}[-.\\s]?\\d{1,9}',\n",
    "        r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "        r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "    ]\n",
    "    \n",
    "    phones = set()\n",
    "    for pattern in phone_patterns:\n",
    "        found = re.findall(pattern, all_text)\n",
    "        for phone in found:\n",
    "            cleaned = re.sub(r'[^\\d+()-]', '', phone)\n",
    "            if len(re.sub(r'[^\\d]', '', cleaned)) >= 10:\n",
    "                phones.add(phone.strip())\n",
    "    \n",
    "    # Social media extraction from text\n",
    "    socials_from_text = set()\n",
    "    social_patterns = [\n",
    "        r'https?://(?:www\\.)?facebook\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?instagram\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?linkedin\\.com/(?:company|in)/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?(?:twitter|x)\\.com/[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?youtube\\.com/(?:c|channel|user|@)[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?tiktok\\.com/@[a-zA-Z0-9._/-]+',\n",
    "        r'https?://(?:www\\.)?pinterest\\.com/[a-zA-Z0-9._/-]+',\n",
    "    ]\n",
    "    \n",
    "    for pattern in social_patterns:\n",
    "        found = re.findall(pattern, all_text, re.IGNORECASE)\n",
    "        for link in found:\n",
    "            link = link.rstrip('/')\n",
    "            link = re.sub(r'(\\?.*|#.*)$', '', link)\n",
    "            socials_from_text.add(link)\n",
    "    \n",
    "    all_socials = set(social_from_html).union(socials_from_text)\n",
    "    \n",
    "    # Company name\n",
    "    company_name = urlparse(main_website_url).netloc.replace('www.', '').split('.')[0]\n",
    "    company_name = ' '.join(word.capitalize() for word in re.split(r'[-_]', company_name))\n",
    "    \n",
    "    # Generate summary with LLM\n",
    "    summary = generate_better_summary(all_text, company_name)\n",
    "    \n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"company_main_url\": main_website_url,\n",
    "        \"emails\": sorted(list(set(emails)))[:15],\n",
    "        \"contact_numbers\": sorted(list(phones))[:15],\n",
    "        \"social_media_links\": sorted(list(all_socials)),\n",
    "        \"summary\": summary,\n",
    "        \"extraction_method\": \"enhanced_regex_fallback_with_llm_summary\"\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "def scrape_business_data(google_maps_url):\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸš€ STARTING INTELLIGENT BUSINESS DATA EXTRACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    website_url = extract_website_from_google_maps(google_maps_url)\n",
    "    if not website_url:\n",
    "        print(\"âŒ Failed to extract website URL from Google Maps\")\n",
    "        return None, None, 0, 0\n",
    "\n",
    "    parsed = urlparse(website_url)\n",
    "    website_url = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    print(f\"\\nğŸ“ Main Website: {website_url}\\n\")\n",
    "    \n",
    "    all_pages = crawl_website(website_url)\n",
    "    if not all_pages:\n",
    "        print(\"âŒ No pages found to scrape\")\n",
    "        return None, None, 0, 0\n",
    "    \n",
    "    selected_pages = select_relevant_pages_with_llm(all_pages, TOP_PAGES_TO_ANALYZE)\n",
    "    if not selected_pages:\n",
    "        print(\"âŒ No pages selected for analysis\")\n",
    "        return None, None, len(all_pages), 0\n",
    "\n",
    "    print(f\"\\nğŸ“¥ Extracting content from {len(selected_pages)} selected pages...\")\n",
    "    page_contents = {}\n",
    "    all_html = []\n",
    "    \n",
    "    for i, page_url in enumerate(selected_pages, 1):\n",
    "        print(f\"  [{i}/{len(selected_pages)}] Extracting: {page_url}\")\n",
    "        text_content, html_content = extract_page_content(page_url)\n",
    "        if text_content:\n",
    "            page_contents[page_url] = text_content\n",
    "            all_html.append(html_content)\n",
    "            print(f\"      âœ“ Extracted {len(text_content)} characters\")\n",
    "        time.sleep(RATE_LIMIT_DELAY)\n",
    "    \n",
    "    if not page_contents:\n",
    "        print(\"âŒ No content extracted from any page\")\n",
    "        return None, None, len(all_pages), len(selected_pages)\n",
    "\n",
    "    extracted_data = extract_business_data_with_llm(page_contents, website_url, all_html)\n",
    "    business_name = extracted_data.get('company_name', 'unknown_business')\n",
    "    if business_name:\n",
    "        business_name = re.sub(r'[^\\w\\s-]', '', str(business_name))\n",
    "        business_name = re.sub(r'[-\\s]+', '_', business_name).lower()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… EXTRACTION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return extracted_data, business_name, len(all_pages), len(page_contents)\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AND DISPLAY\n",
    "# ============================================================================\n",
    "\n",
    "def save_results(extracted_data, business_name, all_pages_count, selected_pages_count):\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{business_name}_{timestamp}.json\"\n",
    "    output = {\n",
    "        \"business_data\": extracted_data,\n",
    "        \"extraction_metadata\": {\n",
    "            \"total_pages_discovered\": all_pages_count,\n",
    "            \"pages_analyzed\": selected_pages_count,\n",
    "            \"extraction_method\": \"LLM-powered intelligent extraction\",\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"model_used\": \"gpt-4o-mini\"\n",
    "        }\n",
    "    }\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"\\nğŸ’¾ Results saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def display_summary(extracted_data):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ“Š EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ¢ Company Name: {extracted_data.get('company_name', 'N/A')}\")\n",
    "    print(f\"ğŸŒ Website: {extracted_data.get('company_main_url', 'N/A')}\")\n",
    "    \n",
    "    emails = extracted_data.get('emails', [])\n",
    "    print(f\"\\nğŸ“§ Emails ({len(emails)} found):\")\n",
    "    if emails:\n",
    "        for email in emails[:5]:\n",
    "            print(f\"   â€¢ {email}\")\n",
    "        if len(emails) > 5:\n",
    "            print(f\"   ... and {len(emails) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    phones = extracted_data.get('contact_numbers', [])\n",
    "    print(f\"\\nğŸ“± Phone Numbers ({len(phones)} found):\")\n",
    "    if phones:\n",
    "        for phone in phones[:5]:\n",
    "            print(f\"   â€¢ {phone}\")\n",
    "        if len(phones) > 5:\n",
    "            print(f\"   ... and {len(phones) - 5} more\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    socials = extracted_data.get('social_media_links', [])\n",
    "    print(f\"\\nğŸ”— Social Media ({len(socials)} links):\")\n",
    "    if socials:\n",
    "        for link in socials:\n",
    "            platform = \"Unknown\"\n",
    "            if 'facebook.com' in link: platform = \"Facebook\"\n",
    "            elif 'instagram.com' in link: platform = \"Instagram\"\n",
    "            elif 'linkedin.com' in link: platform = \"LinkedIn\"\n",
    "            elif 'twitter.com' in link or 'x.com' in link: platform = \"Twitter/X\"\n",
    "            elif 'youtube.com' in link: platform = \"YouTube\"\n",
    "            elif 'tiktok.com' in link: platform = \"TikTok\"\n",
    "            print(f\"   â€¢ {platform}: {link}\")\n",
    "    else:\n",
    "        print(\"   None found\")\n",
    "    \n",
    "    summary = extracted_data.get('summary', 'N/A')\n",
    "    print(f\"\\nğŸ“ Business Summary:\")\n",
    "    print(f\"   {summary}\")\n",
    "    \n",
    "    if 'extraction_method' in extracted_data:\n",
    "        print(f\"\\nâš™ï¸ Extraction Method: {extracted_data['extraction_method']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "GOOGLE_MAPS_URL = \"https://maps.app.goo.gl/docBXMhBUZ5qGZ3f9\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not OPENAI_API_KEY or OPENAI_API_KEY == \"your-api-key-here\":\n",
    "        print(\"âŒ ERROR: Please set your OPENAI_API_KEY!\")\n",
    "    else:\n",
    "        extracted_data, business_name, all_pages_discovered, pages_analyzed = scrape_business_data(GOOGLE_MAPS_URL)\n",
    "        \n",
    "        if extracted_data and business_name:\n",
    "            filename = save_results(\n",
    "                extracted_data, \n",
    "                business_name,\n",
    "                all_pages_discovered,\n",
    "                pages_analyzed\n",
    "            )\n",
    "            display_summary(extracted_data)\n",
    "            try:\n",
    "                from google.colab import files\n",
    "                files.download(filename)\n",
    "                print(\"ğŸ“¥ File download started!\")\n",
    "            except:\n",
    "                print(f\"ğŸ“ File saved locally: {filename}\")\n",
    "        else:\n",
    "            print(\"âŒ No data extracted. Check the URL and API key, then try again.\")\n",
    "\n",
    "print(\"\\nâœ… Script execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
