{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Using cached trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.10.5 (from selenium)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.15.0)\n",
      "Collecting websocket-client<2.0,>=1.8.0 (from selenium)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.3.0)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached cffi-2.0.0-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Using cached selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Using cached charset_normalizer-3.4.4-cp312-cp312-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Using cached trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached cffi-2.0.0-cp312-cp312-win_amd64.whl (183 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, urllib3, soupsieve, sniffio, pysocks, pycparser, outcome, idna, h11, charset_normalizer, certifi, wsproto, requests, cffi, beautifulsoup4, trio, trio-websocket, selenium\n",
      "Successfully installed beautifulsoup4-4.14.2 certifi-2025.10.5 cffi-2.0.0 charset_normalizer-3.4.4 h11-0.16.0 idna-3.11 outcome-1.3.0.post0 pycparser-2.23 pysocks-1.7.1 requests-2.32.5 selenium-4.38.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.8 trio-0.32.0 trio-websocket-0.12.2 urllib3-2.5.0 websocket-client-1.9.0 wsproto-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from webdriver-manager) (2.32.5)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\abc\\appdata\\roaming\\python\\python312\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abc\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->webdriver-manager) (2025.10.5)\n",
      "Using cached webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.2.1 webdriver-manager-4.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\n",
      "======================================================================\n",
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/Broadway+Pizza+-+Satellite+Town/@33.6333227,73.0342839,14z/data=!4m6!3m5!1s0x38df95373f97dfa5:0xf9c07921bce37c26!8m2!3d33.6333227!4d73.0703328!16s%2Fg%2F11ppw3hd9k?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n",
      "âœ… Found website: http://www.broadwaypizza.com.pk/\n",
      "\n",
      "ðŸŒ Starting website scrape: http://www.broadwaypizza.com.pk/\n",
      "ðŸ“„ Scraping: http://www.broadwaypizza.com.pk/\n",
      "ðŸ“„ Scraping: http://www.broadwaypizza.com.pk/about/\n",
      "âš ï¸  Error scraping http://www.broadwaypizza.com.pk/about/: 404 Client Error: Not Found for url: http://www.broadwaypizza.com.pk/about/\n",
      "ðŸ“„ Scraping: http://www.broadwaypizza.com.pk/contact\n",
      "âš ï¸  Error scraping http://www.broadwaypizza.com.pk/contact: 404 Client Error: Not Found for url: http://www.broadwaypizza.com.pk/contact\n",
      "\n",
      "âœ… Scraping complete!\n",
      "   Emails found: 0\n",
      "   Phones found: 0\n",
      "   Social links: 0\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š RESULTS\n",
      "======================================================================\n",
      "{\n",
      "  \"website\": \"http://www.broadwaypizza.com.pk/\",\n",
      "  \"emails\": [],\n",
      "  \"phones\": [],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [],\n",
      "    \"instagram\": [],\n",
      "    \"linkedin\": [],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": []\n",
      "  },\n",
      "  \"additional_pages\": [\n",
      "    \"http://www.broadwaypizza.com.pk/about/\",\n",
      "    \"http://www.broadwaypizza.com.pk/contact\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Google Maps to Business Website Contact Info Scraper\n",
    "Extracts emails, phones, and social media from business websites found on Google Maps.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "\n",
    "class BusinessInfoScraper:\n",
    "    \"\"\"Scrapes business contact information from Google Maps and business websites.\"\"\"\n",
    "    \n",
    "    # Regex patterns\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    \n",
    "    # Social media domains\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "    \n",
    "    def __init__(self, headless: bool = True):\n",
    "        \"\"\"Initialize the scraper with Selenium options.\"\"\"\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Initialize Chrome WebDriver with appropriate options.\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        return webdriver.Chrome(options=options)\n",
    "    \n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract the business website URL from Google Maps listing.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Business website URL or None if not found\n",
    "        \"\"\"\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            \n",
    "            # Wait for page to load and find website button\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            \n",
    "            # Try multiple selectors for the website button\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[href*='http']:has-text('Website')\",\n",
    "                \"//a[contains(@aria-label, 'Website')]\",\n",
    "                \"//a[contains(., 'Website')]\"\n",
    "            ]\n",
    "            \n",
    "            website_url = None\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    if selector.startswith('//'):\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.XPATH, selector))\n",
    "                        )\n",
    "                    else:\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                        )\n",
    "                    \n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # Clean up Google redirect URLs\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            match = re.search(r'[?&]q=([^&]+)', website_url)\n",
    "                            if match:\n",
    "                                from urllib.parse import unquote\n",
    "                                website_url = unquote(match.group(1))\n",
    "                        break\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    continue\n",
    "            \n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found on Google Maps listing\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting website from Google Maps: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract email addresses from text.\"\"\"\n",
    "        emails = self.EMAIL_PATTERN.findall(text)\n",
    "        # Filter out common false positives\n",
    "        filtered = {\n",
    "            email.lower() for email in emails\n",
    "            if not email.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg'))\n",
    "        }\n",
    "        return filtered\n",
    "    \n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract phone numbers from text.\"\"\"\n",
    "        phones = self.PHONE_PATTERN.findall(text)\n",
    "        # Clean and filter phone numbers\n",
    "        cleaned = set()\n",
    "        for phone in phones:\n",
    "            # Remove excessive spaces/dashes\n",
    "            clean = re.sub(r'\\s+', ' ', phone.strip())\n",
    "            # Only keep if it has enough digits\n",
    "            digit_count = sum(c.isdigit() for c in clean)\n",
    "            if digit_count >= 9:  # Minimum valid phone number length\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "    \n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract social media links from page.\"\"\"\n",
    "        social_links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            \n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(domain in href for domain in domains):\n",
    "                    full_url = urljoin(base_url, link['href'])\n",
    "                    if full_url not in social_links[platform]:\n",
    "                        social_links[platform].append(full_url)\n",
    "        \n",
    "        return social_links\n",
    "    \n",
    "    def _find_contact_pages(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Find links to contact and about pages.\"\"\"\n",
    "        contact_pages = []\n",
    "        keywords = ['contact', 'about', 'reach', 'connect', 'support']\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            text = link.get_text().lower()\n",
    "            \n",
    "            # Check if link contains contact-related keywords\n",
    "            if any(keyword in href or keyword in text for keyword in keywords):\n",
    "                full_url = urljoin(base_url, link['href'])\n",
    "                parsed = urlparse(full_url)\n",
    "                \n",
    "                # Only include internal links\n",
    "                if parsed.netloc == urlparse(base_url).netloc:\n",
    "                    if full_url not in contact_pages and full_url != base_url:\n",
    "                        contact_pages.append(full_url)\n",
    "        \n",
    "        return contact_pages[:5]  # Limit to 5 additional pages\n",
    "    \n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Scrape a single page for contact information.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (emails, phones, soup object)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            response = self.session.get(url, timeout=10, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for element in soup(['script', 'style', 'noscript']):\n",
    "                element.decompose()\n",
    "            \n",
    "            text = soup.get_text()\n",
    "            \n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            \n",
    "            return emails, phones, soup\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error scraping {url}: {e}\")\n",
    "            return set(), set(), None\n",
    "    \n",
    "    def scrape_business_website(self, website_url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Scrape business website for contact information.\n",
    "        \n",
    "        Args:\n",
    "            website_url: Business website URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all extracted information\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸŒ Starting website scrape: {website_url}\")\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            'website': website_url,\n",
    "            'emails': [],\n",
    "            'phones': [],\n",
    "            'social_links': {platform: [] for platform in self.SOCIAL_PLATFORMS},\n",
    "            'additional_pages': []\n",
    "        }\n",
    "        \n",
    "        all_emails = set()\n",
    "        all_phones = set()\n",
    "        pages_to_scrape = [website_url]\n",
    "        scraped_pages = set()\n",
    "        \n",
    "        # Scrape main page\n",
    "        emails, phones, soup = self._scrape_page(website_url)\n",
    "        all_emails.update(emails)\n",
    "        all_phones.update(phones)\n",
    "        scraped_pages.add(website_url)\n",
    "        \n",
    "        if soup:\n",
    "            # Extract social media links from main page\n",
    "            social_links = self._extract_social_links(soup, website_url)\n",
    "            for platform, links in social_links.items():\n",
    "                result['social_links'][platform].extend(links)\n",
    "            \n",
    "            # Find contact pages\n",
    "            contact_pages = self._find_contact_pages(soup, website_url)\n",
    "            result['additional_pages'] = contact_pages\n",
    "            \n",
    "            # Scrape additional pages\n",
    "            for page_url in contact_pages:\n",
    "                if page_url not in scraped_pages:\n",
    "                    time.sleep(1)  # Polite delay\n",
    "                    emails, phones, page_soup = self._scrape_page(page_url)\n",
    "                    all_emails.update(emails)\n",
    "                    all_phones.update(phones)\n",
    "                    scraped_pages.add(page_url)\n",
    "                    \n",
    "                    # Extract social links from additional pages\n",
    "                    if page_soup:\n",
    "                        social_links = self._extract_social_links(page_soup, website_url)\n",
    "                        for platform, links in social_links.items():\n",
    "                            for link in links:\n",
    "                                if link not in result['social_links'][platform]:\n",
    "                                    result['social_links'][platform].append(link)\n",
    "        \n",
    "        # Deduplicate and sort results\n",
    "        result['emails'] = sorted(list(all_emails))\n",
    "        result['phones'] = sorted(list(all_phones))\n",
    "        \n",
    "        # Remove duplicate social links\n",
    "        for platform in result['social_links']:\n",
    "            result['social_links'][platform] = list(set(result['social_links'][platform]))\n",
    "        \n",
    "        print(f\"\\nâœ… Scraping complete!\")\n",
    "        print(f\"   Emails found: {len(result['emails'])}\")\n",
    "        print(f\"   Phones found: {len(result['phones'])}\")\n",
    "        print(f\"   Social links: {sum(len(v) for v in result['social_links'].values())}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def scrape_from_google_maps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Complete pipeline: Google Maps â†’ Website â†’ Contact Info.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all extracted information or None if website not found\n",
    "        \"\"\"\n",
    "        # Step 1: Get website from Google Maps\n",
    "        website_url = self.get_website_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if not website_url:\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Scrape the business website\n",
    "        return self.scrape_business_website(website_url)\n",
    "    \n",
    "    def save_to_json(self, data: Dict, filename: str = 'business_info.json'):\n",
    "        \"\"\"Save scraped data to JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nðŸ’¾ Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving to JSON: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get Google Maps URL from user\n",
    "    gmaps_url = input(\"\\nðŸ“ Enter Google Maps business URL: \").strip()\n",
    "    \n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = BusinessInfoScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Run the complete scraping pipeline\n",
    "        result = scraper.scrape_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ“Š RESULTS\")\n",
    "            print(\"=\" * 70)\n",
    "            print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            # Save to file\n",
    "            scraper.save_to_json(result)\n",
    "        else:\n",
    "            print(\"\\nâŒ Could not find business website on Google Maps listing.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\n",
      "======================================================================\n",
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/Capital+Pizza/@33.6333227,73.0342839,14z/data=!4m6!3m5!1s0x38df95043e2bff5d:0x260f799fbba0b76f!8m2!3d33.6229741!4d73.0616476!16s%2Fg%2F11w1klzrk6?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n",
      "âœ… Found website: https://capitalpizza.pk/\n",
      "\n",
      "ðŸŒ Starting website scrape: https://capitalpizza.pk/\n",
      "ðŸ“„ Scraping: https://capitalpizza.pk/\n",
      "ðŸ“„ Scraping: https://capitalpizza.pk/about-us/\n",
      "ðŸ“„ Scraping: https://capitalpizza.pk/contact/\n",
      "\n",
      "âœ… Scraping complete!\n",
      "   Emails found: 2\n",
      "   Phones found: 1\n",
      "   Social links: 4\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š RESULTS\n",
      "======================================================================\n",
      "{\n",
      "  \"website\": \"https://capitalpizza.pk/\",\n",
      "  \"emails\": [\n",
      "    \"info@capitalpizza.pk\",\n",
      "    \"order@capitalpizza.pk\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"0318 893 000 3\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"https://www.facebook.com/Capitalpizza.pk\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/capitalpizza.pk/\"\n",
      "    ],\n",
      "    \"linkedin\": [\n",
      "      \"https://www.linkedin.com/company/capital-pizza-pk/\"\n",
      "    ],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": [\n",
      "      \"https://www.youtube.com/@capitalpizzapk\"\n",
      "    ]\n",
      "  },\n",
      "  \"additional_pages\": [\n",
      "    \"https://capitalpizza.pk/about-us/\",\n",
      "    \"https://capitalpizza.pk/contact/\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Google Maps to Business Website Contact Info Scraper\n",
    "Extracts emails, phones, and social media from business websites found on Google Maps.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "\n",
    "class BusinessInfoScraper:\n",
    "    \"\"\"Scrapes business contact information from Google Maps and business websites.\"\"\"\n",
    "    \n",
    "    # Regex patterns\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    \n",
    "    # Social media domains\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "    \n",
    "    def __init__(self, headless: bool = True):\n",
    "        \"\"\"Initialize the scraper with Selenium options.\"\"\"\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Initialize Chrome WebDriver with appropriate options.\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        return webdriver.Chrome(options=options)\n",
    "    \n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract the business website URL from Google Maps listing.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Business website URL or None if not found\n",
    "        \"\"\"\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            \n",
    "            # Wait for page to load and find website button\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            \n",
    "            # Try multiple selectors for the website button\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[href*='http']:has-text('Website')\",\n",
    "                \"//a[contains(@aria-label, 'Website')]\",\n",
    "                \"//a[contains(., 'Website')]\"\n",
    "            ]\n",
    "            \n",
    "            website_url = None\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    if selector.startswith('//'):\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.XPATH, selector))\n",
    "                        )\n",
    "                    else:\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                        )\n",
    "                    \n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # Clean up Google redirect URLs\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            match = re.search(r'[?&]q=([^&]+)', website_url)\n",
    "                            if match:\n",
    "                                from urllib.parse import unquote\n",
    "                                website_url = unquote(match.group(1))\n",
    "                        break\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    continue\n",
    "            \n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found on Google Maps listing\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting website from Google Maps: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract email addresses from text.\"\"\"\n",
    "        emails = self.EMAIL_PATTERN.findall(text)\n",
    "        # Filter out common false positives\n",
    "        filtered = {\n",
    "            email.lower() for email in emails\n",
    "            if not email.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg'))\n",
    "        }\n",
    "        return filtered\n",
    "    \n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract phone numbers from text.\"\"\"\n",
    "        phones = self.PHONE_PATTERN.findall(text)\n",
    "        # Clean and filter phone numbers\n",
    "        cleaned = set()\n",
    "        for phone in phones:\n",
    "            # Remove excessive spaces/dashes\n",
    "            clean = re.sub(r'\\s+', ' ', phone.strip())\n",
    "            # Only keep if it has enough digits\n",
    "            digit_count = sum(c.isdigit() for c in clean)\n",
    "            if digit_count >= 9:  # Minimum valid phone number length\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "    \n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract social media links from page.\"\"\"\n",
    "        social_links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            \n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(domain in href for domain in domains):\n",
    "                    full_url = urljoin(base_url, link['href'])\n",
    "                    if full_url not in social_links[platform]:\n",
    "                        social_links[platform].append(full_url)\n",
    "        \n",
    "        return social_links\n",
    "    \n",
    "    def _find_contact_pages(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Find links to contact and about pages.\"\"\"\n",
    "        contact_pages = []\n",
    "        keywords = ['contact', 'about', 'reach', 'connect', 'support']\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            text = link.get_text().lower()\n",
    "            \n",
    "            # Check if link contains contact-related keywords\n",
    "            if any(keyword in href or keyword in text for keyword in keywords):\n",
    "                full_url = urljoin(base_url, link['href'])\n",
    "                parsed = urlparse(full_url)\n",
    "                \n",
    "                # Only include internal links\n",
    "                if parsed.netloc == urlparse(base_url).netloc:\n",
    "                    if full_url not in contact_pages and full_url != base_url:\n",
    "                        contact_pages.append(full_url)\n",
    "        \n",
    "        return contact_pages[:5]  # Limit to 5 additional pages\n",
    "    \n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Scrape a single page for contact information.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (emails, phones, soup object)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            response = self.session.get(url, timeout=10, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for element in soup(['script', 'style', 'noscript']):\n",
    "                element.decompose()\n",
    "            \n",
    "            text = soup.get_text()\n",
    "            \n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            \n",
    "            return emails, phones, soup\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error scraping {url}: {e}\")\n",
    "            return set(), set(), None\n",
    "    \n",
    "    def scrape_business_website(self, website_url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Scrape business website for contact information.\n",
    "        \n",
    "        Args:\n",
    "            website_url: Business website URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all extracted information\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸŒ Starting website scrape: {website_url}\")\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            'website': website_url,\n",
    "            'emails': [],\n",
    "            'phones': [],\n",
    "            'social_links': {platform: [] for platform in self.SOCIAL_PLATFORMS},\n",
    "            'additional_pages': []\n",
    "        }\n",
    "        \n",
    "        all_emails = set()\n",
    "        all_phones = set()\n",
    "        pages_to_scrape = [website_url]\n",
    "        scraped_pages = set()\n",
    "        \n",
    "        # Scrape main page\n",
    "        emails, phones, soup = self._scrape_page(website_url)\n",
    "        all_emails.update(emails)\n",
    "        all_phones.update(phones)\n",
    "        scraped_pages.add(website_url)\n",
    "        \n",
    "        if soup:\n",
    "            # Extract social media links from main page\n",
    "            social_links = self._extract_social_links(soup, website_url)\n",
    "            for platform, links in social_links.items():\n",
    "                result['social_links'][platform].extend(links)\n",
    "            \n",
    "            # Find contact pages\n",
    "            contact_pages = self._find_contact_pages(soup, website_url)\n",
    "            result['additional_pages'] = contact_pages\n",
    "            \n",
    "            # Scrape additional pages\n",
    "            for page_url in contact_pages:\n",
    "                if page_url not in scraped_pages:\n",
    "                    time.sleep(1)  # Polite delay\n",
    "                    emails, phones, page_soup = self._scrape_page(page_url)\n",
    "                    all_emails.update(emails)\n",
    "                    all_phones.update(phones)\n",
    "                    scraped_pages.add(page_url)\n",
    "                    \n",
    "                    # Extract social links from additional pages\n",
    "                    if page_soup:\n",
    "                        social_links = self._extract_social_links(page_soup, website_url)\n",
    "                        for platform, links in social_links.items():\n",
    "                            for link in links:\n",
    "                                if link not in result['social_links'][platform]:\n",
    "                                    result['social_links'][platform].append(link)\n",
    "        \n",
    "        # Deduplicate and sort results\n",
    "        result['emails'] = sorted(list(all_emails))\n",
    "        result['phones'] = sorted(list(all_phones))\n",
    "        \n",
    "        # Remove duplicate social links\n",
    "        for platform in result['social_links']:\n",
    "            result['social_links'][platform] = list(set(result['social_links'][platform]))\n",
    "        \n",
    "        print(f\"\\nâœ… Scraping complete!\")\n",
    "        print(f\"   Emails found: {len(result['emails'])}\")\n",
    "        print(f\"   Phones found: {len(result['phones'])}\")\n",
    "        print(f\"   Social links: {sum(len(v) for v in result['social_links'].values())}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def scrape_from_google_maps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Complete pipeline: Google Maps â†’ Website â†’ Contact Info.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all extracted information or None if website not found\n",
    "        \"\"\"\n",
    "        # Step 1: Get website from Google Maps\n",
    "        website_url = self.get_website_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if not website_url:\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Scrape the business website\n",
    "        return self.scrape_business_website(website_url)\n",
    "    \n",
    "    def save_to_json(self, data: Dict, filename: str = 'business_info.json'):\n",
    "        \"\"\"Save scraped data to JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nðŸ’¾ Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving to JSON: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get Google Maps URL from user\n",
    "    gmaps_url = input(\"\\nðŸ“ Enter Google Maps business URL: \").strip()\n",
    "    \n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = BusinessInfoScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Run the complete scraping pipeline\n",
    "        result = scraper.scrape_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ“Š RESULTS\")\n",
    "            print(\"=\" * 70)\n",
    "            print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            # Save to file\n",
    "            scraper.save_to_json(result)\n",
    "        else:\n",
    "            print(\"\\nâŒ Could not find business website on Google Maps listing.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\n",
      "======================================================================\n",
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/Saffron+Foodies/@33.6333907,73.0342839,14z/data=!4m7!3m6!1s0x38df95209a7d563f:0x97f7b42c71db5bc3!8m2!3d33.6381307!4d73.0682772!15sCgpyZXN0YXVyYW50WgwiCnJlc3RhdXJhbnSSAQpyZXN0YXVyYW50mgEjQ2haRFNVaE5NRzluUzBWSlEwRm5TVVIxTms5MU5HWjNFQUXgAQD6AQUIqgIQSg!16s%2Fg%2F11c30vmh9h?authuser=0&coh=277533&entry=tts&g_ep=EgoyMDI1MTEwNC4xIPu8ASoASAFQAw%3D%3D&skid=e79c76e9-1573-4518-bd64-36cec7f049a0\n",
      "âœ… Found website: http://www.saffron.com.pk/\n",
      "\n",
      "ðŸŒ Starting website scrape: http://www.saffron.com.pk/\n",
      "ðŸ“„ Scraping: http://www.saffron.com.pk/\n",
      "\n",
      "âœ… Scraping complete!\n",
      "   Emails found: 0\n",
      "   Phones found: 0\n",
      "   Social links: 0\n",
      "\n",
      "======================================================================\n",
      "ðŸ“Š RESULTS\n",
      "======================================================================\n",
      "{\n",
      "  \"website\": \"http://www.saffron.com.pk/\",\n",
      "  \"emails\": [],\n",
      "  \"phones\": [],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [],\n",
      "    \"instagram\": [],\n",
      "    \"linkedin\": [],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": []\n",
      "  },\n",
      "  \"additional_pages\": []\n",
      "}\n",
      "\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Google Maps to Business Website Contact Info Scraper\n",
    "Extracts emails, phones, and social media from business websites found on Google Maps.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "\n",
    "class BusinessInfoScraper:\n",
    "    \"\"\"Scrapes business contact information from Google Maps and business websites.\"\"\"\n",
    "    \n",
    "    # Regex patterns\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    \n",
    "    # Social media domains\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "    \n",
    "    def __init__(self, headless: bool = True):\n",
    "        \"\"\"Initialize the scraper with Selenium options.\"\"\"\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Initialize Chrome WebDriver with appropriate options.\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        return webdriver.Chrome(options=options)\n",
    "    \n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract the business website URL from Google Maps listing.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Business website URL or None if not found\n",
    "        \"\"\"\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            \n",
    "            # Wait for page to load and find website button\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            \n",
    "            # Try multiple selectors for the website button\n",
    "            selectors = [\n",
    "                \"a[data-item-id='authority']\",\n",
    "                \"a[aria-label*='Website']\",\n",
    "                \"a[href*='http']:has-text('Website')\",\n",
    "                \"//a[contains(@aria-label, 'Website')]\",\n",
    "                \"//a[contains(., 'Website')]\"\n",
    "            ]\n",
    "            \n",
    "            website_url = None\n",
    "            for selector in selectors:\n",
    "                try:\n",
    "                    if selector.startswith('//'):\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.XPATH, selector))\n",
    "                        )\n",
    "                    else:\n",
    "                        element = wait.until(\n",
    "                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                        )\n",
    "                    \n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # Clean up Google redirect URLs\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            match = re.search(r'[?&]q=([^&]+)', website_url)\n",
    "                            if match:\n",
    "                                from urllib.parse import unquote\n",
    "                                website_url = unquote(match.group(1))\n",
    "                        break\n",
    "                except (TimeoutException, NoSuchElementException):\n",
    "                    continue\n",
    "            \n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found on Google Maps listing\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error extracting website from Google Maps: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "    \n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract email addresses from text.\"\"\"\n",
    "        emails = self.EMAIL_PATTERN.findall(text)\n",
    "        # Filter out common false positives\n",
    "        filtered = {\n",
    "            email.lower() for email in emails\n",
    "            if not email.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.svg'))\n",
    "        }\n",
    "        return filtered\n",
    "    \n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        \"\"\"Extract phone numbers from text.\"\"\"\n",
    "        phones = self.PHONE_PATTERN.findall(text)\n",
    "        # Clean and filter phone numbers\n",
    "        cleaned = set()\n",
    "        for phone in phones:\n",
    "            # Remove excessive spaces/dashes\n",
    "            clean = re.sub(r'\\s+', ' ', phone.strip())\n",
    "            # Only keep if it has enough digits\n",
    "            digit_count = sum(c.isdigit() for c in clean)\n",
    "            if digit_count >= 9:  # Minimum valid phone number length\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "    \n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract social media links from page.\"\"\"\n",
    "        social_links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            \n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(domain in href for domain in domains):\n",
    "                    full_url = urljoin(base_url, link['href'])\n",
    "                    if full_url not in social_links[platform]:\n",
    "                        social_links[platform].append(full_url)\n",
    "        \n",
    "        return social_links\n",
    "    \n",
    "    def _find_contact_pages(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        \"\"\"Find links to contact and about pages.\"\"\"\n",
    "        contact_pages = []\n",
    "        keywords = ['contact', 'about', 'reach', 'connect', 'support']\n",
    "        \n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href'].lower()\n",
    "            text = link.get_text().lower()\n",
    "            \n",
    "            # Check if link contains contact-related keywords\n",
    "            if any(keyword in href or keyword in text for keyword in keywords):\n",
    "                full_url = urljoin(base_url, link['href'])\n",
    "                parsed = urlparse(full_url)\n",
    "                \n",
    "                # Only include internal links\n",
    "                if parsed.netloc == urlparse(base_url).netloc:\n",
    "                    if full_url not in contact_pages and full_url != base_url:\n",
    "                        contact_pages.append(full_url)\n",
    "        \n",
    "        return contact_pages[:5]  # Limit to 5 additional pages\n",
    "    \n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Scrape a single page for contact information.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (emails, phones, soup object)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            response = self.session.get(url, timeout=10, allow_redirects=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove script and style elements\n",
    "            for element in soup(['script', 'style', 'noscript']):\n",
    "                element.decompose()\n",
    "            \n",
    "            text = soup.get_text()\n",
    "            \n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            \n",
    "            return emails, phones, soup\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error scraping {url}: {e}\")\n",
    "            return set(), set(), None\n",
    "    \n",
    "    def scrape_business_website(self, website_url: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Scrape business website for contact information.\n",
    "        \n",
    "        Args:\n",
    "            website_url: Business website URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing all extracted information\n",
    "        \"\"\"\n",
    "        print(f\"\\nðŸŒ Starting website scrape: {website_url}\")\n",
    "        \n",
    "        # Initialize result structure\n",
    "        result = {\n",
    "            'website': website_url,\n",
    "            'emails': [],\n",
    "            'phones': [],\n",
    "            'social_links': {platform: [] for platform in self.SOCIAL_PLATFORMS},\n",
    "            'additional_pages': []\n",
    "        }\n",
    "        \n",
    "        all_emails = set()\n",
    "        all_phones = set()\n",
    "        pages_to_scrape = [website_url]\n",
    "        scraped_pages = set()\n",
    "        \n",
    "        # Scrape main page\n",
    "        emails, phones, soup = self._scrape_page(website_url)\n",
    "        all_emails.update(emails)\n",
    "        all_phones.update(phones)\n",
    "        scraped_pages.add(website_url)\n",
    "        \n",
    "        if soup:\n",
    "            # Extract social media links from main page\n",
    "            social_links = self._extract_social_links(soup, website_url)\n",
    "            for platform, links in social_links.items():\n",
    "                result['social_links'][platform].extend(links)\n",
    "            \n",
    "            # Find contact pages\n",
    "            contact_pages = self._find_contact_pages(soup, website_url)\n",
    "            result['additional_pages'] = contact_pages\n",
    "            \n",
    "            # Scrape additional pages\n",
    "            for page_url in contact_pages:\n",
    "                if page_url not in scraped_pages:\n",
    "                    time.sleep(1)  # Polite delay\n",
    "                    emails, phones, page_soup = self._scrape_page(page_url)\n",
    "                    all_emails.update(emails)\n",
    "                    all_phones.update(phones)\n",
    "                    scraped_pages.add(page_url)\n",
    "                    \n",
    "                    # Extract social links from additional pages\n",
    "                    if page_soup:\n",
    "                        social_links = self._extract_social_links(page_soup, website_url)\n",
    "                        for platform, links in social_links.items():\n",
    "                            for link in links:\n",
    "                                if link not in result['social_links'][platform]:\n",
    "                                    result['social_links'][platform].append(link)\n",
    "        \n",
    "        # Deduplicate and sort results\n",
    "        result['emails'] = sorted(list(all_emails))\n",
    "        result['phones'] = sorted(list(all_phones))\n",
    "        \n",
    "        # Remove duplicate social links\n",
    "        for platform in result['social_links']:\n",
    "            result['social_links'][platform] = list(set(result['social_links'][platform]))\n",
    "        \n",
    "        print(f\"\\nâœ… Scraping complete!\")\n",
    "        print(f\"   Emails found: {len(result['emails'])}\")\n",
    "        print(f\"   Phones found: {len(result['phones'])}\")\n",
    "        print(f\"   Social links: {sum(len(v) for v in result['social_links'].values())}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def scrape_from_google_maps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Complete pipeline: Google Maps â†’ Website â†’ Contact Info.\n",
    "        \n",
    "        Args:\n",
    "            gmaps_url: Google Maps business listing URL\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all extracted information or None if website not found\n",
    "        \"\"\"\n",
    "        # Step 1: Get website from Google Maps\n",
    "        website_url = self.get_website_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if not website_url:\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Scrape the business website\n",
    "        return self.scrape_business_website(website_url)\n",
    "    \n",
    "    def save_to_json(self, data: Dict, filename: str = 'business_info.json'):\n",
    "        \"\"\"Save scraped data to JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"\\nðŸ’¾ Data saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving to JSON: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ—ºï¸  GOOGLE MAPS BUSINESS INFO SCRAPER\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get Google Maps URL from user\n",
    "    gmaps_url = input(\"\\nðŸ“ Enter Google Maps business URL: \").strip()\n",
    "    \n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = BusinessInfoScraper(headless=True)\n",
    "    \n",
    "    try:\n",
    "        # Run the complete scraping pipeline\n",
    "        result = scraper.scrape_from_google_maps(gmaps_url)\n",
    "        \n",
    "        if result:\n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 70)\n",
    "            print(\"ðŸ“Š RESULTS\")\n",
    "            print(\"=\" * 70)\n",
    "            print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            # Save to file\n",
    "            scraper.save_to_json(result)\n",
    "        else:\n",
    "            print(\"\\nâŒ Could not find business website on Google Maps listing.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ—ºï¸  WEBSITE CRAWLER FOR CONTACT INFO (requests + Selenium fallback)\n",
      "======================================================================\n",
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/Rawalpindi+Medicare+Hospital/@33.6334588,73.0342839,14z/data=!4m6!3m5!1s0x38dfebade4e83c15:0x8072321813277499!8m2!3d33.6307011!4d73.0881662!16s%2Fg%2F11rs9hm8v1?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n",
      "âœ… Found website: http://www.rmh.com.pk/\n",
      "\n",
      "ðŸŒ Begin crawl for: http://www.rmh.com.pk/\n",
      "ðŸ“„ Scraping: http://www.rmh.com.pk/\n",
      "ðŸ“„ Scraping: http://www.rmh.com.pk/#content\n",
      "{\n",
      "  \"website\": \"http://www.rmh.com.pk/\",\n",
      "  \"emails\": [\n",
      "    \"info@rmh.com.pk\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"+923345933391\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"http://web.archive.org/web/20230524195052/https://www.facebook.com/RastgarMaterialsHandling/\"\n",
      "    ],\n",
      "    \"instagram\": [],\n",
      "    \"linkedin\": [\n",
      "      \"http://web.archive.org/web/20230524195052/https://www.linkedin.com/company/rastgarmaterialshandling/\"\n",
      "    ],\n",
      "    \"twitter\": [\n",
      "      \"https://pmx.com.pk/\"\n",
      "    ],\n",
      "    \"youtube\": []\n",
      "  },\n",
      "  \"visited_pages\": [\n",
      "    \"http://www.rmh.com.pk/\",\n",
      "    \"http://www.rmh.com.pk/#content\"\n",
      "  ]\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Universal Website Contact Info Crawler\n",
    "- Starts from Google Maps link\n",
    "- Scrapes all internal pages recursively\n",
    "- Extracts emails, phones, social links, and addresses\n",
    "- Requests first, Selenium fallback for JS-heavy pages\n",
    "- Outputs JSON file\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "# Webdriver manager\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "class UniversalCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_PATTERN = re.compile(r'\\d{1,5}\\s\\w+\\s\\w+|[A-Za-z0-9.,\\s\\-]+')  # Simplistic\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            website_btn_selectors = [\n",
    "                \"//a[contains(@aria-label, 'Website')]\",\n",
    "                \"//a[contains(text(),'Website')]\"\n",
    "            ]\n",
    "\n",
    "            website_url = None\n",
    "            for sel in website_btn_selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # Remove Google redirect\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            from urllib.parse import unquote, parse_qs, urlparse\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found on Google Maps listing\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error getting website: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            for element in soup(['script', 'style', 'noscript']):\n",
    "                element.decompose()\n",
    "\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "\n",
    "            return emails, phones, social_links, soup\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}, trying Selenium: {e}\")\n",
    "            # Fallback to Selenium\n",
    "            try:\n",
    "                driver = self._init_driver()\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                driver.quit()\n",
    "                for element in soup(['script', 'style', 'noscript']):\n",
    "                    element.decompose()\n",
    "                text = soup.get_text()\n",
    "                emails = self._extract_emails(text)\n",
    "                phones = self._extract_phones(text)\n",
    "                social_links = self._extract_social_links(soup, url)\n",
    "                return emails, phones, social_links, soup\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Selenium failed for {url}: {e2}\")\n",
    "                return set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}, None\n",
    "\n",
    "    def _find_internal_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:\n",
    "        internal_links = []\n",
    "        parsed_base = urlparse(base_url)\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            parsed_href = urlparse(urljoin(base_url, href))\n",
    "            if parsed_href.netloc == parsed_base.netloc:\n",
    "                full_url = parsed_href.geturl()\n",
    "                if full_url not in internal_links:\n",
    "                    internal_links.append(full_url)\n",
    "        return internal_links\n",
    "\n",
    "    def crawl_website(self, base_url: str, max_pages: int = 80) -> Dict:\n",
    "        print(f\"\\nðŸŒ Begin crawl for: {base_url}\")\n",
    "        visited = set()\n",
    "        to_visit = [base_url]\n",
    "\n",
    "        all_emails = set()\n",
    "        all_phones = set()\n",
    "        all_social = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        page_count = 0\n",
    "        while to_visit and page_count < max_pages:\n",
    "            url = to_visit.pop(0)\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "            page_count += 1\n",
    "\n",
    "            emails, phones, social_links, soup = self._scrape_page(url)\n",
    "            all_emails.update(emails)\n",
    "            all_phones.update(phones)\n",
    "            for p in all_social:\n",
    "                all_social[p].extend([l for l in social_links[p] if l not in all_social[p]])\n",
    "\n",
    "            if soup:\n",
    "                internal_links = self._find_internal_links(soup, base_url)\n",
    "                for link in internal_links:\n",
    "                    if link not in visited and link not in to_visit:\n",
    "                        to_visit.append(link)\n",
    "\n",
    "            time.sleep(1)  # Polite delay\n",
    "\n",
    "        return {\n",
    "            'website': base_url,\n",
    "            'emails': sorted(list(all_emails)),\n",
    "            'phones': sorted(list(all_phones)),\n",
    "            'social_links': {k: list(set(v)) for k, v in all_social.items()},\n",
    "            'visited_pages': list(visited)\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.crawl_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename: str = 'business_info.json'):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ðŸ—ºï¸  WEBSITE CRAWLER FOR CONTACT INFO (requests + Selenium fallback)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided. Exiting.\")\n",
    "        return\n",
    "\n",
    "    crawler = UniversalCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not retrieve website or scrape data.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/Rawalpindi+Medicare+Hospital/@33.6334588,73.0342839,14z/data=!4m6!3m5!1s0x38dfebade4e83c15:0x8072321813277499!8m2!3d33.6307011!4d73.0881662!16s%2Fg%2F11rs9hm8v1?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n",
      "âœ… Found website: http://www.rmh.com.pk/\n",
      "ðŸ“„ Scraping: http://www.rmh.com.pk/\n",
      "ðŸ“„ Scraping: https://rmh.com.pk/about-us/\n",
      "ðŸ“„ Scraping: https://rmh.com.pk/contact-us/\n",
      "ðŸ“„ Scraping: https://rmh.com.pk/reach-trucks/\n",
      "{\n",
      "  \"website\": \"http://www.rmh.com.pk/\",\n",
      "  \"emails\": [\n",
      "    \"info@rmh.com.pk\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"+922135123112\",\n",
      "    \"+922135123114\",\n",
      "    \"+922135123116-17\",\n",
      "    \"+923345933391\",\n",
      "    \"+923444437099\"\n",
      "  ],\n",
      "  \"addresses\": [\n",
      "    \"Darul Uloom Plot # 9, Shahrah-e-Darul Uloom, Sector 28, Korangi Industrial Area, Karachi â€“ 75180, Pakistan\",\n",
      "    \"House # 344, St # 69, F-11/1, Nazim-ud-din Road Islamabad\",\n",
      "    \"Islamabad Office\",\n",
      "    \"Lahore Office\",\n",
      "    \"Plot No. 123, Sundar Industrial Area, Gate No.2, Lahore Pakistan\",\n",
      "    \"Plot No. 123, Sundar Industrial Estate, Lahore Pakistan\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"http://web.archive.org/web/20230524195052/https://www.facebook.com/RastgarMaterialsHandling/\"\n",
      "    ],\n",
      "    \"instagram\": [],\n",
      "    \"linkedin\": [\n",
      "      \"http://web.archive.org/web/20230524195052/https://www.linkedin.com/company/rastgarmaterialshandling/\"\n",
      "    ],\n",
      "    \"twitter\": [\n",
      "      \"https://pmx.com.pk/\"\n",
      "    ],\n",
      "    \"youtube\": []\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/NYC+Health+%2B+Hospitals+%2F+Bellevue/@40.7396865,-74.2645955,11z/data=!3m1!5s0x89c2599e8c7ec08d:0x5caf69bb05ed42d1!4m6!3m5!1s0x89c2590c212fc7a9:0x67a293dafcc5f7e0!8m2!3d40.7396865!4d-73.9762044!16zL20vMDYwMGp0?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n",
      "âœ… Found website: http://www.nychealthandhospitals.org/bellevue/\n",
      "ðŸ“„ Scraping: http://www.nychealthandhospitals.org/bellevue/\n",
      "{\n",
      "  \"website\": \"http://www.nychealthandhospitals.org/bellevue/\",\n",
      "  \"emails\": [],\n",
      "  \"phones\": [],\n",
      "  \"addresses\": [],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [],\n",
      "    \"instagram\": [],\n",
      "    \"linkedin\": [],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": []\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://www.google.com/maps/place/The+Mount+Sinai+Hospital/@40.7396865,-74.2645955,11z/data=!3m1!5s0x89c2377a56a7853f:0x4ad2a9bc5cffd767!4m6!3m5!1s0x89c2f63dcaeeda93:0x9797c11e6d7bc63f!8m2!3d40.7899484!4d-73.9524454!16zL20vMDRsZjNi?authuser=0&entry=ttu&g_ep=EgoyMDI1MTEwNC4xIKXMDSoASAFQAw%3D%3D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get LATEST chromedriver version for google-chrome\n",
      "Get LATEST chromedriver version for google-chrome\n",
      "Driver [C:\\Users\\ABC\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.61\\chromedriver-win32/chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found website: https://www.mountsinai.org/locations/mount-sinai?utm_source=Yext&utm_medium=local_listing&utm_campaign=MountSinai&y_source=1_MTgwODM1MS03MTUtbG9jYXRpb24ud2Vic2l0ZQ==\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai?utm_source=Yext&utm_medium=local_listing&utm_campaign=MountSinai&y_source=1_MTgwODM1MS03MTUtbG9jYXRpb24ud2Vic2l0ZQ==\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/patient-tools-and-resources/check-symptoms-get-care\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/access\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/network\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/ethics\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/contact\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/patient-tools-and-resources\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/executive-leadership\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/facts\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/financial-assistance\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/insurance\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/lgbt-health\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/language-accessibility\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/mountsinaidaily\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/msd\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/medical-records\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/medical-staff\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/mymountsinai\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/newsroom\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/mission\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/patient-safety-quality\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/pay-my-bill\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/access/physician-access\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/visit-us\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/auxiliary-board\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/billing-patient-financial-services\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/community\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/contact\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/compliance\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/history\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/health-professionals\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/insurance/msh\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/leadership\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/volunteer\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/about/healthcare-associated-infections\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/support-services\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/support-services/representatives\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/support-services/social-work\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/support-services/organ-donation\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/locations/mount-sinai/support-services/north-help\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/newsroom/2025/mount-sinai-health-system-hospitals-recognized-among-worlds-best-and-number-1-in-new-york-in-newsweek-statista-rankings\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/newsroom/2025/mount-sinai-hospital-ranked-among-top-in-the-nation-by-us-news-world-report\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/newsroom/2024/the-mount-sinai-hospital-achieves-fifth-consecutive-magnet-recognition\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/pay-my-bill/hospital/msh#pmb__main-content-link\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/insurance/rights-protections\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/access/patient-transfer\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/hospital-sponsored-programs-office\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/compliance/hipaa-privacy-security-compliance-program\n",
      "ðŸ“„ Scraping: https://www.mountsinai.org/about/insurance/price-transparency\n",
      "{\n",
      "  \"website\": \"https://www.mountsinai.org/locations/mount-sinai?utm_source=Yext&utm_medium=local_listing&utm_campaign=MountSinai&y_source=1_MTgwODM1MS03MTUtbG9jYXRpb24ud2Vic2l0ZQ==\",\n",
      "  \"emails\": [\n",
      "    \"CBOCustomerService@mountsinai.org\",\n",
      "    \"Kevin.Chason@mountsinai.org\",\n",
      "    \"LGBTinfo@mountsinai.org\",\n",
      "    \"MSBPatientRep@mountsinai.org\",\n",
      "    \"MSDaily_Feedback@mountsinai.org\",\n",
      "    \"MSHPatientRelations@mountsinai.org\",\n",
      "    \"MSMPatientRelations@mountsinai.org\",\n",
      "    \"MSQPatientRelations@mountsinai.org\",\n",
      "    \"MSS-Intake@mountsinai.org\",\n",
      "    \"MSSNPatientRelations@mountsinai.org\",\n",
      "    \"NewsMedia@mssm.edu\",\n",
      "    \"OGARA@chpnet.org\",\n",
      "    \"PatientRelations@nyee.edu\",\n",
      "    \"VolunteerDept@mountsinai.org\",\n",
      "    \"Yosef.Travis@mountsinai.org\",\n",
      "    \"access@mountsinai.org\",\n",
      "    \"ayana.haynes@mountsinai.org\",\n",
      "    \"chargemaster@mountsinai.org\",\n",
      "    \"edwardinemirna.mohanraj@mountsinai.org\",\n",
      "    \"ethics.committee@snch.org\",\n",
      "    \"ethicsMSSLW@mountsinai.org\",\n",
      "    \"james.murphy2@mountsinai.org\",\n",
      "    \"jean.stenard@mountsinai.org\",\n",
      "    \"jolion.mcgreevy@mountsinai.org\",\n",
      "    \"jonathan.altus@mountsinai.org\",\n",
      "    \"jstenard@chpnet.org\",\n",
      "    \"krishna.chokshi@mountsinai.org\",\n",
      "    \"mshethics@mountsinai.org\",\n",
      "    \"mswpatientrelations@mountsinai.org\",\n",
      "    \"surprisemedicalbills@dfs.ny.gov\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"1-800-342-3736\",\n",
      "    \"1-800-985-3059\",\n",
      "    \"1-855-343-34701-855-343-3470\",\n",
      "    \"10003212-979-4352\",\n",
      "    \"10011212-420-2665\",\n",
      "    \"10017-5629\",\n",
      "    \"10025212-523-3265\",\n",
      "    \"10029-6574\",\n",
      "    \"10029212-241-7607\",\n",
      "    \"10087-7761\",\n",
      "    \"11102718-808-7683\",\n",
      "    \"11234718-951-2806\",\n",
      "    \"11572516-632-3907\",\n",
      "    \"2022-20249\",\n",
      "    \"212-241-0478\",\n",
      "    \"212-241-0478212-241-0478\",\n",
      "    \"212-241-0519\",\n",
      "    \"212-241-1100\",\n",
      "    \"212-241-1900\",\n",
      "    \"212-241-2700\",\n",
      "    \"212-241-3211\",\n",
      "    \"212-241-3211212-241-3211800-853-9212\",\n",
      "    \"212-241-3300\",\n",
      "    \"212-241-4983\",\n",
      "    \"212-241-6372\",\n",
      "    \"212-241-6500\",\n",
      "    \"212-241-6500212-241-6500212-590-3300212-590-3300\",\n",
      "    \"212-241-6500212-590-3300\",\n",
      "    \"212-241-6512212-241-6512\",\n",
      "    \"212-241-6585\",\n",
      "    \"212-241-6639\",\n",
      "    \"212-241-6696\",\n",
      "    \"212-241-6756\",\n",
      "    \"212-241-6800\",\n",
      "    \"212-241-6800212-241-6800\",\n",
      "    \"212-241-7262\",\n",
      "    \"212-241-7383\",\n",
      "    \"212-241-7601\",\n",
      "    \"212-241-7607\",\n",
      "    \"212-241-7778\",\n",
      "    \"212-241-7791\",\n",
      "    \"212-241-8947\",\n",
      "    \"212-241-9200\",\n",
      "    \"212-241-9200212-241-9200\",\n",
      "    \"212-241-9500\",\n",
      "    \"212-420-2665\",\n",
      "    \"212-423-2140\",\n",
      "    \"212-492-1818212-492-1818\",\n",
      "    \"212-523-2187\",\n",
      "    \"212-523-3336\",\n",
      "    \"212-523-4000\",\n",
      "    \"212-523-6623\",\n",
      "    \"212-523-6800\",\n",
      "    \"212-590-3300\",\n",
      "    \"212-604-6550\",\n",
      "    \"212-641-4500\",\n",
      "    \"212-659-6702\",\n",
      "    \"212-659-8500\",\n",
      "    \"212-659-8990\",\n",
      "    \"212-659-8990212-659-8990\",\n",
      "    \"212-690-0303\",\n",
      "    \"212-731-3100\",\n",
      "    \"212-731-3100212-731-3100\",\n",
      "    \"212-731-3600\",\n",
      "    \"212-731-7419\",\n",
      "    \"212-731-7888\",\n",
      "    \"212-824-8351212-824-8351\",\n",
      "    \"212-828-3250\",\n",
      "    \"212-828-3260\",\n",
      "    \"212-844-8400\",\n",
      "    \"212-857-9990\",\n",
      "    \"212-979-4000\",\n",
      "    \"212-979-4093\",\n",
      "    \"212-996-2230\",\n",
      "    \"516-632-3414\",\n",
      "    \"516-632-3484\",\n",
      "    \"630-792-5636\",\n",
      "    \"631-351-3700\",\n",
      "    \"646-605-8945646-605-8945\",\n",
      "    \"646-605-8995646-605-8995\",\n",
      "    \"718-252-3000\",\n",
      "    \"718-267-4273\",\n",
      "    \"718-267-4310\",\n",
      "    \"718-289-3100\",\n",
      "    \"718-312-7300\",\n",
      "    \"718-420-1279\",\n",
      "    \"718-808-7777\",\n",
      "    \"718-932-1000\",\n",
      "    \"718-951-2901\",\n",
      "    \"718-951-3005\",\n",
      "    \"800-804-5447\",\n",
      "    \"800-853-9212\",\n",
      "    \"800-867-4624\",\n",
      "    \"844-463-2778\",\n",
      "    \"866-674-3721\",\n",
      "    \"866-682-9380\",\n",
      "    \"929-210-6000\",\n",
      "    \"929-210-6300\"\n",
      "  ],\n",
      "  \"addresses\": [\n",
      "    \"13th floor, Suite B\",\n",
      "    \"150 East 42nd Street, Fifth Floor, Section D\",\n",
      "    \"1904â€”The 456-bed, 10-pavilion Mount Sinai Hospital is dedicated on Fifth Avenue at 100th Street, marking a significant expansion to accommodate growing patient needs.\",\n",
      "    \"30-14 Crescent Street, 5th Floor\",\n",
      "    \"310 East 14th Street\",\n",
      "    \"325 West 15th Street\",\n",
      "    \"5 East 102nd Street, Room D1-228\",\n",
      "    \"Approaches to Building Resilience\",\n",
      "    \"Building a Resilient Family\",\n",
      "    \"CAM Building, at 17 East 102nd Street\",\n",
      "    \"During the Civil War, the hospital expanded to accommodate Union soldiers. After this, to reflect its broadened mission and to ensureÂ its eligibility for state and city support, the Jews' Hospital formally abandoned its sectarian charter in 1866 and was renamed The Mount Sinai Hospital. In 1872, it moved to a new 120-bed facility on Lexington Avenue, between 66th and 67th Streets, nearly tripling its original capacity.\",\n",
      "    \"Ethics consultants make recommendations that can help patients, families, and health care professionals navigate ethically complex situations in patient care. Consultations are confidential, non-judgmental, and focused on supporting respectful, values-based decision-making and building consensus.\",\n",
      "    \"Financial Counseling, located in the Annenberg building\",\n",
      "    \"In 1904, the new 456-bed, 10-pavilion Mount Sinai Hospital was dedicated on Fifth Avenue at 100th Street. The President of the hospital, Isaac Wallach, described The Mount Sinai Hospital as â€œthis House of noble deedsâ€ with a three-fold mission of â€œbenevolence, science, and education.â€ Over the years, the hospital has expanded rapidly both physically and in terms of service, becoming a full-service medical facility capable of treating complex conditions.Â In an effort to help the hospitalâ€™s patients balance their medical and social needs, a department of Social Work Services was created in 1907.Â The latter is supported by the Auxiliary Board, which was formed in 1916 to provide financial support and labor resources to social service-related activities at the Hospital. The Auxiliary today works diligently to support vital hospital and community outreach projects.\",\n",
      "    \"Location:150 East 42nd Street, 10th FloorNew York, NY 10017-5629\",\n",
      "    \"Main Cashier; located in the Guggenheim pavilion with entrances at 1190 Fifth Avenue (at E. 101st Street) and 1468 Madison Avenue\",\n",
      "    \"Mount Sinai Daily is the central location for all Mount Sinai employee news. In addition to many of the important communications previously accessible only via broadcast email messages, you will find new content focused on helping you stay connected and feel supported at Mount Sinai including site-specific hospital news, daily wellness calendars, cafÃ© menus, updates from nursing leadership, and more. Users can follow the topics most relevant to them.\",\n",
      "    \"Mount Sinai Doctors East 85th Street: 212-241-6585\",\n",
      "    \"Mount Sinai Doctors-5030 Broadway\",\n",
      "    \"Mount Sinai Heart Ambulatory Cardiovascular Center at 181st Street\",\n",
      "    \"Mount Sinai Hospital (New York): Department of Financial Counseling, 17 East 102nd Street, Room D1-228, New York, New York 10029, (212) 824-7274 (p), (212) 876-7775 (f); Department of Financial Counseling, 1468 Madison Avenue, Room 210, New York, New York 10029, (212) 241-4851 (p), (212) 426-1094 (f); REAP 1405-05 Madison Ave, New York, NY 10029 (212) 423-2800 (p), (212) 534-5721 (F)\",\n",
      "    \"Mount Sinai Queens: Crescent Condo, Suite 1D, 23-22 30th Road, Long Island City, New York 11102, (718) 267-4369 (p), (718) 726-2967 (f)\",\n",
      "    \"Mount Sinai School of Medicine opened in 1968 in affiliation with The City University of New York. In building the medical school, trustees envisioned a new kind of medical institutionâ€”a university of the health sciences. This new institution would encompass a medical school supported by a strong teaching hospital, a graduate school of biological sciences, a graduate school of physical sciences, and undergraduate programs for allied health workers.\",\n",
      "    \"Mount Sinai St. Lukeâ€™s (HEAL Center): 1111 Amsterdam Avenue, Clark Building, Room 108, New York, New York 10025, (212) 523-3900 (p), (212) 523-3955 (f)\",\n",
      "    \"Mount Sinai St. Lukeâ€™s: Department of Patient Financial Counseling, 1111 Amsterdam Avenue at 114th Street, Room 1B-105, New York, New York 10025, (212) 523-2552 (p), (212) 523-5620 (f)\",\n",
      "    \"Mount Sinai-Union Square Urgent CareMount Sinai Doctors-Urgent Care & Multispecialty, Upper West SideMount Sinai Doctors West 23rd StreetMount Sinai Doctors East 34th StreetMount Sinai Express Care-QueensMount Sinai-Urgent Care, East 14th StreetVirtual Urgent Care\",\n",
      "    \"NYEEI: NYEEI Admitting Department, 310 East 14th Street, New York, New York 10003, (212) 979 4115, Attn: Brian Goldstein\",\n",
      "    \"New York Eye and Ear Infirmary of Mount Sinai: First Floor, 310 East 14th Street, New York, New York 10003, (212) 979-4183 (p), (212) 353-5738 (f)\",\n",
      "    \"On January 15, 1852, nine men representing a variety of Jewish charities agreed on a vision for free medical care for indigent Jews in New York City. In 1855, that vision came to fruition with the establishment of the 45-bed Jews' Hospital in New York in what was then a rural neighborhood on West 28th Street between Seventh and Eighth Avenues.\",\n",
      "    \"Return of Road to Resilience\",\n",
      "    \"Road to Resilience\",\n",
      "    \"Road to Resilience Archive\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"http://www.facebook.com/socialworkmshospital\",\n",
      "      \"https://www.facebook.com/mountsinainyc?utm_medium=cpc&utm_source=google&utm_content=googlesem&utm_campaign=mshs-respiratoryinstitute\",\n",
      "      \"https://www.facebook.com/mountsinainyc\",\n",
      "      \"https://www.facebook.com/mountsinainyc/\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/mountsinainyc/\",\n",
      "      \"http://www.instagram.com/socialworkmsh\",\n",
      "      \"https://www.instagram.com/mountsinainyc/?hl=en\",\n",
      "      \"https://www.instagram.com/diversityatmountsinai\"\n",
      "    ],\n",
      "    \"linkedin\": [\n",
      "      \"http://www.linkedin.com/company/socialworkmsh\",\n",
      "      \"https://www.linkedin.com/company/mountsinainyc\",\n",
      "      \"https://www.linkedin.com/company/the-mount-sinai-hospital\",\n",
      "      \"https://www.linkedin.com/company/mountsinainyc/\"\n",
      "    ],\n",
      "    \"twitter\": [\n",
      "      \"https://twitter.com/socialworkmsh\",\n",
      "      \"https://twitter.com/MountSinaiNYC\",\n",
      "      \"https://twitter.com/mountsinainyc?utm_medium=cpc&utm_source=google&utm_content=googlesem&utm_campaign=mshs-respiratoryinstitute\",\n",
      "      \"https://x.com/MountSinaiNYC\",\n",
      "      \"https://www.swellbox.com/mtsinai-wizard.html\"\n",
      "    ],\n",
      "    \"youtube\": [\n",
      "      \"https://youtu.be/zFvTeKbsXws\",\n",
      "      \"https://youtu.be/GMw52QtgTTI\",\n",
      "      \"https://www.youtube.com/embed/1VjrbnHzFYQ?autoplay=1\",\n",
      "      \"https://www.youtube.com/embed/Cok82QGnzJM?autoplay=1\",\n",
      "      \"https://www.youtube.com/embed/7xjyVO_Sz4c?autoplay=1\",\n",
      "      \"https://youtu.be/KQqWcW-lj-o\",\n",
      "      \"https://youtu.be/luFyP7tIorI\",\n",
      "      \"https://www.youtube.com/embed/e8SQT9GdWVo?autoplay=1\",\n",
      "      \"https://www.youtube.com/user/MountSinaiNY\",\n",
      "      \"https://www.youtube.com/playlist?list=PLCT7BA-HcHljc8siUGWPBWXejXrqERJB6\",\n",
      "      \"https://www.youtube.com/embed/PhOfO9jO1b8?autoplay=1\",\n",
      "      \"https://www.youtube.com/mountsinainy?utm_medium=cpc&utm_source=google&utm_content=googlesem&utm_campaign=mshs-respiratoryinstitute\",\n",
      "      \"https://www.youtube.com/embed/hvAdQSue9EU?autoplay=1\",\n",
      "      \"https://www.youtube.com/embed/SKtXdr5Z03A?autoplay=1\",\n",
      "      \"https://youtu.be/rGm0veAJvlc\",\n",
      "      \"https://youtu.be/t-KPhUlLvxg\",\n",
      "      \"https://www.youtube.com/embed/yTvLs5U2Ugs?autoplay=1\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://maps.app.goo.gl/izAmtdEixL4P9TBo6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get LATEST chromedriver version for google-chrome\n",
      "Get LATEST chromedriver version for google-chrome\n",
      "Driver [C:\\Users\\ABC\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.61\\chromedriver-win32/chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found website: http://www.nyp.org/\n",
      "ðŸ“„ Scraping: http://www.nyp.org/\n",
      "ðŸ“„ Scraping: http://www.nyp.org/about\n",
      "ðŸ“„ Scraping: http://www.nyp.org/contact\n",
      "ðŸ“„ Scraping: https://healthmatters.nyp.org/what-to-know-about-the-new-covid-variant-nb-1-8-1-known-as-nimbus-and-how-to-protect-yourself-this-summer/\n",
      "ðŸ“„ Scraping: http://www.nyp.org/about/community\n",
      "{\n",
      "  \"website\": \"http://www.nyp.org/\",\n",
      "  \"emails\": [],\n",
      "  \"phones\": [\n",
      "    \"10032212-305-2500\",\n",
      "    \"10034212-932-4000\",\n",
      "    \"10038212-312-5000\",\n",
      "    \"10065212-746-5454\",\n",
      "    \"10567914-737-9000\",\n",
      "    \"10605914-682-9100\",\n",
      "    \"10708914-787-1000\",\n",
      "    \"11355718-670-2000\",\n",
      "    \"212-305-2500\",\n",
      "    \"212-305-5437\",\n",
      "    \"212-312-5000\",\n",
      "    \"212-543-8862\",\n",
      "    \"212-746-5454\",\n",
      "    \"212-932-4000\",\n",
      "    \"718-499-2273\",\n",
      "    \"718-670-2000\",\n",
      "    \"718-780-3000\",\n",
      "    \"718-780-5367\",\n",
      "    \"800-245-5437\",\n",
      "    \"800-282-6684\",\n",
      "    \"855-969-7564\",\n",
      "    \"866-697-2553\",\n",
      "    \"877-697-9355\",\n",
      "    \"888-694-5700\",\n",
      "    \"888-697-4010\",\n",
      "    \"914-682-9100\",\n",
      "    \"914-737-9000\",\n",
      "    \"914-787-1000\",\n",
      "    \"914-787-5000\"\n",
      "  ],\n",
      "  \"addresses\": [\n",
      "    \"Capacity Building\",\n",
      "    \"Community & Population HealthNewYork-Presbyterian's Division of Community and Population Health collaborates with community partners across New York City to improve the health and well-being of the communities we serve with the goal to achieve health equity for all.Click here to learn more about Community & Population Health.Dalio Center for Health JusticeThe Dalio Center for Health Justice at NewYork-Presbyterian, launched in October 2020, aims to understand and address the root causes of health inequities with the goal of setting a new standard of health for our patients, our team members, and the communities we serve. Through the Dalio Center, we invest in research, education, and programming, and advocate for policy change to drive measurable improvements in health outcomes for all.Click here to learn more about the Dalio Center for Health Justice.Government & Community AffairsGovernment and Community Affairs works to support the health and well-being of New Yorkers to benefit the communities we serve. We endeavor to build relationships and trust with our local communities and government officials via ongoing communication, advocacy, community capacity building and support for NYPâ€™s community health programming.Community Programs & ServicesAt NewYork-Presbyterian, we have a long-standing commitment to understanding the needs of our communities and helping meet those needs. Our goal is to reduce and erase disparities by linking our neighbors with the worldâ€™s best healthcare services and supporting them to reduce social disparities of health. We collaborate with community-based organizations, chambers of commerce, faith-based organizations, community groups, and residents to address local issues through a range of innovative programs and services.Speakers BureauNewYork-Presbyterian Queens and NewYork-Presbyterian Brooklyn Methodist Hospital are pleased to offer lectures as well as a variety of educational programs to organizations (including schools, senior centers, community centers, etc.) in our community. Â We have healthcare professionals available to lecture or lead a group discussion on various health topics. Contact External Affairs to find out what topics are currently being offered or to suggest a health-related topic that would be of interest to your group. Please also let us know if you would like to request a presentation in a language other than English.To arrange for a speaker, or to request more information, please contact External Affairs via email at [emailÂ protected] or call 718-780-5367 (Brooklyn only). If possible, please contact us at least 6 weeks before the date you are requesting.Care in the CommunityThe community has access to primary and specialty care in convenient locations in the tri-state area through NewYork-Presbyterian Medical Group, ColumbiaDoctors, Weill Cornell Medicine, the Ambulatory Care Network, and NewYork-Presbyterian Queens Department of Ambulatory Care.Follow us on Instagram! @nypcommunity\",\n",
      "    \"Hospitals          NewYork-Presbyterian Allen Hospital             5141 Broadway, New York, NY 10034  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian / Columbia University Irving Medical Center622 West 168th StreetNew York, NY 10032212-305-2500NewYork-Presbyterian/Columbia, affiliated with Columbia University Vagelos College of Physicians and Surgeons, is one of the leading academic medical centers in the world. Providing emergency, primary, and specialty care in virtually every field of medicine, the 738-bed Medical Center offers a number of distinguished programs. These include the National Cancer Institute-designated Herbert Irving Comprehensive Cancer Center â€“ one of only three comprehensive cancer centers so designated in New York State â€“ which brings together internationally recognized researchers and clinicians to develop and implement the latest approaches for prevention, diagnosis, and treatment, with more than 200 clinical trials ongoing. In addition, the Department of Orthopedic Surgery provides the most advanced treatments and programs for musculoskeletal conditions in adults and children, including joint replacement surgery, sports medicine, cerebral palsy, and scoliosis. NewYork-Presbyterian/Columbia is internationally renowned for its cardiac programs â€“ offering the latest interventional therapies and pioneering cardiac surgery expertise, including heart transplant.\",\n",
      "    \"NewYork-Presbyterian / Weill Cornell Medical Center525 East 68th StreetNew York, NY 10065212-746-5454NewYork-Presbyterian/Weill Cornell Medical Center, affiliated with Weill Cornell Medicine, is among the foremost academic medical centers in the world with 862 beds providing emergency, primary, and specialty care in virtually every field of medicine. The center offers children and adults the latest diagnostic and treatment approaches in digestive disorders through its Center for Advanced Digestive Care â€“ a multidisciplinary program that includes an emphasis on minimally invasive colorectal surgery. Through major programs in neuroscience, NewYork-Presbyterian/Weill Cornell provides expertise in such areas as epilepsy, Alzheimerâ€™s disease, stroke and Neurosurgery. Och Spine at NewYork-Presbyterian/Weill Cornell serves as one of the countryâ€™s premier providers of minimally invasive spine surgery. The Kidney and Pancreas Transplant Program, the oldest kidney transplant program in New York State and one of the highest volume programs in the country, is nationally recognized for developing innovative strategies that allow for successful transplants. NewYork-Presbyterian/Weill Cornell is also home to the William Randolph Hearst Burn Center â€“ a world leader in burn treatment, rehabilitation, research, and education.NewYork-Presbyterian Komansky Children's Hospital at NewYork-Presbyterian/Weill Cornell Medical Center is a full-service, multidisciplinary pediatric program. Here highly skilled physicians, all of whom are affiliated with Weill Cornell Medicine, offer the full range of pediatric subspecialty care, including for respiratory conditions, cancer, and congenital heart disease. The Centerâ€™s Pediatric Emergency Department is a Level 1 Trauma Center and a New York City regional burn center.\",\n",
      "    \"NewYork-Presbyterian Allen Hospital5141 Broadway at 220th StreetNew York, NY 10034212-932-4000NewYork-Presbyterian Allen Hospital is a 196-bed community hospital serving northern Manhattan, Riverdale, and other communities in the Bronx, Westchester, and Northern New Jersey. All of the Hospitalâ€™s physicians are on the faculty of Columbia University Vagelos College of Physicians and Surgeons. Among the Hospitalâ€™s major services are cardiology, geriatric medicine, obstetrics and gynecology, general surgery, orthopedics, emergency medicine, and family medicine. In addition, the hospital is home to Och Spine Hospital. The Och Spine Hospital includes both orthopedic, neurosurgical, and rehabilitation spine specialists who provide comprehensive operative and nonoperative care for the broad range of spinal disorders.\",\n",
      "    \"NewYork-Presbyterian Brooklyn Methodist Hospital             506 6th Street, Brooklyn, NY 11215  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Hudson Valley Hospital             1980 Crompond Road, Cortlandt Manor, NY 10567  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Hudson Valley Hospital1980 Crompond RoadCortlandt Manor, NY 10567914-737-9000NewYork-Presbyterian Hudson Valley Hospital, a member of the NewYork-Presbyterian Regional Hospital Network, has been providing quality health care to residents of Westchester, Putnam, and Dutchess counties for more than 125 years. Today, the 128-bed hospital provides a wide range of ambulatory care and inpatient services in more than 60 specialties and continues to expand programs and resources in collaboration with NewYork-Presbyterian and ColumbiaDoctors, the faculty practice of Columbia University Medical Center. The first hospital in the region to achieve Magnet status in recognition of the commitment to excellence, nursing innovation, and outstanding quality treatment, NewYork-Presbyterian/Hudson Valley Hospital continues to lead the way in ensuring exceptional patient care.Patients benefit from the availability of specialists in virtually every field of medicine, access to the latest technological advances in diagnosis and treatment, and care that is provided in state-of-the-art facilities, including a four-story patient tower with 84 private rooms, the latest radiology and imaging technologies, a minimally invasive surgery center, and a comprehensive cancer center. The hospital is also home to the regionâ€™s only 24-hour â€œno waitâ€ Emergency Department, which sees more than 40,000 visits a year â€“ one of the largest volumes in Westchester County.\",\n",
      "    \"NewYork-Presbyterian Komansky Children's Hospital             525 East 68th Street, New York, NY 10065  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Lower Manhattan Hospital             170 William Street, New York, NY 10038  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Lower Manhattan Hospital170 William StreetNew York, NY 10038212-312-5000NewYork-Presbyterian Lower Manhattan Hospital is the only full-service, acute care, and emergency medicine facility south of 14th Street. The Hospital is affiliated with Weill Cornell Medicine and all of its physicians have appointments on the Weill Cornell Medical College faculty. Located at the foot of the Brooklyn Bridge and a few blocks from City Hall and the World Trade Center, the 180-bed Hospital serves the growing business and residential communities of Wall Street, Chinatown, SoHo, TriBeCa, Battery Park City, the Lower East Side, and surrounding neighborhoods. The Hospitalâ€™s Emergency Department (ED) is a community trauma center, a designated Stroke Center, a certified Chest Pain Center, and includes one of the largest decontamination units in New York City.\",\n",
      "    \"NewYork-Presbyterian Morgan Stanley Children's Hospital             3959 Broadway, New York, NY 10032  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Morgan Stanley Children's Hospital3959 BroadwayNew York, NY 10032212-305-2500NewYork-Presbyterian Morgan Stanley Childrenâ€™s Hospital offers the highest quality care in every pediatric subspecialty â€“ including the most complex neonatal, cardiac, and cancer care. Affiliated with Columbia University Vagelos College of Physicians and Surgeons, the 269-bed Hospital is one of the largest providers of childrenâ€™s health services in the tri-state area, as well a major international referral center, meeting the special needs of children from infancy through adolescence worldwide. Sloane Hospital for Women is located within NewYork-Presbyterian/Morgan Stanley Childrenâ€™s Hospital, providing the highest quality, most compassionate obstetrical, gynecological, and maternal-fetal care.\",\n",
      "    \"NewYork-Presbyterian Och Spine Hospital             5141 Broadway New York, NY 10034  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Queens             56-45 Main Street, Flushing, NY 11355  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Queens56-45 Main StreetFlushing, NY 11355718-670-2000NewYork-Presbyterian Queens, located in Flushing, New York, is a community teaching hospital serving residents of Queens and metro New York. The 535-bed tertiary care facility provides services in 14 clinical departments and numerous subspecialties, with 15,000 surgeries, 4,000 infant deliveries, and 124,000 emergency service visits each year.Through its network of affiliated primary and multispecialty care physician practices and community-based health centers, NewYork-Presbyterian/Queens also provides approximately 162,000 ambulatory care visits annually. The Hospital is a member of the NewYork-Presbyterian Regional Hospital Network and is affiliated with Weill Cornell Medicine.\",\n",
      "    \"NewYork-Presbyterian Westchester Behavioral Health Center             21 Bloomingdale Road, White Plains, NY 10605  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian Westchester Behavioral Health Center21 Bloomingdale RoadWhite Plains, NY 10605914-682-9100NewYork-Presbyterian Westchester Behavioral Health Â is a Planetree Designated Patient-Centered Hospital â€“ the first behavioral health hospital in the U.S. to receive this distinction. Located about 25 miles north of midtown Manhattan, NewYork-Presbyterian/Westchester has 270 inpatient beds, and extensive outpatient, partial hospital, and day treatment programs. Leading psychiatrists and mental health professionals in the field provide a full continuum of psychiatric care for children, adolescents, adults, and the elderly. Highly specialized services are available for the full range of psychiatric illnesses, including personality disorders, eating disorders, anxiety and mood disorders, and psychotic disorders. NewYork-Presbyterian/Westchester is also home to the state-of-the-art Center for Autism and the Developing Brain.\",\n",
      "    \"NewYork-Presbyterian/Columbia University Irving Medical Center             630 West 168th Street, New York, NY 10032  Opens in Google Maps\",\n",
      "    \"NewYork-Presbyterian/Weill Cornell Medical Center             525 East 68th Street, New York, NY 10065  Opens in Google Maps\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fhealthmatters.nyp.org%2Fwhat-to-know-about-covid-variant-xfg-stratus-and-how-to-protect-yourself%2F\",\n",
      "      \"https://www.facebook.com/newyorkpresbyterian\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/nyphospital/\",\n",
      "      \"https://urldefense.com/v3/__https://www.instagram.com/nypcommunity/?hl=en__;!!Aut6IJkzM0Y!skbXKkI47p6En3wn6Rkx38SOTmsCOdRIggW0q0pNwGmMtQxkfhfUoJIw5SSDEy2CNBd_kVa_xCtsRQNxPsExsejadH59AA$\",\n",
      "      \"https://instagram.com/nyphospital\"\n",
      "    ],\n",
      "    \"linkedin\": [\n",
      "      \"https://www.linkedin.com/shareArticle?url=https%3A%2F%2Fhealthmatters.nyp.org%2Fwhat-to-know-about-covid-variant-xfg-stratus-and-how-to-protect-yourself%2F&title=What+to+Know+About+COVID+Variant+XFG+%28Stratus%29+and+How+to+Protect+Yourself&mini=1\",\n",
      "      \"https://www.linkedin.com/company/new-york-presbyterian-hospital\"\n",
      "    ],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": [\n",
      "      \"https://www.youtube.com/user/newyorkpresbyterian\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://maps.app.goo.gl/Rj2r8F4QMRCfph9L9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get LATEST chromedriver version for google-chrome\n",
      "Get LATEST chromedriver version for google-chrome\n",
      "Driver [C:\\Users\\ABC\\.wdm\\drivers\\chromedriver\\win64\\142.0.7444.61\\chromedriver-win32/chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found website: http://www.vu.edu.pk/\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/AboutVU.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/visionAndmission.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/Education.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/Milestones.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/MOUsCollaboration.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/organization.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/BOG.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/Directorates.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/Faculty.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/AdjunctFaculty.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/RTI.aspx\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=BS-Lateral\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=BS\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=B.Ed\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=Associate Degree Programs\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=MS\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=Diploma\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=Short Courses\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=Specialization Certificate\n",
      "ðŸ“„ Scraping: http://www.vu.edu.pk/AboutUs/ProgramsOffered.aspx?Degree=Zero Semester\n",
      "ðŸ“„ Scraping: https://www.vu.edu.pk/AboutUs/RTI.aspx\n",
      "ðŸ“„ Scraping: https://www.vu.edu.pk/contact\n",
      "ðŸ“„ Scraping: https://www.vu.edu.pk/SupportSystem/\n",
      "{\n",
      "  \"website\": \"http://www.vu.edu.pk/\",\n",
      "  \"emails\": [\n",
      "    \"acf619@vu.edu.pk\",\n",
      "    \"acfi619@vu.edu.pk\",\n",
      "    \"bnk619@vu.edu\",\n",
      "    \"bnk620@vu.edu.pk\",\n",
      "    \"bnki619@vu.edu.pk\",\n",
      "    \"bnki620@vu.edu.pk\",\n",
      "    \"com619@vu.edu.pk\",\n",
      "    \"comi619@vu.edu.pk\",\n",
      "    \"cs619@vu.edu.pk\",\n",
      "    \"cs@vu.edu.pk\",\n",
      "    \"eco@vu.edu.pk\",\n",
      "    \"edu@vu.edu.pk\",\n",
      "    \"eng@vu.edu.pk\",\n",
      "    \"fin619@vu.edu.pk\",\n",
      "    \"fin620@vu.edu.pk\",\n",
      "    \"fini619@vu.edu.pk\",\n",
      "    \"fini620@vu.edu.pk\",\n",
      "    \"hrm619@vu.edu.pk\",\n",
      "    \"hrm620@vu.edu.pk\",\n",
      "    \"hrmi619@vu.edu.pk\",\n",
      "    \"hrmi620@vu.edu.pk\",\n",
      "    \"info@vu.edu.pk\",\n",
      "    \"isl@vu.edu.pk\",\n",
      "    \"it619@vu.edu.pk\",\n",
      "    \"it620@vu.edu.pk\",\n",
      "    \"iti619@vu.edu.pk\",\n",
      "    \"lms_support@vu.edu.pk\",\n",
      "    \"masscomm@vu.edu.pk\",\n",
      "    \"mcm619@vu.edu.pk\",\n",
      "    \"mcmi619@vu.edu.pk\",\n",
      "    \"mgit661@vu.edu.pk\",\n",
      "    \"mgit662@vu.edu.pk\",\n",
      "    \"mgt619@vu.edu.pk\",\n",
      "    \"mgt620@vu.edu.pk\",\n",
      "    \"mgt661@vu.edu.pk\",\n",
      "    \"mgt662@vu.edu.pk\",\n",
      "    \"mgt@vu.edu.pk\",\n",
      "    \"mgti619@vu.edu.pk\",\n",
      "    \"mgti620@vu.edu.pk\",\n",
      "    \"mis619@vu.edu.pk\",\n",
      "    \"misi619@vu.edu.pk\",\n",
      "    \"misi620@vu.edu.pk\",\n",
      "    \"mkt619@vu.edu.pk\",\n",
      "    \"mkt620@vu.edu.pk\",\n",
      "    \"mkti619@vu.edu.pk\",\n",
      "    \"mkti620@vu.edu.pk\",\n",
      "    \"mscs_courseselection@vu.edu.pk\",\n",
      "    \"mth@vu.edu.pk\",\n",
      "    \"pa@vu.edu.pk\",\n",
      "    \"pad619@vu.edu.pk\",\n",
      "    \"padi619@vu.edu.pk\",\n",
      "    \"pak@vu.edu.pk\",\n",
      "    \"password@vu.edu.pk\",\n",
      "    \"phy@vu.edu.pk\",\n",
      "    \"pio@vu.edu.pk\",\n",
      "    \"psp@vu.edu.pk\",\n",
      "    \"psy619@vu.edu.pk\",\n",
      "    \"psy@vu.edu.pk\",\n",
      "    \"psyi619@vu.edu.pk\",\n",
      "    \"referenceletter.arts@vu.edu.pk\",\n",
      "    \"referenceletter.cs@vu.edu.pk\",\n",
      "    \"referenceletter.edu@vu.edu.pk\",\n",
      "    \"referenceletter.mass@vu.edu.pk\",\n",
      "    \"referenceletter.mgt@vu.edu.pk\",\n",
      "    \"referenceletter.psy@vu.edu.pk\",\n",
      "    \"referenceletter.soc@vu.edu.pk\",\n",
      "    \"saleha.ali@vu.edu.pk\",\n",
      "    \"soc@vu.edu.pk\",\n",
      "    \"sta@vu.edu.pk\",\n",
      "    \"urd@vu.edu.pk\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"111880880\",\n",
      "    \"1118808801288\"\n",
      "  ],\n",
      "  \"addresses\": [\n",
      "    \"Abdus Salam School of Mathematical Sciences, GC University, Lahore, Pakistan\",\n",
      "    \"Address:Â 54 Lawrence Road, Lahore\",\n",
      "    \"Address:Â D-102, M.A. Road Satellite Town, Rawalpindi.\",\n",
      "    \"Address:Â Plot No. D - 3. First Floor. Above: Habib Metropolitan Bank and Bank Al-Baraka. Near Five Star Chowrangi. Block - D. North Nazimabad, Karachi.\",\n",
      "    \"Address:Â Plot No.26-B, Ali Arcade Near Babri Mosque & Bank Al-falah Ltd, G-10 Markaz, Islamabad.\",\n",
      "    \"Allama Iqbal Open University, Islamabad.\",\n",
      "    \"Block-D, Pak Secretariat, Islamabad.\",\n",
      "    \"CEMB, University of the Punjab, Lahore\",\n",
      "    \"CM Pak Office, Jinnah Avenue, Blue Area, Islamabad.\",\n",
      "    \"COMSATS University Isalamabad, Lahore, Pakistan\",\n",
      "    \"COMSATS University Islamabad\",\n",
      "    \"COMSATS University, Islamabad\",\n",
      "    \"Cabinet Block, Islamabad.\",\n",
      "    \"Capital University of Science & Technology (CUST), Islamabad.\",\n",
      "    \"Comsats University Islamabad, Lahore, Pakistan\",\n",
      "    \"Department of Political Science, University of the Punjab, Lahore.\",\n",
      "    \"FAST Lahore, Lahore Leads University\",\n",
      "    \"FC College University, Lahore\",\n",
      "    \"Forman Christian College, University, Lahore\",\n",
      "    \"Foundation University School of Science & Technology (FUSST) Islamabad\",\n",
      "    \"GC University Lahore, Pakistan\",\n",
      "    \"GC University, Lahore\",\n",
      "    \"GC university Lahore\",\n",
      "    \"GCU, Lahore\",\n",
      "    \"Government College University (GCU), Lahore\",\n",
      "    \"Government College University Lahore\",\n",
      "    \"Government College University, Lahore\",\n",
      "    \"Government College University, Lahore.\",\n",
      "    \"Government College, Lahore.\",\n",
      "    \"Government College, Punjab University, Lahore\",\n",
      "    \"Government College, University (GCU), Lahore, Pakistan\",\n",
      "    \"Govt. College University, Lahore\",\n",
      "    \"HEJ Research Institute of Chemistry, University of Karachi.\",\n",
      "    \"Head Office (Islamabad)\",\n",
      "    \"Headquarters, Sector F-5/1, Islamabad.\",\n",
      "    \"IBA-Karachi\",\n",
      "    \"Information Technology & Telecom, M/o Information Technology, Government of Pakistan, Islamabad.\",\n",
      "    \"Institute of Business Administration,University of the Punjab, Lahore\",\n",
      "    \"Institute of Communication Studies, Punjab University, Lahore Pakistan.\",\n",
      "    \"Institute of Social and Cultural Studies, University of the Punjab, Lahore, Pakistan\",\n",
      "    \"Institute of Zoology, University of the Punjab, Lahore\",\n",
      "    \"International  Islamic University Islamabad\",\n",
      "    \"International Islamic University Islamabad\",\n",
      "    \"International Islamic University, Islamabad\",\n",
      "    \"Islamabad\",\n",
      "    \"Islamabad Office\",\n",
      "    \"Islamabad.\",\n",
      "    \"Jeju National University South Korea, Pak-AIMS Lahore\",\n",
      "    \"Jeju National University, South Korea, NUST Islamabad, UET Lahore\",\n",
      "    \"Karachi Office\",\n",
      "    \"Karachi University\",\n",
      "    \"LCWU,Lahore\",\n",
      "    \"Lahore College for Women University\",\n",
      "    \"Lahore College for Women University Lahore\",\n",
      "    \"Lahore College for Women University, Lahore\",\n",
      "    \"Lahore College of Business Administration\",\n",
      "    \"Lahore Leads University, Lahore College for Women University\",\n",
      "    \"Lahore Leads University, Pakistan\",\n",
      "    \"Lahore Leads Unviersity, Lahore.\",\n",
      "    \"Lahore Office\",\n",
      "    \"Lahore University of Management Sciences\",\n",
      "    \"Lahore, Pakistan.\",\n",
      "    \"Lawrence Road Office\",\n",
      "    \"M Phil. Islamic Studies, University of The Punjab, Lahore, Pakistan.\",\n",
      "    \"M.A. Jinnah Campus, Defence Road, Off Raiwind Road,\",\n",
      "    \"Minhaj University Lahore\",\n",
      "    \"Minhaj University Lahore - University of Okara\",\n",
      "    \"Ministry of Information and Broadcasting,\",\n",
      "    \"Moscow State University (Russia)- University of Karachi- University of the Punjab\",\n",
      "    \"NCBA & E,      NCBA & E,   FAST NUCES Lahore\",\n",
      "    \"NUML, Islamabad\",\n",
      "    \"NUST Islamabad\",\n",
      "    \"NUST, Islamabad\",\n",
      "    \"NUST, Islamabad, Pakistan\",\n",
      "    \"National College of Business & Economics, Lahore\",\n",
      "    \"National College of Business Administration & Economics Lahore (Lahore/Pakistan)\",\n",
      "    \"National University of Modern Languages Islamabad\",\n",
      "    \"National University of Sciences and Technology, Islamabad Pakistan\",\n",
      "    \"PIEAS, Islamabad\",\n",
      "    \"PIQC-Superior University, Lahore, Pakistan\",\n",
      "    \"PUNJAB UNIVERSITY LAHORE\",\n",
      "    \"Pakistan Institute of Development Economics (PIDE), Islamabad\",\n",
      "    \"Pakistan Institute of Engineering and Applied Sciences, Islamabad\",\n",
      "    \"Punjab University Lahore and University of Saskatchewan, (UOS) Canada\",\n",
      "    \"Punjab University Lahore; The George Washington University Washington D.C.\",\n",
      "    \"Punjab University, Lahore\",\n",
      "    \"Punjab University, Lahore.\",\n",
      "    \"Quaid-e-Azam University, Islamabad\",\n",
      "    \"Quaid-i-Azam University Islamabad\",\n",
      "    \"Quaid-i-Azam University Islamabad Pakistan\",\n",
      "    \"Quaid-i-Azam University,  Islamabad\",\n",
      "    \"Quaid-i-Azam University, Islamabad\",\n",
      "    \"Quaid-i-azam University, islamabad\",\n",
      "    \"SBS. University of the Punjab. Lahore, Pakistan.\",\n",
      "    \"School of Business & Economics, University of Management & Technology, Lahore\",\n",
      "    \"Secretary , Ministry of Information & Media Development, Government of Pakistan, Islamabad\",\n",
      "    \"Secretary, Ministry of Education, Islamabad or his nominee not below the rank of Joint Secretary\",\n",
      "    \"Secretary, Ministry of Information Technology & Telecom, Government of Pakistan, Islamabad\",\n",
      "    \"Secretary, Ministry of Information and Broadcasting, Government of Pakistan (Ex-Officio)\",\n",
      "    \"Sector H-9, Islamabad.\",\n",
      "    \"Sir Syed CASE Institute of Technology, Islamabad.\",\n",
      "    \"Sir Syed Memorial Society Building, 19-Ataturk Avenue, G-5/1, Islamabad\",\n",
      "    \"Sir Syed Memorial Society Building, 19-Ataturk Avenue, G-5/1, Islamabad.\",\n",
      "    \"Superior University, Lahore\",\n",
      "    \"The University of Lahore\",\n",
      "    \"The Virtual University, Pakistanâ€™s first University based completely on modern Information and Communication Technologies, was established by the Government as a public sector, not-for-profit institution with a clear mission: to provide extremely affordable world class education to aspiring students all over the country. Using free-to-air satellite television broadcasts and the Internet, the Virtual University allows students to follow its rigorous programs regardless of their physical locations.\",\n",
      "    \"UET, Lahore.\",\n",
      "    \"UMT, Lahore, Pakistan\",\n",
      "    \"UNIVERSITY OF MANAGEMENT AND TECHNOLOGY, LAHORE\",\n",
      "    \"UVAS, Lahore\",\n",
      "    \"UVAS, Lahore-Pakistan\",\n",
      "    \"University of Engineering & Technology Lahore\",\n",
      "    \"University of Engineering & Technology, Lahore\",\n",
      "    \"University of Engineering and Technology (UET), Lahore\",\n",
      "    \"University of Engineering and Technology (UET), Lahore, Pakistan\",\n",
      "    \"University of Karachi\",\n",
      "    \"University of Management and Technology, Lahore\",\n",
      "    \"University of Management and Technology, Lahore (UMT)\",\n",
      "    \"University of Punjab, Lahore/ Karachi University\",\n",
      "    \"University of Veterinary and Animal Sciences, Lahore\",\n",
      "    \"University of Veterinary and Animal Sciences, Lahore, Pakistan.\",\n",
      "    \"University of the Punjab Lahore\",\n",
      "    \"University of the Punjab, Lahore\",\n",
      "    \"University of the Punjab, Lahore, Pakistan\",\n",
      "    \"University of the Punjab, Lahore.\",\n",
      "    \"Virtual University launches 2 free to air Educational Broadcast Television Channels (VUTV 1 & VUTV 2)\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"https://www.facebook.com/VirtualUniversityOfPakistan\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/virtualuniversityofpakistan/?hl=en\"\n",
      "    ],\n",
      "    \"linkedin\": [\n",
      "      \"https://pk.linkedin.com/school/virtual-university-of-pakistan/?trk=public_profile_topcard-school\"\n",
      "    ],\n",
      "    \"twitter\": [\n",
      "      \"https://twitter.com/VUPakistan\"\n",
      "    ],\n",
      "    \"youtube\": [\n",
      "      \"https://www.youtube.com/vu\",\n",
      "      \"https://www.youtube.com/@thevirtualuniversityofpakistan\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info1.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info1.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://maps.app.goo.gl/CjSpy46fYEErzZpy7\n",
      "âœ… Found website: https://zainabclinics.com/\n",
      "ðŸ“„ Scraping: https://zainabclinics.com/\n",
      "âš ï¸ Requests failed for https://zainabclinics.com/: HTTPSConnectionPool(host='zainabclinics.com', port=443): Read timed out.\n",
      "ðŸ“„ Scraping: https://zainabclinics.com/about-us-zainab-dental-clinic/\n",
      "ðŸ“„ Scraping: https://zainabclinics.com/contact-us-zainab-dental-clinic/\n",
      "ðŸ“„ Scraping: https://zainabclinics.com/contact-us/\n",
      "ðŸ“„ Scraping: https://zainabclinics.com/about-us/\n",
      "{\n",
      "  \"website\": \"https://zainabclinics.com/\",\n",
      "  \"emails\": [\n",
      "    \"info@zainabclinics.com\"\n",
      "  ],\n",
      "  \"phones\": [\n",
      "    \"03435485407\"\n",
      "  ],\n",
      "  \"addresses\": [\n",
      "    \"Main PWD Rd, opposite bank alfalah, Block C Sector C PWD Society, Islamabad, 44000\",\n",
      "    \"Your Trusted Dental Clinic in PWD, Islamabad\",\n",
      "    \"Your Trusted Dental Health Partner in PWD, Islamabad\"\n",
      "  ],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"https://www.facebook.com/ZainabMedical/\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/zainabdental/\"\n",
      "    ],\n",
      "    \"linkedin\": [],\n",
      "    \"twitter\": [],\n",
      "    \"youtube\": []\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info3.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info3.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Opening Google Maps: https://maps.app.goo.gl/6LH3qEbrVK2YKNSx7\n",
      "âœ… Found website: https://med.uth.edu/\n",
      "ðŸ“„ Scraping: https://med.uth.edu/\n",
      "ðŸ“„ Scraping: https://outlook.office.com/\n",
      "ðŸ“„ Scraping: https://med.uth.edu/about-us/\n",
      "ðŸ“„ Scraping: https://med.uth.edu/about-us/dual-degree-programs/\n",
      "âš ï¸ Requests failed for https://med.uth.edu/about-us/dual-degree-programs/: HTTPSConnectionPool(host='med.uth.edu', port=443): Read timed out.\n",
      "ðŸ“„ Scraping: https://med.uth.edu/contact-us/\n",
      "âš ï¸ Requests failed for https://med.uth.edu/contact-us/: HTTPSConnectionPool(host='med.uth.edu', port=443): Read timed out. (read timeout=10)\n",
      "{\n",
      "  \"website\": \"https://med.uth.edu/\",\n",
      "  \"emails\": [],\n",
      "  \"phones\": [\n",
      "    \"30033-4097\",\n",
      "    \"713-500-51026431\"\n",
      "  ],\n",
      "  \"addresses\": [],\n",
      "  \"social_links\": {\n",
      "    \"facebook\": [\n",
      "      \"https://www.facebook.com/mcgovernmed/\"\n",
      "    ],\n",
      "    \"instagram\": [\n",
      "      \"https://www.instagram.com/mcgovernmed/\"\n",
      "    ],\n",
      "    \"linkedin\": [\n",
      "      \"https://www.linkedin.com/school/mcgovern/\"\n",
      "    ],\n",
      "    \"twitter\": [\n",
      "      \"https://twitter.com/McGovernMed\"\n",
      "    ],\n",
      "    \"youtube\": [\n",
      "      \"https://youtu.be/AR9o6UIiIY0?si=CiCH7hMxl6vllX7j\",\n",
      "      \"https://www.youtube.com/user/utmedicalschool#grid/user/DDBA88ACF6124877\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "ðŸ’¾ Data saved to business_info4.json\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ðŸŒ Focused Contact Info Scraper\n",
    "- Gets main website from Google Maps\n",
    "- Scrapes main page & priority pages only (contact/about)\n",
    "- Extracts emails, phones, addresses, social links\n",
    "- Output JSON contains only real contact info, main website, social media\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Dict, List, Set, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "class FocusedCrawler:\n",
    "    EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}')\n",
    "    PHONE_PATTERN = re.compile(r'\\+?\\d[\\d\\s\\-]{8,}\\d')\n",
    "    ADDRESS_KEYWORDS = ['street', 'road', 'plaza', 'building', 'suite', 'islamabad', 'lahore', 'karachi']\n",
    "\n",
    "    SOCIAL_PLATFORMS = {\n",
    "        'facebook': ['facebook.com', 'fb.com'],\n",
    "        'instagram': ['instagram.com'],\n",
    "        'linkedin': ['linkedin.com'],\n",
    "        'twitter': ['twitter.com', 'x.com'],\n",
    "        'youtube': ['youtube.com', 'youtu.be']\n",
    "    }\n",
    "\n",
    "    PRIORITY_KEYWORDS = ['contact', 'about', 'reach', 'support', 'office']\n",
    "\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "\n",
    "    def _init_driver(self) -> webdriver.Chrome:\n",
    "        options = Options()\n",
    "        if self.headless:\n",
    "            options.add_argument(\"--headless=new\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(f\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\")\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "\n",
    "    def get_website_from_google_maps(self, gmaps_url: str) -> Optional[str]:\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"ðŸ” Opening Google Maps: {gmaps_url}\")\n",
    "            driver = self._init_driver()\n",
    "            driver.get(gmaps_url)\n",
    "            wait = WebDriverWait(driver, 15)\n",
    "            selectors = [\"//a[contains(@aria-label, 'Website')]\", \"//a[contains(text(),'Website')]\"]\n",
    "            website_url = None\n",
    "            for sel in selectors:\n",
    "                try:\n",
    "                    element = wait.until(EC.presence_of_element_located((By.XPATH, sel)))\n",
    "                    website_url = element.get_attribute('href')\n",
    "                    if website_url and website_url.startswith('http'):\n",
    "                        # remove Google redirect\n",
    "                        from urllib.parse import unquote, parse_qs, urlparse\n",
    "                        if 'google.com/url?' in website_url:\n",
    "                            query = parse_qs(urlparse(website_url).query)\n",
    "                            if 'q' in query:\n",
    "                                website_url = unquote(query['q'][0])\n",
    "                        break\n",
    "                except TimeoutException:\n",
    "                    continue\n",
    "            if website_url:\n",
    "                print(f\"âœ… Found website: {website_url}\")\n",
    "                return website_url\n",
    "            else:\n",
    "                print(\"âŒ No website found\")\n",
    "                return None\n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "    def _extract_emails(self, text: str) -> Set[str]:\n",
    "        return set(self.EMAIL_PATTERN.findall(text))\n",
    "\n",
    "    def _extract_phones(self, text: str) -> Set[str]:\n",
    "        numbers = self.PHONE_PATTERN.findall(text)\n",
    "        cleaned = set()\n",
    "        for p in numbers:\n",
    "            clean = re.sub(r'\\s+', '', p)\n",
    "            if sum(c.isdigit() for c in clean) >= 9:\n",
    "                cleaned.add(clean)\n",
    "        return cleaned\n",
    "\n",
    "    def _extract_addresses(self, text: str) -> Set[str]:\n",
    "        addresses = set()\n",
    "        for line in text.splitlines():\n",
    "            for keyword in self.ADDRESS_KEYWORDS:\n",
    "                if keyword.lower() in line.lower() and len(line) > 10:\n",
    "                    addresses.add(line.strip())\n",
    "        return addresses\n",
    "\n",
    "    def _extract_social_links(self, soup: BeautifulSoup, base_url: str) -> Dict[str, List[str]]:\n",
    "        links = {platform: [] for platform in self.SOCIAL_PLATFORMS}\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href'].lower()\n",
    "            for platform, domains in self.SOCIAL_PLATFORMS.items():\n",
    "                if any(d in href for d in domains):\n",
    "                    full_url = urljoin(base_url, a['href'])\n",
    "                    if full_url not in links[platform]:\n",
    "                        links[platform].append(full_url)\n",
    "        return links\n",
    "\n",
    "    def _scrape_page(self, url: str) -> tuple:\n",
    "        try:\n",
    "            print(f\"ðŸ“„ Scraping: {url}\")\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for el in soup(['script','style','noscript']):\n",
    "                el.decompose()\n",
    "            text = soup.get_text()\n",
    "            emails = self._extract_emails(text)\n",
    "            phones = self._extract_phones(text)\n",
    "            addresses = self._extract_addresses(text)\n",
    "            social_links = self._extract_social_links(soup, url)\n",
    "            return emails, phones, addresses, social_links\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Requests failed for {url}: {e}\")\n",
    "            return set(), set(), set(), {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "    def scrape_main_website(self, website_url: str) -> Dict:\n",
    "        urls_to_scrape = [website_url]  # main page\n",
    "        main_emails, main_phones, main_addresses = set(), set(), set()\n",
    "        main_social = {p: [] for p in self.SOCIAL_PLATFORMS}\n",
    "\n",
    "        # Try priority pages first: contact/about\n",
    "        try:\n",
    "            resp = self.session.get(website_url, timeout=10)\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                href = a['href']\n",
    "                if any(k in href.lower() for k in self.PRIORITY_KEYWORDS):\n",
    "                    full_url = urljoin(website_url, href)\n",
    "                    if full_url not in urls_to_scrape:\n",
    "                        urls_to_scrape.append(full_url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for url in urls_to_scrape:\n",
    "            emails, phones, addresses, social_links = self._scrape_page(url)\n",
    "            main_emails.update(emails)\n",
    "            main_phones.update(phones)\n",
    "            main_addresses.update(addresses)\n",
    "            for p in main_social:\n",
    "                main_social[p].extend([l for l in social_links[p] if l not in main_social[p]])\n",
    "\n",
    "        return {\n",
    "            \"website\": website_url,\n",
    "            \"emails\": sorted(list(main_emails)),\n",
    "            \"phones\": sorted(list(main_phones)),\n",
    "            \"addresses\": sorted(list(main_addresses)),\n",
    "            \"social_links\": {p: list(set(v)) for p,v in main_social.items()}\n",
    "        }\n",
    "\n",
    "    def scrape_from_gmaps(self, gmaps_url: str) -> Optional[Dict]:\n",
    "        website = self.get_website_from_google_maps(gmaps_url)\n",
    "        if website:\n",
    "            return self.scrape_main_website(website)\n",
    "        return None\n",
    "\n",
    "    def save_to_json(self, data: Dict, filename='business_info4.json'):\n",
    "        with open(filename,'w',encoding='utf-8') as f:\n",
    "            json.dump(data,f,indent=2,ensure_ascii=False)\n",
    "        print(f\"ðŸ’¾ Data saved to {filename}\")\n",
    "\n",
    "def main():\n",
    "    gmaps_url = input(\"ðŸ“ Enter Google Maps URL: \").strip()\n",
    "    if not gmaps_url:\n",
    "        print(\"âŒ No URL provided\")\n",
    "        return\n",
    "    crawler = FocusedCrawler(headless=True)\n",
    "    result = crawler.scrape_from_gmaps(gmaps_url)\n",
    "    if result:\n",
    "        print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "        crawler.save_to_json(result)\n",
    "    else:\n",
    "        print(\"âŒ Could not scrape website\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
